\documentclass[12pt]{article}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fontspec}
\usepackage{xfrac}
\usepackage{array}
\usepackage{siunitx}
\usepackage{gensymb}
\usepackage{enumitem}
\usepackage{dirtytalk}
\usepackage{bm}
\title{Vector spaces (exercises)}
\author{ZoÃ« Sparks}

\begin{document}

\theoremstyle{definition}

\sisetup{quotient-mode=fraction}
\newtheorem{thm}{Theorem}
\newtheorem*{nthm}{Theorem}
\newtheorem{sthm}{}[thm]
\newtheorem{lemma}{Lemma}[thm]
\newtheorem*{nlemma}{Lemma}
\newtheorem{cor}{Corollary}[thm]
\newtheorem*{prop}{Property}
\newtheorem*{defn}{Definition}
\newtheorem*{comm}{Comment}
\newtheorem*{exm}{Example}

\maketitle

\begin{enumerate}
  \item
    Given a field $F$, we would like to show that $F^n$ as we
    defined in the first example of \say{Vector spaces} is indeed
    a vector space over $F$.

    If $\alpha = (x_1,x_2,\ldots,x_n)$ of scalars $x_i$ in $F$
    and $\beta = (y_1,y_2,\ldots,y_n)$ with $y_i$ in $F$, we have
    that
    \begin{align*}
      \alpha + \beta = (x_1+y_1,\ x_2+y_2,\ \ldots,\ x_n+y_n),
    \end{align*}
    and thus that
    \begin{align*}
      \beta + \alpha = (y_1+x_1,\ y_2+x_2,\ \ldots,\ y_n+x_n).
    \end{align*}
    Since addition in $F$ is associative,
    \begin{align*}
      (x_1+y_1,\ x_2+y_2,\ \ldots,\ x_n+y_n) = (y_1+x_1,\
      y_2+x_2,\ \ldots,\ y_n+x_n),
    \end{align*}
    and thus
    \begin{align*}
      \alpha + \beta = \beta + \alpha.
    \end{align*}

    If $\gamma = (z_1,z_2,\ldots,z_n)$ of scalars $z_i$ in $F$,
    then
    \begin{align*}
      (\alpha + \beta) + \gamma = [(x_1+y_1)+z_1,\
      (x_2+y_2)+z_2,\ \ldots,\ (x_n+y_n)+z_n].
    \end{align*}
    Also,
    \begin{align*}
      \beta + \gamma = (y_1+z_1,\ y_2+z_2,\ \ldots,\ y_n+z_n),
    \end{align*}
    so
    \begin{align*}
      \alpha + (\beta + \gamma) = [x_1+(y_1+z_1),\
      x_2+(y_2+z_2),\ \ldots,\ x_n+(y_n+z_n)].
    \end{align*}
    Since addition in $F$ is commutative,
    \begin{align*}
      [x_1+(y_1+z_1),\ x_2+(y_2+z_2),\ \ldots,\ x_n+(y_n+z_n)]
      &=\\
      [(x_1+y_1)+z_1,\ (x_2+y_2)+z_2,\ \ldots,\ (x_n+y_n)+z_n],
    \end{align*}
    and thus
    \begin{align*}
      \alpha + (\beta + \gamma) = (\alpha + \beta) + \gamma.
    \end{align*}

    The zero vector in $F^{n}$ is the vector $0 =
    (p_1,p_2,\ldots,p_n)$ for which all $p_i = 0$. This is
    because
    \begin{align*}
      \alpha + 0 &= (x_1+0,\ x_2+0,\ \ldots,\ x_n+0)\\
                 &= (x_1,\ x_2,\ \ldots,\ x_n)\\
                 &= \alpha.
    \end{align*}

    The additive inverse of $\alpha = (x_1,x_2,\ldots,x_n)$ is
    the vector $-\alpha =\\ (-x_1,-x_2,\ldots,-x_n)$. This is
    because
    \begin{align*}
      \alpha + (-\alpha) &= (x_1-x_1,\ x_2-x_2,\ \ldots,\ x_n-x_n)\\
                         &= (0,0,\ldots,0)\\
                         &= 0.
    \end{align*}

    For a scalar $c$, we have that
    \begin{align*}
      c\alpha = (cx_1,cx_2,\ldots,cx_3).
    \end{align*}
    Then
    \begin{align*}
      c(\alpha + \beta) &= [c(x_1+y_1),\ c(x_2+y_2),\ \ldots,\ c(x_n+y_n)]\\
                        &= (cx_1+cy_1,\ cx_2+cy_2,\ \ldots,\ cx_n+cy_n)\\
                        &= c\alpha + c\beta,\\\\
      (c_1+c_2)\alpha &= [(c_1+c_2)x_1,\ (c_1+c_2)x_2,\ \ldots,\ (c_1+c_2)x_n]\\
                      &= (c_1x_1+c_2x_1,\ c_1x_2+c_2x_2,\ \ldots,\ c_1x_n+c_2x_n)\\
                      &= c_1\alpha + c_2\alpha,\\
      \shortintertext{and}
      (c_1c_2)\alpha &= [(c_1c_2)x_1,\ (c_1c_2)x_2,\ \ldots,\ (c_1c_2)x_n]\\
                     &= [c_1(c_2x_1),\ c_1(c_2x_2),\ \ldots,\ c_1(c_2x_n)]\\
                     &= c_1(c_2\alpha).
    \end{align*}
    Also, for every $\alpha$ in $V$, we have that
    \begin{align*}
      1\alpha &= (1x_1,\ 1x_2,\ \ldots,\ 1x_n)\\
              &= (x_1,\ x_2,\ \ldots,\ x_n)\\
              &= \alpha.
    \end{align*}

    Thus, our definitions of vector addition and scalar multiplication for $F^{n}$
    are proper by way of a vector space.

  \item
    Given a field $F$ and a vector space $V$ over it, we would like to verify that
    \begin{align*}
      (\alpha_1 + \alpha_2) + (\alpha_3 + \alpha_4) =
      [\alpha_2 + (\alpha_3 + \alpha_1)] + \alpha_4
    \end{align*}
    for all vectors $\alpha_1,\alpha_2,\alpha_3,\alpha_4$ in $V$.

    By 3(a) and 3(b),
    \begin{align*}
      (\alpha_1 + \alpha_2) + (\alpha_3 + \alpha_4) &=
      (\alpha_2 + \alpha_1) + (\alpha_3 + \alpha_4)\\
      &= \alpha_2 + (\alpha_1 + \alpha_3) + \alpha_4\\
      &= \alpha_2 + (\alpha_3 + \alpha_1) + \alpha_4\\
      &= [\alpha_2 + (\alpha_3 + \alpha_1)] + \alpha_4.
    \end{align*}

  \item
    Given $C$ as the field of complex numbers, we would like to determine which
    vectors in $C^3$ are linear combinations of $(1,0,-1)$, $(0,1,1)$, and $(1,1,1)$.

    These would be vectors $\beta$ such that
    \begin{align*}
      \beta &= c_1(1,0,-1) + c_2(0,1,1) + c_3(1,1,1)\\
            &= (c_1,0,-c_1) + (0,c_2,c_2) + (c_3,c_3,c_3)\\
            &= (c_1+0+c_3,0+c_2+c_3,-c_1+c_2+c_3)\\
            &= (c_1+c_3,c_2+c_3,-c_1+c_2+c_3)
    \end{align*}
    for scalars $c_1,c_2,c_3$ in $C$.

  \item
    Given $V$ as the set of all pairs $(x,y)$ of real numbers and $F$ as the field of
    real numbers, we have
    \begin{align*}
      (x,y) + (x_1,y_1) &= (x + x_1,\ y + y_1)\\
      c(x,y) &= (cx,y).
    \end{align*}
    We would like to determine if $V$, together with these operations, constitutes a
    vector space over $F$.

    This definition of vector addition is the same as we have for the 2-tuple space
    $F^{2}$, and since the requirements for vector addition don't involve scalar
    multiplication, we can be confident that this definition is sufficient for a
    vector space. However, this definition of scalar multiplication varies, so let's
    look there.

    If $\alpha = (x_1,x_2)$ and $\beta = (y_1,y_2)$ are vectors in $V$ and
    $c,c_1,c_2$ are scalars in $F$,
    \begin{align*}
      c(\alpha + \beta) &= c[(x_1,x_2) + (y_1,y_2)]\\
                        &= c(x_1 + y_1,x_2 + y_2)\\
                        &= [c(x_1 + y_1),x_2 + y_2]\\
                        &= (cx_1 + cy_1,x_2 + y_2)\\
                        &= (cx_1,x_2) + (cy_1,y_2)\\
                        &= c(x_1,x_2) + c(y_1,y_2)\\
                        &= c\alpha + c\beta.\\
      \shortintertext{However,}
      (c_1 + c_2)\alpha &= (c_1 + c_2)(x_1,x_2)\\
                        &= [(c_1 + c_2)x_1,x_2]\\
                        &= (c_1x_1 + c_2x_1,x_2)\\
                        &= (c_1x_1 + c_2x_1,x_2 + 0)\\
                        &= (c_1x_1,x_2) + (c_2x_1,0)\\
                        &= c_1(x_1,x_2) + c_2(x_1,0)\\
                        &= c_1\alpha + c_2(x_1,0)\\
                        &\neq c_1\alpha + c_2\alpha,
    \end{align*}
    so this definition of scalar multiplication is not sufficient for $V$ to
    constitute a vector space.

  \item
    On $R^{n}$, we have two operations
    \begin{align*}
      \alpha \oplus \beta &= \alpha - \beta\\
      c \cdot \alpha &= -c\alpha,
    \end{align*}
    where $\alpha - \beta$ and $-c\alpha$ have their conventional definitions for the
    $n$-tuple space $F^{n}$ over $R$. We would like to determine which of the axioms
    for a vector space are satisfied by $(R^{n},\oplus,\cdot)$.

    Let $\alpha = (x_1,x_2,\ldots,x_n),\ \beta = (y_1,y_2,\ldots,y_n),\ \gamma =
    (z_1,z_2,\ldots,z_n)$ be vectors in $R^{n}$, such that $x_i,y_i,z_i$ are scalars
    in $R$.

    Note that, for 3(c), if the zero vector in $R^{n}$ is $0 = (p_1,p_2,\ldots,p_n)$
    where $p_i = 0$ in $R$,
    \begin{align*}
      \alpha \oplus 0 &= \alpha - 0\\
                      &= (x_1,x_2,\ldots,x_n) - 0\\
                      &= (x_1-0,\ x_2-0,\ \ldots,\ x_n-0)\\
                      &= (x_1,x_2,\ldots,x_n)\\
                      &= \alpha,
    \end{align*}
    so 3(c) is satisfied.

    Also, for 3(d), the additive inverse of $\alpha$ under these conditions is
    $\alpha$ itself; if $\alpha \oplus \beta = \alpha - \beta = 0$, then $\alpha -
    \beta + \beta = 0 + \beta$, so $\alpha = \beta$ and thus $\alpha \oplus \alpha =
    0$.  $\alpha$ is unique in $F^{n}$, so 3(d) is satisfied. Significantly, this
    implies that $\alpha = -\alpha$ here.

    For 3(a),
    \begin{align*}
      \alpha \oplus \beta &= \alpha - \beta\\
                          &= (x_1,x_2,\ldots,x_n) - (y_1,y_2,\ldots,y_n)\\
                          &= (x_1-y_1,\ x_2-y_2,\ \ldots,\ x_n-y_n)\\
                          &= (-y_1+x_1,\ -y_2+x_2,\ \ldots,\ -y_n+x_n)\\
                          &= (-y_1,-y_2,\ldots,-y_n) + (x_1,x_2,\ldots,x_n)\\
                          &= -(y_1,y_2,\ldots,y_n) - [-(x_1,x_2,\ldots,x_n)]\\
                          &= -\beta \oplus -\alpha.\\
                          &= \beta \oplus \alpha,
    \end{align*}
    so 3(a) is satisfied.

    For 3(b),
    \begin{align*}
      \alpha \oplus (\beta \oplus \gamma) &= \alpha - (\beta - \gamma)\\
      &= (x_1,x_2,\ldots,x_n) - [(y_1,y_2,\ldots,y_n) - (z_1,z_2,\ldots,z_n)]\\
      &= (x_1,x_2,\ldots,x_n) - (y_1-z_1,\ y_2-z_2,\ \ldots,\ y_n-z_n)\\
      &= [x_1-(y_1-z_1),\ x_2-(y_2-z_2),\ \ldots,\ x_n-(y_n-z_n)]\\
      &= (x_1-y_1+z_1,\ x_2-y_2+z_2,\ \ldots,\ x_n-y_n+z_n)\\
      &= [(x_1-y_1)+z_1,\ (x_2-y_2)+z_2,\ \ldots,\ (x_n-y_n)+z_n]\\
      &= (x_1-y_1,\ x_2-y_2,\ \ldots,\ x_n-y_n) + (z_1,z_2,\ldots,z_n)\\
      &= [(x_1,x_2,\ldots,x_n) - (y_1,y_2,\ldots,y_n)] + (z_1,z_2,\ldots,z_n)\\
      &= [(x_1,x_2,\ldots,x_n) - (y_1,y_2,\ldots,y_n)] - [-(z_1,z_2,\ldots,z_n)]\\
      &= (\alpha - \beta) - (-\gamma)\\
      &= (\alpha \oplus \beta) \oplus -\gamma\\
      &= (\alpha \oplus \beta) \oplus \gamma,
    \end{align*}
    so 3(b) is satisfied.

    Let $c,c_1,c_2$ be scalars in $R$.

    For 4(a),
    \begin{align*}
      c \cdot (\alpha \oplus \beta) &= -c(\alpha - \beta)\\
      &= -c[(x_1,x_2,\ldots,x_n) - (y_1,y_2,\ldots,y_n)]\\
      &= -c(x_1-y_1,\ x_2-y_2,\ \ldots,\ x_n-y_n)\\
      &= [-c(x_1-y_1),\ -c(x_2-y_2),\ \ldots,\ -c(x_n-y_n)]\\
      &= [-cx_1-(-cy_1),\ -cx_2-(-cy_2),\ \ldots,\ -cx_n-(-cy_n)]\\
      &= (-cx_1+cy_1,\ -cx_2+cy_2,\ \ldots,\ -cx_n+cy_n)\\
      &= (-cx_1,-cx_2,\ldots,-cx_n) + (cy_1,cy_2,\ldots,cy_n)\\
      &= -c(x_1,x_2,\ldots,x_n) + c(y_1,y_2,\ldots,y_n)\\
      &= -c(x_1,x_2,\ldots,x_n) - [-c(y_1,y_2,\ldots,y_n)]\\
      &= -c\alpha \oplus -c\beta\\
      &= (c \cdot \alpha) \oplus (c \cdot \beta),
    \end{align*}
    so 4(a) is satisfied.

    For 4(b),
    \begin{align*}
      (c_1 + c_2) \cdot \alpha &= -(c_1 + c_2)\alpha\\
      &= -(c_1 + c_2)(x_1,x_2,\ldots,x_n)\\
      &= [-(c_1 + c_2)x_1,\ -(c_1 + c_2)x_2,\ \ldots,\ -(c_1 + c_2)x_n]\\
      &= [-(c_1 + c_2)x_1,\ -(c_1 + c_2)x_2,\ \ldots,\ -(c_1 + c_2)x_n]\\
      &= (-c_1x_1 - c_2x_1,\ -c_1x_2 - c_2x_2,\ \ldots,\ -c_1x_n - c_2x_n)\\
      &= (-c_1x_1,-c_1x_2,\ldots,-c_1x_n) + (-c_2x_1,-c_2x_2,\ldots,-c_2x_n)\\
      &= -c_1(x_1,x_2,\ldots,x_n) - c_2(x_1,x_2,\ldots,x_n)\\
      &= -c_1\alpha - c_2\alpha\\
      &= -c_1\alpha - [-(-c_2\alpha)]\\
      &= (c_1 \cdot \alpha) - [-(c_2 \cdot \alpha)]\\
      &= (c_1 \cdot \alpha) \oplus -(c_2 \cdot \alpha)\\
      &= (c_1 \cdot \alpha) \oplus (c_2 \cdot \alpha),
    \end{align*}
    so 4(b) is satisfied.

    For 4(c),
    \begin{align*}
      (c_1c_2)\alpha &= -(c_1c_2)\alpha\\
                     &= -(c_1c_2)(x_1,x_2,\ldots,x_n)\\
                     &= (-(c_1c_2)x_1,\ -(c_1c_2)x_2,\ \ldots,\ -(c_1c_2)x_n)\\
                     &= (-c_1c_2x_1,\ -c_1c_2x_2,\ \ldots,\ -c_1c_2x_n)\\
                     &= [(-1)c_1c_2x_1,\ (-1)c_1c_2x_2,\ \ldots,\ (-1)c_1c_2x_n]\\
                     &= [c_1(-1)c_2x_1,\ c_1(-1)c_2x_2,\ \ldots,\ c_1(-1)c_2x_n]\\
                     &= [c_1(-c_2x_1),\ c_1(-c_2x_2),\ \ldots,\ c_1(-c_2x_n)]\\
                     &= c_1(-c_2x_1,\ -c_2x_2,\ \ldots,\ -c_2x_n)\\
                     &= c_1[-c_2(x_1,x_2,\ldots,x_n)]\\
                     &= c_1(-c_2\alpha)\\
                     &= c_1(c_2 \cdot \alpha),
    \end{align*}
    so 4(c) is satisfied.

    For 4(d),
    \begin{align*}
      1 \cdot \alpha &= -1\alpha\\
                     &= -\alpha\\
                     &= \alpha,
    \end{align*}
    so 4(d) is satisfied.

    Thus $(R^{n},\oplus,\cdot)$ is a vector space.

  \item
    Given $V$ as the set of all complex-valued functions $f$ on the real line such
    that (for all $t$ in $\mathbb{R}$)
    \begin{align*}
      f(-t) = \overline{f(t)},
    \end{align*}
    we would like to show that $V$, together with the operations
    \begin{align*}
      (f + g)(t) &= f(t) + g(t)\\
      (cf)(t) &= cf(t)
    \end{align*}
    is a vector space over the field of \textit{real} numbers. We would also like to
    give an example of a function in $V$ which is not real-valued.

    We might as well do the latter first since it's relatively independent of the
    rest of the problem: one such function is
    \begin{align*}
      f(t) &= it.\\
      \shortintertext{For a real number $t$,}
      f(-t) &= -it\\
      \shortintertext{and}
      \overline{f(t)} &= \overline{it} = -it,
    \end{align*}
    so $f(-t) = \overline{f(t)}$.

    This function is arguably real-valued to the extent that $f(0) = -i0 = 0$, since
    zero is a real number. However, if $f(t)$ has any nonzero multiple of $i$ that is
    independent of $t$ on the right side of its definition, its sign will be flipped
    when taking $\overline{f(t)}$, and thus the conjugate of $f(t)$ will not be equal
    to $f(-t)$ since the sign of $t$ will make no difference in that regard. The same
    occurs if we take the absolute value of $t$ and add $1$ or other such strategies;
    if the codomain of $f(t)$ is such that $i$ is always positive, $\overline{f(t)}$
    will always give negative $i$ and vice versa. As such, the sign of $i$
    \textit{must} match the sign of $t$ in the codomain of $f$ for $f$ to fit the
    criteria, and therefore $f(0) = 0$ must be permitted.

    Now, let $f,g,h$ be vectors in $V$.

    For 3(a), since addition in $\mathbb{C}$ is commutative,
    \begin{align*}
      (f + g)(t) &= f(t) + g(t)\\
                 &= g(t) + f(t)\\
                 &= (g + f)(t).
    \end{align*}

    For 3(b), since addition in $\mathbb{C}$ is associative,
    \begin{align*}
      [f + (g + h)](t) &= f(t) + (g + h)(t)\\
                       &= f(t) + g(t) + h(t)\\
                       &= (f + g)(t) + h(t)\\
                       &= [(f + g) + h](t).
    \end{align*}

    For 3(c), the zero vector in $V$ is the function $p(t) = 0$:
    \begin{align*}
      (f + p)(t) &= f(t) + p(t)\\
                 &= f(t) + 0\\
                 &= f(t).
    \end{align*}

    For 3(d), the additive inverse of $f$ is the function $(-f)$ such that $(-f)(t) =
    -f(t)$:
    \begin{align*}
      [f + (-f)](t) &= f(t) + (-f)(t)\\
                    &= f(t) - f(t)\\
                    &= 0\\
                    &= p(t).
    \end{align*}

    Let $c,c_1,c_2$ be scalars in $\mathbb{R}$. Note that $\mathbb{R}$ is a subset of
    $\mathbb{C}$.

    For 4(a),
    \begin{align*}
      c[(f + g)(t)] &= c[f(t) + g(t)]\\
                    &= cf(t) + cg(t)\\
                    &= (cf)(t) + (cg)(t).
    \end{align*}

    For 4(b),
    \begin{align*}
      (c_1 + c_2)f(t) &= c_1f(t) + c_2f(t)\\
                      &= (c_1f)(t) + (c_2f)(t).
    \end{align*}

    For 4(c),
    \begin{align*}
      (c_1c_2)f(t) &= c_1c_2f(t)\\
                   &= c_1(c_2f)(t).
    \end{align*}

    For 4(d),
    \begin{align*}
      1f(t) = f(t),
    \end{align*}
    as $f(t)$ is a complex number and $1(x + iy) = x + iy$ for real numbers $x,y$.

    Now, to return momentarily to $f(t) = it$, the additive inverse of $f(t)$ in $V$
    is $(-f)(t) = -it$, as $[f + (-f)](t) = it - it = 0 = p(t)$. $(-f)(-t) = -(-it) =
    it$, and $\overline{(-f)(t)} = \overline{-it} = it$ as well, so $-f$ is in $V$.
    Aside from that, the membership of $f$ in $V$ follows immediately from $f(t)$
    being a complex number.
\end{enumerate}

\end{document}
