\documentclass[12pt]{article}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fontspec}
\usepackage{xfrac}
\usepackage{array}
\usepackage{siunitx}
\usepackage{gensymb}
\usepackage{enumitem}
\usepackage{dirtytalk}
\usepackage{bm}
\title{Vector spaces}
\author{ZoÃ« Sparks}

\begin{document}

\theoremstyle{definition}

\sisetup{quotient-mode=fraction}
\newtheorem{thm}{Theorem}
\newtheorem*{nthm}{Theorem}
\newtheorem{sthm}{}[thm]
\newtheorem{lemma}{Lemma}[thm]
\newtheorem*{nlemma}{Lemma}
\newtheorem{cor}{Corollary}[thm]
\newtheorem*{prop}{Property}
\newtheorem*{defn}{Definition}
\newtheorem*{comm}{Comment}
\newtheorem*{exm}{Example}

\maketitle

\begin{defn}
  A \textbf{vector space}, also known as a linear space, consists
  of:
  \begin{enumerate}
    \item
      a field $F$ of scalars;
    \item
      a set $V$ of objects called \textbf{vectors};
    \item
      a rule or operation called \textbf{vector addition},
      which associates each pair of vectors $\alpha,\ \beta$ in
      $V$ with a vector $\alpha + \beta$ in $V$, called the sum
      of $\alpha$ and $\beta$, and which fulfills the criteria
      that
      \begin{enumerate}
        \item
          vector addition is commutative: $\alpha + \beta = \beta
          + \alpha$;
        \item
          vector addition is associative: $\alpha + (\beta +
          \gamma) = (\alpha + \beta) + \gamma$;
        \item
          there is a unique vector $0$ in $V$ called the
          \textbf{zero vector} and which is the additive identity
          for vector addition, i.e. $\alpha + 0 = \alpha$ for all
          $\alpha$ in $V$;
        \item
          for each vector $\alpha$ in $V$ there is a unique
          vector $-\alpha$ in $V$ which is the additive inverse
          of $\alpha$, i.e. $\alpha + (-\alpha) = 0$;
      \end{enumerate}
    \item
      a rule or operation called \textbf{scalar multiplication},
      which associates each scalar $c$ in $F$ and vector $\alpha$
      in $V$ with a vector $c\alpha$ in $V$, called the product
      of $c$ and $\alpha$, and which fulfills the criteria that
      \begin{enumerate}
        \item
          $c(\alpha + \beta) = c\alpha + c\beta$;
        \item
           $(c_1 +c_2)\alpha = c_1\alpha + c_2\alpha$;
        \item
          $(c_1c_2)\alpha = c_1(c_2\alpha)$;
        \item
          $1\alpha = \alpha$ for every $\alpha$ in $V$.
      \end{enumerate}
  \end{enumerate}
  Sometimes we will refer to a vector space simply as $V$ if
  there is no chance of confusion about the specific vector space
  under discussion. In some cases, we will need to specify the
  field, in which case we will say that $V$ is a \textbf{vector
  space over the field} $F$.
\end{defn}

\begin{comm}
  It's important to note that a vector space is a
  \textit{composite} object, consisting of a field $F$, a set $V$
  of some kind of objects called \say{vectors,} and two
  operations with certain properties. You may have certain
  preconceived ideas about what constitutes a vector, but it's
  worth putting those aside to an extent if so, as the vectors of
  a given vector space may be quite different from what you
  currently imagine. In particular, the elements of $V$ need have
  no intrinsic relationship to $F$ aside from what's needed to
  define vector addition and scalar multiplication between them.
\end{comm}

\begin{exm}
  \textbf{The \textit{n}-tuple space}, $F^n$. Let $F$ be any
  field, and let $V$ be the set of all $n$-tuples $\alpha =
  (x_1,x_2,\ldots,x_n)$ of scalars $x_i$ in $F$. If $\beta =
  (y_1,y_2,\ldots,y_n)$ with $y_i$ in $F$, the sum of $\alpha$
  and $\beta$ is defined as
  \begin{align*}
    \alpha + \beta = (x_1 + y_1,x_2 + y_2,\ldots,x_n + y_n),
  \end{align*}
  and the product of a scalar $c$ and a vector $\alpha$ is
  defined by
  \begin{align*}
    c\alpha = (cx_1,cx_2,\ldots,cx_n).
  \end{align*}
\end{exm}

\begin{exm}
  \textbf{The space of} $m \times n$ \textbf{matrices}, $F^{m
  \times n}$. Let $F$ be any field and let $m$ and $n$ be
  positive integers. Let $F^{m \times n}$ be the set of all $m
  \times n$ matrices over $F$. The sum of two vectors $A$ and $B$
  in $F^{m \times n}$ is defined as
  \begin{align*}
    (A + B)_{ij} = A_{ij} + B_{ij},
  \end{align*}
  and the product of a scalar $c$ and the matrix $A$ is defined
  by
  \begin{align*}
    (cA)_{ij} = cA_{ij}.
  \end{align*}
  Notably, $F^{1 \times n} = F^n$.
\end{exm}

\begin{exm}
  \textbf{The space of functions from a set into a field.} Let
  $F$ be any field and let $S$ be any non-empty set. Let $V$ be
  the set of all functions from $S$ into $F$. Then the sum of two
  vectors $f$ and $g$ in $V$ is defined as
  \begin{align*}
    (f + g)(s) = f(s) + g(s).
  \end{align*}
  The product of the scalar $c$ and the function (vector) $f$ is
  the function $cf$, defined as
  \begin{align*}
    (cf)(s) = cf(s).
  \end{align*}
\end{exm}

\begin{comm}
  The two preceding examples are, in fact, special cases of this
  one. An $n$-tuple of elements of $F$ can be treated as a
  function from the set $S$ of integers $1,\ldots,n$ into $F$,
  just as we earlier defined an $m \times n$ matrix over $F$ as a
  function from the set $S$ of pairs of integers, $(i,j),\ 1 \leq
  i \leq m,\ 1 \leq j \leq n$, into $F$.
\end{comm}

\begin{exm}
  \textbf{The space of polynomial functions over a field $F$.}
  Let $F$ be a field and let $V$ be the set of all functions $f$
  from $F$ into $F$ which have a rule of the form
  \begin{align*}
    f(x) = c_0 + c_1x + \cdots + c_nx^n,
  \end{align*}
  where $c_0,c_1,\ldots,c_n$ are fixed scalars in $F$ with no
  dependence on $x$. This sort of function is called a
  \textbf{polynomial function on $F$}. Addition and scalar
  multiplication can be defined for this vector space in the same
  manner as the preceding example.
\end{exm}

\begin{exm}
  The field $\mathbb{C}$ of complex numbers can be treated as a
  vector space over the field $\mathbb{R}$ of real numbers. More
  generally, we can let $F$ be the field of \textit{real} numbers
  and $V$ be the set of $n$-tuples $\alpha = (x_1,\ldots,x_n)$
  where $x_1,\ldots,x_n$ are \textit{complex} numbers, defining
  vector addition and scalar multiplication as we did for the
  $n$-tuple space $F^n$, which yields a different sort of vector
  space than either $\mathbb{C}^n$ or $\mathbb{R}^n$.
\end{exm}

\begin{comm}
  If $c$ is a scalar and $0$ is the zero vector, then by 3(c) and
  4(a) we have that
  \begin{align*}
    c0 = c(0 + 0) = c0 + c0;
  \end{align*}
  then by 3(d) we have that
  \begin{align*}
    c0 + c0 - (c0) = c0 - (c0) = c0 = 0.
  \end{align*}
  Likewise, for the scalar $0$ and any vector $\alpha$, we have
  that
  \begin{align*}
    0\alpha = 0,
  \end{align*}
  in which the latter $0$ is the zero vector.

  If $c \neq 0$ is a scalar and $\alpha$ is a vector such that
  $c\alpha = 0$, we then have that $c^{-1}(c\alpha) = c^{-1}(0) =
  0$; this implies that
  \begin{align*}
    c^{-1}(c\alpha) = (c^{-1}c)\alpha = 1\alpha = \alpha = 0.
  \end{align*}

  So, if $c$ is a scalar and $\alpha$ is a vector such that
  $c\alpha = 0$, then either $c = 0$, $\alpha = 0$, or both.
\end{comm}

\begin{comm}
  If $\alpha$ is any vector,
  \begin{align*}
    0 = 0\alpha = (1 - 1)\alpha = 1\alpha + (-1)\alpha = \alpha +
    (-1)\alpha,
  \end{align*}
  and thus
  \begin{align*}
    (-1)\alpha = -\alpha.
  \end{align*}
\end{comm}

\begin{comm}
  Since vector addition is both associative and commutative, we
  have that if $\alpha_1,\alpha_2,\alpha_3,\alpha_4$ are vectors,
  then
  \begin{align*}
    (\alpha_1 + \alpha_2) + (\alpha_3 + \alpha_4) &=\\
    \alpha_4 + [\alpha_2 + (\alpha_1 + \alpha_3)] &=\\
    \alpha_1 + \alpha_2 + \alpha_3 + \alpha_4&
  \end{align*}
  and so on.
\end{comm}

\begin{defn}
  A vector $\beta$ in $V$ is called a \textbf{linear combination}
  of the vectors $\alpha_1,\ldots,\alpha_n$ in $V$ if there are
  scalars $c_1,\ldots,c_n$ in $F$ such that
  \begin{align*}
    \beta &= c_1\alpha_1 + \cdots + c_n\alpha_n\\
          &= \sum_{i = 1}^{n}c_i\alpha_i.
  \end{align*}
\end{defn}

\begin{comm}
  The associativity of vector addition and the distributivities
  4(a) and 4(b) of scalar multiplication imply that
  \begin{align*}
    \sum_{i = 1}^{n}c_i\alpha_i + \sum_{i = 1}^{n}d_i\alpha_i =
    \sum_{i = 1}^{n}(c_i + d_i)\alpha_i
  \end{align*}
  and that
  \begin{align*}
    c\sum_{i = 1}^{n}c_i\alpha_i = \sum_{i =
    1}^{n}(cc_i)\alpha_i.
  \end{align*}
\end{comm}

\begin{defn}
  If $V$ is a vector space over the field $F$, we say that a subset $W$ of $V$ is a
  \textbf{subspace} of $V$ if $W$ is itself a vector space over $F$ with the
  operations of vector addition and scalar multiplication on $V$.
\end{defn}

\begin{comm}
  By the axioms for a vector space, the subset $W$ of $V$ is a subspace if
  \begin{enumerate}
    \item
      for each $\alpha$ and $\beta$ in $W$, the vector $\alpha + \beta$ is in $W$,
    \item
      the $0$ vector is in $W$,
    \item
      for each $\alpha$ in $W$ and each scalar $c$ the vector $c\alpha$ is in $W$,
      and
    \item
      for each $\alpha$ in $W$ the vector $(-\alpha)$ is in $W$.
  \end{enumerate}
  The other axioms determine only the properties of vector addition and scalar
  multiplication in $V$, as opposed to the necessary elements in $V$, and thus are
  automatically satisfied for $W$ as long as the operations themselves stay within
  $W$. For instance, if vector addition is associative and commutative with $V$, it
  is with $W$ as well.

  In practice, it's not necessary to check all of these properties separately to
  prove that $W$ is a subspace of $V$. They imply a simpler method, as stated by the
  following theorem.
\end{comm}

\begin{thm} \label{thm:subspcaplb}
  A non-empty subset $W$ of $V$ is a subspace of $V$ if and only if, for each pair of
  vectors $\alpha,\ \beta$ in $W$ and each scalar $c$ in $F$, the vector $c\alpha +
  \beta$ is again in $W$.

  \begin{proof}
    If $c\alpha + \beta$ is in $W$, then $1\alpha + \beta = \alpha + \beta$ is in
    $W$. Beyond this, because $W$ is non-empty, it must contain some vector $\rho$.
    Since $\rho$ is in $V$, there is an element $-\rho$ in $V$ such that $\rho +
    (-\rho) = 0$. Then because $c\alpha + \beta$ is in $W$, $(-1)\rho + \rho = -\rho
    + \rho = 0$ is in $W$, which implies that $c\alpha + 0 = c\alpha$ is in $W$.
    Finally, because $c\alpha$ is in $W$, $(-1)\alpha = -\alpha$ is in $W$, and thus
    $W$ is a subspace of $V$.

    If $W$ is a subspace of $V$, then naturally $c\alpha + \beta$ is in $W$, as
    vector addition and scalar multiplication with elements of $W$ will remain in
    $W$. For the same reason, if $W$ is not a subspace of $V$, $c\alpha + \beta$
    cannot be guaranteed to be in $W$, and if $c\alpha + \beta$ is not in $W$ despite
    $\alpha,\beta$ being in $W$, $W$ cannot be a subspace of $V$ for the same reason.
  \end{proof}
\end{thm}

\begin{exm}
  If $V$ is any vector space, $V$ is a subspace of $V$.
\end{exm}

\begin{exm}
  In $F^{n}$, the set of $n$-tuples $(x_1,\ldots,x_n)$ with $x_1 = 0$ is a subspace.
  However, the set of $n$-tuples with $x_1 = 1 + x_2$ is not a subspace ($n \geq 2$
  of course).
\end{exm}

\begin{exm}
  The space of polynomial functions over a field $F$ is a subspace of the space of
  all functions from $F$ into $F$.
\end{exm}

\begin{exm}
  The symmetric matrices form a subspace of the space of all $n \times n$ matrices
  over $F$.
\end{exm}

\begin{exm}
  The set of all Hermitian matrices is \textit{not} a subspace of the space of all $n
  \times n$ complex matrices; if $A$ is a Hermitian matrix, its diagonal entries
  $A_{11},A_{22},\ldots,$ are all real, but the diagonal entries of $iA$ generally
  are not. That said, the set of $n \times n$ complex Hermitian matrices with the
  usual operations is a vector space over the real numbers.
\end{exm}

\begin{exm}
  If $A$ is an $m \times n$ matrix over $F$, the set of all $n \times 1$ (column)
  matrices $X$ over $F$ such that $AX = 0$, i.e. the solution space of the system of
  homogeneous linear equations $AX = 0$, is a subspace of all $n \times 1$ matrices
  over $F$. This follows from the following lemma.

  \begin{nlemma}
    If $A$ is an $m \times n$ matrix over $F$, and $B$ and $C$ are $n \times p$
    matrices over $F$, then
    \begin{align*}
      A(dB + C) = d(AB) + AC
    \end{align*}
    for each scalar $d$ in $F$.

    \begin{proof}
      \begin{align*}
        [A(dB + C)]_{ij} &= \sum_{r = 1}^{n}A_{ir}(dB + C)_{rj}\\
        &= \sum_{r = 1}^{n}A_{ir}(dB_{rj} + C_{rj})\\
        &= \sum_{r = 1}^{n}(dA_{ir}B_{rj} + A_{ir}C_{rj})\\
        &= \sum_{r = 1}^{n}dA_{ir}B_{rj} + \sum_{r = 1}^{n}A_{ir}C_{rj}\\
        &= d\sum_{r = 1}^{n}A_{ir}B_{rj} + \sum_{r = 1}^{n}A_{ir}C_{rj}\\
        &= d(AB)_{ij} + (AC)_{ij}.
      \end{align*}
    \end{proof}
  \end{nlemma}

  Because this is true, we know that $A(cX + Y) = 0$ if $AX = 0$, $AY = 0$, and $c$
  is any scalar in $F$.
\end{exm}

\begin{defn}
  If $V$ is any vector space, the subset consisting of the zero vector of $V$ by
  itself is a subspace of $V$ called its \textbf{zero subspace}.
\end{defn}

\begin{thm}
  If $V$ is a vector space over the field $F$, then the intersection of any
  collection of subspaces of $V$ is itself a subspace of $V$.

  \begin{proof}
    Let $\{W_n\}$ be a set of subspaces of $V$. Because any subspace contains the
    zero vector $0$, $0 \in \bigcap_{n}W_n$, so $\bigcap_{n}W_n$ is non-empty. Thus
    let $\alpha,\beta$ be vectors in $\bigcap_{n}W_n$. Because the sum of vectors
    within a subspace is itself within that subspace, $\alpha + \beta$ must be within
    all of $W_n$ simultaneously, so $(\alpha + \beta) \in \bigcap_{n}W_n$. Because
    the product of a scalar and a vector within a subspace is itself within that
    subspace, if $c$ is a scalar in $F$, $c\alpha \in \bigcap_{n}W_n$. Therefore,
    $(c\alpha + \beta) \in \bigcap_{n}W_n$, so $\bigcap_{n}W_n$ is a subspace of $V$
    by theorem \ref{thm:subspcaplb}.
  \end{proof}
\end{thm}

\begin{defn}
  If $S$ is a set of vectors in a vector space $V$, we say that the \textbf{subspace
  spanned by $S$} is the interesction $W$ of all the subspaces of $V$ which contain
  $S$; i.e., if ${W_n}$ is the set of all subspaces of $V$ such that $S \in \{W_n\}$,
  then $W = \bigcap_{n}W_n$.

  If $S$ is a finite set of vectors, $S = \{\alpha_1,\alpha_2,\ldots,\alpha_n\}$, we
  can simply call $W$ the \textbf{subspace spanned by the vectors
  $\alpha_1,\alpha_2,\ldots,\alpha_n$}.
\end{defn}

\begin{comm}
  Note that if $\alpha$ is a vector in a vector space $V$, the subspace spanned by
  $\alpha$ is the smallest subspace in $V$ which contains $\alpha$.
\end{comm}

\begin{thm}
  The subspace $W$ spanned by a non-empty subset $S$ of a vector space $V$ over a
  field $F$ is the set of all linear combinations of vectors in $S$.

  \begin{proof}
    A linear combination of vectors in $S$ is a vector $\sum_{n}c_n\alpha_n$ where
    $c_n \in F,\ \alpha_n \in S$. Let $L$ be the set of all linear combinations of
    vectors in $S$; then $L \in W$, because $S \in W$ and the vectors given by vector
    addition and scalar multiplication of vectors in a subspace are also in that
    subspace.

    Consider vectors $\alpha,\beta \in L$. $\alpha = \sum_{o}c_o\gamma_o$ and $\beta
    = \sum_{p}d_p\delta_p$, where $c_o,d_p \in F$ and $\gamma_o,\delta_p \in S$. A
    vector $\epsilon = e\alpha + \beta,\ e \in F$ is then
    \begin{align*}
      e\sum_{o}c_o\gamma_o + \sum_{p}d_p\delta_p &=
      \sum_{o}(ec_o)\gamma_o + \sum_{p}d_p\delta_p,
    \end{align*}
    which is itself a linear combination of vectors in $S$, so $\epsilon \in L$ and
    thus $L$ is a subspace of $V$.

    Therefore $L$ must be a subset of any subspace of $V$ that contains $S$. Since
    $c\alpha + \beta,\ c \in F,\ \alpha,\beta \in S$ is a linear combination of
    vectors in $S$ and thus is in $L$, this implies that $L$ is the interesction of
    all subspaces of $V$ that contain $S$, and thus $W = L$.
  \end{proof}
\end{thm}

\begin{defn}
  If $S_1,S_2,\ldots,S_k$ are subsets of a vector space $V$, the set of all sums
  \begin{align*}
    \alpha_1 + \alpha_2 + \cdots + \alpha_k
  \end{align*}
  of vectors $\alpha_i \in S_i$ is called the \textbf{sum} of the subsets
  $S_1,S_2,\ldots,S_k$, and is denoted by
  \begin{align*}
    S_1 + S_2 + \cdots + S_k
  \end{align*}
  or by
  \begin{align*}
    \sum_{i = 1}^{k}Si.
  \end{align*}
\end{defn}

\begin{comm}
  If $W_1,W_2,\ldots,W_k$ are subspaces of $V$, the sum
  \begin{align*}
    W = \sum_{i = 1}^{k}W_i
  \end{align*}
  is of course a subspace of $V$ which contains each subspace $W_i$, and thus
  $W$ is the subspace spanned by $\bigcup_{i = 1}^{k}W_i$.
\end{comm}

\begin{exm}
  Let $F$ be a subfield of $\mathbb{C}$. Say we have
  \begin{align*}
    \alpha_1 &= (1,2,0,3,0)\\
    \alpha_2 &= (0,0,1,4,0)\\
    \alpha_3 &= (0,0,0,0,1).
  \end{align*}
  We denote the subspace spanned by $\alpha_1,\alpha_2,\alpha_3$ as $W$. Then a
  vector $\alpha$ is in $W$ if and only if there are scalars $c_1,c_2,c_3 \in F$ such
  that
  \begin{align*}
    \alpha &= c_1\alpha_1 + c_2\alpha_2 + c_3\alpha_3.
  \end{align*}
  Therefore $W$ consists of all the vectors $\alpha$ such that
  \begin{align*}
    \alpha &= (c_1,\ 2c_1,\ c_2,\ 3c_1+4c_2,\ c_3),
  \end{align*}
  where $c_1,c_2,c_3$ are arbitrary scalars in $F$. We can also describe $W$ as the
  set of all 5-tuples
  \begin{align*}
    \alpha &= (x_1,x_2,x_3,x_4,x_5)
  \end{align*}
  where $x_i \in F$ and
  \begin{align*}
    x_2 &= 2x_1\\
    x_4 &= 3x_1 + 4x_3.
  \end{align*}
  Therefore $(-3,-6,1,-5,2)$ is in $W$, while $(2,4,6,7,8)$ is not.
\end{exm}

\begin{exm}
  Let $F$ be a subfield of $\mathbb{C}$, and let $V$ be the vector space of all $2
  \times 2$ matrices over $F$. Let $W_1$ be the subset of $V$ consisting of all
  matrices of the form
  \begin{align*}
    \begin{bmatrix}
      x & y\\
      z & 0
    \end{bmatrix},
  \end{align*}
  where $x,y,z$ are arbitrary scalars in $F$. Finally, let $W_2$ be the subset of $V$
  consisting of all matrices of the form
  \begin{align*}
    \begin{bmatrix}
      x & 0\\
      0 & y
    \end{bmatrix},
  \end{align*}
  where $x$ and $y$ are arbitrary scalars in $F$. Then $W_1$ and $W_2$ are subspaces
  of $V$, and also
  \begin{align*}
    V = W_1 + W_2,
  \end{align*}
  because
  \begin{align*}
    \begin{bmatrix}
      a & b\\
      c & d
    \end{bmatrix}
    =
    \begin{bmatrix}
      a & b\\
      c & 0
    \end{bmatrix}
    +
    \begin{bmatrix}
      0 & 0\\
      0 & d
    \end{bmatrix}.
  \end{align*}
  However, the subspace $W_1 \cap W_2$ consists of all matrices of the form
  \begin{align*}
    \begin{bmatrix}
      x & 0\\
      0 & 0
    \end{bmatrix}.
  \end{align*}
\end{exm}

\begin{defn}
  If $A$ is an $m \times n$ matrix over a field $F$, the \textbf{row vectors} of $A$
  are the vectors in $F^{n}$ given by $\alpha_i = (A_{i1},\ldots,A_{in}),\ i =
  1,\ldots,m$.
\end{defn}

\end{document}
