\documentclass[12pt]{article}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fontspec}
\usepackage{xfrac}
\usepackage{array}
\usepackage{siunitx}
\usepackage{gensymb}
\usepackage{enumitem}
\usepackage{dirtytalk}
\usepackage{bm}
\title{Invertible matrices (exercises)}
\author{ZoÃ« Sparks}

\begin{document}

\theoremstyle{definition}

\sisetup{quotient-mode=fraction}
\newtheorem{thm}{Theorem}
\newtheorem*{nthm}{Theorem}
\newtheorem{sthm}{}[thm]
\newtheorem{lemma}{Lemma}[thm]
\newtheorem*{nlemma}{Lemma}
\newtheorem{cor}{Corollary}[thm]
\newtheorem*{prop}{Property}
\newtheorem*{defn}{Definition}
\newtheorem*{comm}{Comment}
\newtheorem*{exm}{Example}

\maketitle

\begin{enumerate}
  \item We have
    \begin{align*}
      A =
      \begin{bmatrix}
        1  & 2  & 1 & 0\\
        -1 & 0  & 3 & 5\\
        1  & -2 & 1 & 1\\
      \end{bmatrix}.
    \end{align*}
    We would like to find a row-reduced echelon matrix $R$ which
    is row-equivalent to $A$ and an invertible $3 \times 3$
    matrix $P$ such that $R = PA$.
    \begin{align*}
      \begin{bmatrix}
        1  & 2  & 1 & 0\\
        -1 & 0  & 3 & 5\\
        1  & -2 & 1 & 1\\
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1\\
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1  & 2  & 1 & 0\\
        0  & 2  & 4 & 5\\
        1  & -2 & 1 & 1\\
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0 & 0\\
        1 & 1 & 0\\
        0 & 0 & 1\\
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1  & 2  & 1 & 0\\
        0  & 2  & 4 & 5\\
        0  & -4 & 0 & 1\\
      \end{bmatrix},\
      \begin{bmatrix}
        1  & 0 & 0\\
        1  & 1 & 0\\
        -1 & 0 & 1\\
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1  & 0  & -3 & -5\\
        0  & 2  & 4  & 5\\
        0  & -4 & 0  & 1\\
      \end{bmatrix},\
      \begin{bmatrix}
        0  & -1 & 0\\
        1  & 1  & 0\\
        -1 & 0  & 1\\
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1  & 0  & -3 & -5\\
        0  & 2  & 4  & 5\\
        0  & 0  & 8  & 11\\
      \end{bmatrix},\
      \begin{bmatrix}
        0  & -1 & 0\\
        1  & 1  & 0\\
        1  & 2  & 1\\
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1  & 0  & -3 & -5\\
        0  & 1  & 2  & \frac{5}{2}\\
        0  & 0  & 8  & 11\\
      \end{bmatrix},\
      \begin{bmatrix}
        0  & -1 & 0\\
        \frac{1}{2} & \frac{1}{2} & 0\\
        1 & 2  & 1\\
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1  & 0  & -3 & -5\\
        0  & 1  & 2  & \frac{5}{2}\\
        0  & 0  & 1  & \frac{11}{8}\\
      \end{bmatrix},\
      \begin{bmatrix}
        0  & -1 & 0\\
        \frac{1}{2} & \frac{1}{2} & 0\\
        \frac{1}{8} & \frac{1}{4} & \frac{1}{8}\\
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1  & 0  & -3 & -5\\
        0  & 1  & 0  & -\frac{1}{4}\\
        0  & 0  & 1  & \frac{11}{8}\\
      \end{bmatrix},\
      \begin{bmatrix}
        0  & -1 & 0\\
        \frac{1}{4} & 0 & -\frac{1}{4}\\
        \frac{1}{8} & \frac{1}{4} & \frac{1}{8}\\
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1  & 0  & 0  & -\frac{7}{8}\\
        0  & 1  & 0  & -\frac{1}{4}\\
        0  & 0  & 1  & \frac{11}{8}\\
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{3}{8}  & -\frac{1}{4} & \frac{3}{8}\\
        \frac{1}{4} & 0 & -\frac{1}{4}\\
        \frac{1}{8} & \frac{1}{4} & \frac{1}{8}\\
      \end{bmatrix}.
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1  & 0  & 0  & -\frac{7}{8}\\
        0  & 1  & 0  & -\frac{1}{4}\\
        0  & 0  & 1  & \frac{11}{8}\\
      \end{bmatrix}
      =
      \begin{bmatrix}
        \frac{3}{8}  & -\frac{1}{4} & \frac{3}{8}\\
        \frac{1}{4} & 0 & -\frac{1}{4}\\
        \frac{1}{8} & \frac{1}{4} & \frac{1}{8}\\
      \end{bmatrix}
      \begin{bmatrix}
        1  & 2  & 1 & 0\\
        -1 & 0  & 3 & 5\\
        1  & -2 & 1 & 1\\
      \end{bmatrix}.
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        \frac{3}{8}  & -\frac{1}{4} & \frac{3}{8}\\
        \frac{1}{4} & 0 & -\frac{1}{4}\\
        \frac{1}{8} & \frac{1}{4} & \frac{1}{8}\\
      \end{bmatrix}
    \end{align*}
    is row-equivalent to $I$ as we have seen and is thus
    invertible, so
    \begin{align*}
      R =
      \begin{bmatrix}
        1  & 0  & 0  & -\frac{7}{8}\\
        0  & 1  & 0  & -\frac{1}{4}\\
        0  & 0  & 1  & \frac{11}{8}\\
      \end{bmatrix}\ \text{and}\
      P =
      \begin{bmatrix}
        \frac{3}{8}  & -\frac{1}{4} & \frac{3}{8}\\
        \frac{1}{4} & 0 & -\frac{1}{4}\\
        \frac{1}{8} & \frac{1}{4} & \frac{1}{8}\\
      \end{bmatrix}.
    \end{align*}

  \item
    We have
    \begin{align*}
      A =
      \begin{bmatrix}
        2 & 0 & i\\
        1 & -3 & -i\\
        i & 1 & 1
      \end{bmatrix},
    \end{align*}
    and we'd like to do the same thing with it that we did with
    $A$ in (1).
    \begin{align*}
      \begin{bmatrix}
        2 & 0 & i\\
        1 & -3 & -i\\
        i & 1 & 1
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & \frac{1}{2}i\\
        1 & -3 & -i\\
        i & 1 & 1
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{2} & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & \frac{1}{2}i\\
        0 & -3 & -\frac{3}{2}i\\
        i & 1 & 1
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{2} & 0 & 0\\
        -\frac{1}{2} & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & \frac{1}{2}i\\
        0 & -3 & -\frac{3}{2}i\\
        0 & 1 & \frac{3}{2}
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{2} & 0 & 0\\
        -\frac{1}{2} & 1 & 0\\
        -\frac{1}{2}i & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & \frac{1}{2}i\\
        0 & 1 & \frac{1}{2}i\\
        0 & 1 & \frac{3}{2}
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{2} & 0 & 0\\
        \frac{1}{6} & -\frac{1}{3} & 0\\
        -\frac{1}{2}i & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & \frac{1}{2}i\\
        0 & 1 & \frac{1}{2}i\\
        0 & 0 & \frac{3}{2}-\frac{1}{2}i
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{2} & 0 & 0\\
        \frac{1}{6} & -\frac{1}{3} & 0\\
        -\frac{1}{6}-\frac{1}{2}i & \frac{1}{3} & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & \frac{1}{2}i\\
        0 & 1 & \frac{1}{2}i\\
        0 & 0 & 1
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{2} & 0 & 0\\
        \frac{1}{6} & -\frac{1}{3} & 0\\
        -\frac{1}{3}i & \frac{1}{5}+\frac{1}{15}i & \frac{3}{5}+\frac{1}{5}i
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & \frac{1}{2}i\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{2} & 0 & 0\\
        0 & -\frac{3}{10}-\frac{1}{10}i & \frac{1}{10}-\frac{3}{10}i\\
        -\frac{1}{3}i & \frac{1}{5}+\frac{1}{15}i & \frac{3}{5}+\frac{1}{5}i
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{3} & \frac{1}{30}-\frac{1}{10}i & \frac{1}{10}-\frac{3}{10}i\\
        0 & -\frac{3}{10}-\frac{1}{10}i & \frac{1}{10}-\frac{3}{10}i\\
        -\frac{1}{3}i & \frac{1}{5}+\frac{1}{15}i & \frac{3}{5}+\frac{1}{5}i
      \end{bmatrix}.
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix}=
      \begin{bmatrix}
        \frac{1}{3} & \frac{1}{30}-\frac{1}{10}i & \frac{1}{10}-\frac{3}{10}i\\
        0 & -\frac{3}{10}-\frac{1}{10}i & \frac{1}{10}-\frac{3}{10}i\\
        -\frac{1}{3}i & \frac{1}{5}+\frac{1}{15}i & \frac{3}{5}+\frac{1}{5}i
      \end{bmatrix}
      \begin{bmatrix}
        2 & 0 & i\\
        1 & -3 & -i\\
        i & 1 & 1
      \end{bmatrix},
    \end{align*}
    so
    \begin{align*}
      R =
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix}\ \text{and}\
      P =
      \begin{bmatrix}
        \frac{1}{3} & \frac{1}{30}-\frac{1}{10}i & \frac{1}{10}-\frac{3}{10}i\\
        0 & -\frac{3}{10}-\frac{1}{10}i & \frac{1}{10}-\frac{3}{10}i\\
        -\frac{1}{3}i & \frac{1}{5}+\frac{1}{15}i & \frac{3}{5}+\frac{1}{5}i
      \end{bmatrix}.
    \end{align*}

  \item
    We have
    \begin{align*}
      A =
      \begin{bmatrix}
        2 & 5 & -1\\
        4 & -1 & 2\\
        6 & 4 & 1
      \end{bmatrix}\ \text{and}\
      B =
      \begin{bmatrix}
        1 & -1 & 2\\
        3 & 2 & 4\\
        0 & 1 & -2
      \end{bmatrix}.
    \end{align*}
    We would like to determine if either is invertible and what
    its inverse is if so.
    \begin{align*}
      \begin{bmatrix}
        2 & 5 & -1\\
        4 & -1 & 2\\
        6 & 4 & 1
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & \frac{5}{2} & -\frac{1}{2}\\
        4 & -1 & 2\\
        6 & 4 & 1
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{2} & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & \frac{5}{2} & -\frac{1}{2}\\
        0 & -11 & 4\\
        6 & 4 & 1
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{2} & 0 & 0\\
        -2 & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & \frac{5}{2} & -\frac{1}{2}\\
        0 & -11 & 4\\
        0 & -11 & 4
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{2} & 0 & 0\\
        -2 & 1 & 0\\
        -3 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & \frac{5}{2} & -\frac{1}{2}\\
        0 & -11 & 4\\
        0 & 0 & 0
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{2} & 0 & 0\\
        -2 & 1 & 0\\
        -1 & -1 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & \frac{5}{2} & -\frac{1}{2}\\
        0 & 1 & -\frac{4}{11}\\
        0 & 0 & 0
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{2} & 0 & 0\\
        \frac{2}{11} & -\frac{1}{11} & 0\\
        -1 & -1 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & \frac{9}{22}\\
        0 & 1 & -\frac{4}{11}\\
        0 & 0 & 0
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{22} & \frac{5}{22} & 0\\
        \frac{2}{11} & -\frac{1}{11} & 0\\
        -1 & -1 & 1
      \end{bmatrix},
    \end{align*}
    so $A$ is not invertible.
    \begin{align*}
      \begin{bmatrix}
        1 & -1 & 2\\
        3 & 2 & 4\\
        0 & 1 & -2
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & -1 & 2\\
        0 & 5 & -2\\
        0 & 1 & -2
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0 & 0\\
        -3 & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & -1 & 2\\
        0 & 1 & -\frac{2}{5}\\
        0 & 1 & -2
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0 & 0\\
        -\frac{3}{5} & \frac{1}{5} & 0\\
        0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & -1 & 2\\
        0 & 1 & -\frac{2}{5}\\
        0 & 0 & -\frac{8}{5}
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0 & 0\\
        -\frac{3}{5} & \frac{1}{5} & 0\\
        \frac{3}{5} & -\frac{1}{5} & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & \frac{8}{5}\\
        0 & 1 & -\frac{2}{5}\\
        0 & 0 & -\frac{8}{5}
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{2}{5} & \frac{1}{5} & 0\\
        -\frac{3}{5} & \frac{1}{5} & 0\\
        \frac{3}{5} & -\frac{1}{5} & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & -\frac{2}{5}\\
        0 & 0 & -\frac{8}{5}
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0 & 1\\
        -\frac{3}{5} & \frac{1}{5} & 0\\
        \frac{3}{5} & -\frac{1}{5} & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & -\frac{2}{5}\\
        0 & 0 & 1
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0 & 1\\
        -\frac{3}{5} & \frac{1}{5} & 0\\
        -\frac{3}{8} & \frac{1}{8} & -\frac{5}{8}
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0 & 1\\
        -\frac{3}{4} & \frac{1}{4} & -\frac{1}{4}\\
        -\frac{3}{8} & \frac{1}{8} & -\frac{5}{8}
      \end{bmatrix},
    \end{align*}
    so $B$ is invertible and its inverse is
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 1\\
        -\frac{3}{4} & \frac{1}{4} & -\frac{1}{4}\\
        -\frac{3}{8} & \frac{1}{8} & -\frac{5}{8}
      \end{bmatrix}.
    \end{align*}

  \item
    We have
    \begin{align*}
      A =
      \begin{bmatrix}
        5 & 0 & 0\\
        1 & 5 & 0\\
        0 & 1 & 5
      \end{bmatrix}.
    \end{align*}
    We would like to know for which $X$ there is a scalar $c$
    such that $AX = cX$. We know that if $R$ is a row-reduced
    echelon matrix that is row-equivalent to $A$, then $R = PA$
    where $P$ is a $3 \times 3$ invertible matrix. The solutions
    of $AX = cX$ are then the same as the solutions of $RX = PAX
    = PcX$.
    \begin{align*}
      \begin{bmatrix}
        5 & 0 & 0\\
        1 & 5 & 0\\
        0 & 1 & 5
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0\\
        1 & 5 & 0\\
        0 & 1 & 5
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{5} & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 5 & 0\\
        0 & 1 & 5
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{5} & 0 & 0\\
        -\frac{1}{5} & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 1 & 5
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{5} & 0 & 0\\
        -\frac{1}{25} & \frac{1}{5} & 0\\
        0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 5
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{5} & 0 & 0\\
        -\frac{1}{25} & \frac{1}{5} & 0\\
        \frac{1}{25} & -\frac{1}{5} & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{5} & 0 & 0\\
        -\frac{1}{25} & \frac{1}{5} & 0\\
        \frac{1}{125} & -\frac{1}{25} & \frac{1}{5}
      \end{bmatrix}.
    \end{align*}
    Therefore $R = I$ and
    \begin{align*}
      P =
      \begin{bmatrix}
        \frac{1}{5} & 0 & 0\\
        -\frac{1}{25} & \frac{1}{5} & 0\\
        \frac{1}{125} & -\frac{1}{25} & \frac{1}{5}
      \end{bmatrix}.
    \end{align*}
    If
    \begin{align*}
      cX =
      \begin{bmatrix}
        cx_1\\
        cx_2\\
        cx_3\\
      \end{bmatrix},
    \end{align*}
    then
    \begin{align*}
      PcX =
      \begin{bmatrix}
        \frac{1}{5} & 0 & 0\\
        -\frac{1}{25} & \frac{1}{5} & 0\\
        \frac{1}{125} & -\frac{1}{25} & \frac{1}{5}
      \end{bmatrix}
      \begin{bmatrix}
        cx_1\\
        cx_2\\
        cx_3\\
      \end{bmatrix}
      =
      \begin{bmatrix}
        \frac{1}{5}cx_1\\
        -\frac{1}{25}cx_1+\frac{1}{5}cx_2\\
        \frac{1}{125}cx_1-\frac{1}{25}cx_2+\frac{1}{5}cx_3\\
      \end{bmatrix}.
    \end{align*}
    $RX = IX = X = PcX$. Since this implies that
    \begin{align*}
      x_1 =&\ \frac{1}{5}cx_1,
    \end{align*}
    $c$ has to be $5$ or $x_1$ has to be $0$. If we assume that
    $c = 5$,
    \begin{align*}
      x_2 =&\ -\frac{1}{5}x_1 + x_2,
    \end{align*}
    which is only possible if $x_1 = 0$, so $x_1 = 0$. Then
    \begin{align*}
      x_2 =&\ \frac{1}{5}cx_2,
    \end{align*}
    which implies that $c$ has to be $5$ or $x_2$ has to be $0$.
    If we assume that $c = 5$,
    \begin{align*}
      x_3 =&\ -\frac{1}{5}x_2 + x_3,
    \end{align*}
    which again is only possible if $x_2 = 0$, so $x_2 = 0$ as
    well. Then
    \begin{align*}
      x_3 =&\ \frac{1}{5}cx_3,
    \end{align*}
    which is valid if $c = 5$ or if $x_3 = 0$.

    Therefore, $AX = cX$ when $X = 0$ or when
    \begin{align*}
      c = 5,\ X =
      \begin{bmatrix}
        0\\
        0\\
        x_3\\
      \end{bmatrix},
    \end{align*}
    where $x_3$ can take on any value in the field.

  \item
    We have
    \begin{align*}
      A =
      \begin{bmatrix}
        1 & 2 & 3 & 4\\
        0 & 2 & 3 & 4\\
        0 & 0 & 3 & 4\\
        0 & 0 & 0 & 4
      \end{bmatrix}.
    \end{align*}
    We would like to know if $A$ is invertible, and what $A^{-1}$
    is if so.

    $A$ seems obviously invertible at first glance, so let's find
    the inverse.
    \begin{align*}
      \begin{bmatrix}
        1 & 2 & 3 & 4\\
        0 & 2 & 3 & 4\\
        0 & 0 & 3 & 4\\
        0 & 0 & 0 & 4
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 2 & 3 & 4\\
        0 & 0 & 3 & 4\\
        0 & 0 & 0 & 4
      \end{bmatrix},\
      \begin{bmatrix}
        1 & -1 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 2 & 0 & 0\\
        0 & 0 & 3 & 4\\
        0 & 0 & 0 & 4
      \end{bmatrix},\
      \begin{bmatrix}
        1 & -1 & 0 & 0\\
        0 & 1 & -1 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 2 & 0 & 0\\
        0 & 0 & 3 & 0\\
        0 & 0 & 0 & 4
      \end{bmatrix},\
      \begin{bmatrix}
        1 & -1 & 0 & 0\\
        0 & 1 & -1 & 0\\
        0 & 0 & 1 & -1\\
        0 & 0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 3 & 0\\
        0 & 0 & 0 & 4
      \end{bmatrix},\
      \begin{bmatrix}
        1 & -1 & 0 & 0\\
        0 & \frac{1}{2} & -\frac{1}{2} & 0\\
        0 & 0 & 1 & -1\\
        0 & 0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 4
      \end{bmatrix},\
      \begin{bmatrix}
        1 & -1 & 0 & 0\\
        0 & \frac{1}{2} & -\frac{1}{2} & 0\\
        0 & 0 & \frac{1}{3} & -\frac{1}{3}\\
        0 & 0 & 0 & 1
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1
      \end{bmatrix},\
      \begin{bmatrix}
        1 & -1 & 0 & 0\\
        0 & \frac{1}{2} & -\frac{1}{2} & 0\\
        0 & 0 & \frac{1}{3} & -\frac{1}{3}\\
        0 & 0 & 0 & \frac{1}{4}
      \end{bmatrix}.
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 2 & 3 & 4\\
        0 & 2 & 3 & 4\\
        0 & 0 & 3 & 4\\
        0 & 0 & 0 & 4
      \end{bmatrix}
      \begin{bmatrix}
        1 & -1 & 0 & 0\\
        0 & \frac{1}{2} & -\frac{1}{2} & 0\\
        0 & 0 & \frac{1}{3} & -\frac{1}{3}\\
        0 & 0 & 0 & \frac{1}{4}
      \end{bmatrix}
      =&\\
      \begin{bmatrix}
        1 & -1 & 0 & 0\\
        0 & \frac{1}{2} & -\frac{1}{2} & 0\\
        0 & 0 & \frac{1}{3} & -\frac{1}{3}\\
        0 & 0 & 0 & \frac{1}{4}
      \end{bmatrix}
      \begin{bmatrix}
        1 & 2 & 3 & 4\\
        0 & 2 & 3 & 4\\
        0 & 0 & 3 & 4\\
        0 & 0 & 0 & 4
      \end{bmatrix}
      =&\\
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1
      \end{bmatrix},
    \end{align*}
    so $A$ is invertible and
    \begin{align*}
      A^{-1} =
      \begin{bmatrix}
        1 & -1 & 0 & 0\\
        0 & \frac{1}{2} & -\frac{1}{2} & 0\\
        0 & 0 & \frac{1}{3} & -\frac{1}{3}\\
        0 & 0 & 0 & \frac{1}{4}
      \end{bmatrix}.
    \end{align*}

  \item
    We would like to prove that, if $A$ is a $2 \times 1$ matrix
    and $B$ is a $1 \times 2$ matrix, $C = AB$ is not invertible.

    If
    \begin{align*}
      A =
      \begin{bmatrix}
        a_1\\
        a_2\\
      \end{bmatrix}\ \text{and}\
      B =
      \begin{bmatrix}
        b_1 & b_2
      \end{bmatrix},
    \end{align*}
    then
    \begin{align*}
      C =
      \begin{bmatrix}
        a_1\\
        a_2\\
      \end{bmatrix}
      \begin{bmatrix}
        b_1 & b_2
      \end{bmatrix}
      =
      \begin{bmatrix}
        a_1b_1 & a_1b_2\\
        a_2b_1 & a_2b_2\\
      \end{bmatrix}.
    \end{align*}
    For $C$ to be invertible, it would need to be row-equivalent
    to the $2 \times 2$ identity matrix by theorem 12. However,
    \begin{align*}
      \begin{bmatrix}
        a_1b_1 & a_1b_2\\
        a_2b_1 & a_2b_2\\
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0\\
        0 & 1\\
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & \frac{b_2}{b_1}\\
        a_2b_1 & a_2b_2\\
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{a_1b_1} & 0\\
        0 & 1\\
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & \frac{b_2}{b_1}\\
        0 & 0\\
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{a_1b_1} & 0\\
        -\frac{a_2}{a_1} & 1\\
      \end{bmatrix}.
    \end{align*}
    Therefore, $C$ is not row-equivalent to the $2 \times 2$
    identity matrix by theorem 4, so $C$ is not invertible.

  \item
    If $A$ is an $n \times n$ matrix, we would like to prove that
    \begin{enumerate}
      \item if $A$ is invertible and $AB = 0$ for some $n \times
        n$ matrix $B$, then $B = 0$, and
      \item if $A$ is not invertible, then there exists an $n
        \times n$ matrix $B$ such that $AB = 0$ but $B \neq 0$.
    \end{enumerate}

    By theorem 13, if $A$ is invertible, $AX = 0$, where $X$ is
    an $n \times 1$ matrix, has only the trivial solution $X =
    0$. For $AB = 0$, each individual column of $B$ can be taken
    separately in an equation $AB_j = 0$, $1 \leq j \leq n$, to
    find the values of $B$. By theorem 13, this can only result
    in values of $0$ for each column of $B$, and thus $B = 0$ as
    a whole.

    Also by theorem 13, if $A$ is not invertible, the homogeneous
    system $AX = 0$ has a solution other than $X = 0$. By the
    same argument, this implies that $AB = 0$ as a whole has a
    solution other than $B = 0$.

  \item
    If
    \begin{align*}
      A =
      \begin{bmatrix}
        a & b\\
        c & d\\
      \end{bmatrix},
    \end{align*}
    we would like to prove, using elementary row operations, that
    $A$ is invertible if and only if $(ad - bc) \neq 0$.
    \begin{align*}
      \begin{bmatrix}
        a & b\\
        c & d\\
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0\\
        0 & 1\\
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & \frac{b}{a}\\
        c & d\\
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{a} & 0\\
        0 & 1\\
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & \frac{b}{a}\\
        0 & \frac{ad - bc}{a}\\
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{a} & 0\\
        -\frac{c}{a} & 1\\
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & \frac{b}{a}\\
        0 & 1\\
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{1}{a} & 0\\
        -\frac{c}{ad - bc}
          & \frac{a}{ad - bc}\\
      \end{bmatrix}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0\\
        0 & 1\\
      \end{bmatrix},\
      \begin{bmatrix}
        \frac{d}{ad - bc}
          & -\frac{b}{ad - bc}\\
        -\frac{c}{ad - bc}
          & \frac{a}{ad - bc}\\
      \end{bmatrix}.
    \end{align*}
    Therefore,
    \begin{align*}
      A^{-1} =
      \begin{bmatrix}
        \frac{d}{ad - bc}
          & -\frac{b}{ad - bc}\\
        -\frac{c}{ad - bc}
          & \frac{a}{ad - bc}\\
      \end{bmatrix}.
    \end{align*}
    Since the entries of $A^{-1}$ are only defined if $(ad - bc)
    \neq 0$, $A$ is only invertible if this is the case.

  \item
    We would like to prove that an upper-triangular (square)
    matrix $A$ is invertible if and only if every entry on its
    main diagonal is different from $0$.

    For $A$ to be invertible, it must be row-equivalent to $I$.
    However, if $A$ has at least one $0$ entry on its main
    diagonal, there is no set of elementary row operations that
    would map $I$ to $A$.

    $I$ has every entry on its main diagonal equal to $1$ and all
    other entries equal to $0$. The only set of elementary row
    operations that would map one of the entries on its main
    diagonal to $0$ would be to add that entry's row to another
    and then to subtract that row from the entry's row, or to
    swap two of the rows. For example:
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1\\
      \end{bmatrix}
      \xrightarrow{(2)}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 1 & 1 & 0\\
        0 & 0 & 0 & 1\\
      \end{bmatrix}
      \xrightarrow{(2)}
      \begin{bmatrix}
        1 & 0 & 0  & 0\\
        0 & 0 & -1 & 0\\
        0 & 1 & 1  & 0\\
        0 & 0 & 0  & 1\\
      \end{bmatrix}.
    \end{align*}
    However, this matrix is not upper-triangular. If we do one of
    the upper rows:
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1\\
      \end{bmatrix}
      \xrightarrow{(2)}
      \begin{bmatrix}
        1 & 1 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1\\
      \end{bmatrix}
      \xrightarrow{(2)}
      \begin{bmatrix}
        1  & 1 & 0 & 0\\
        -1 & 0 & 0 & 0\\
        0  & 0 & 1 & 0\\
        0  & 0 & 0 & 1\\
      \end{bmatrix},
    \end{align*}
    we can see that this matrix is not upper-triangular either.
    Same if we swap:
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1\\
      \end{bmatrix}
      \xrightarrow{(3)}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 0 & 1\\
        0 & 0 & 1 & 0\\
      \end{bmatrix};
    \end{align*}
    one row will end up with a non-$0$ entry below the main
    diagonal no matter which two rows we pick.

    To return the matrix to a state of upper-triangularity, we
    would need to make all the entries on the main diagonal
    non-$0$ again:
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0  & 0\\
        0 & 0 & -1 & 0\\
        0 & 1 & 1  & 0\\
        0 & 0 & 0  & 1\\
      \end{bmatrix}
      \xrightarrow{(3)}
      \begin{bmatrix}
        1 & 0 & 0  & 0\\
        0 & 1 & 1  & 0\\
        0 & 0 & -1 & 0\\
        0 & 0 & 0  & 1\\
      \end{bmatrix};
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1  & 1 & 0 & 0\\
        -1 & 0 & 0 & 0\\
        0  & 0 & 1 & 0\\
        0  & 0 & 0 & 1\\
      \end{bmatrix}
      \xrightarrow{(2)}
      \begin{bmatrix}
        1  & 1 & 0 & 0\\
        0  & 1 & 0 & 0\\
        0  & 0 & 1 & 0\\
        0  & 0 & 0 & 1\\
      \end{bmatrix};
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 0 & 1\\
        0 & 0 & 1 & 0\\
      \end{bmatrix}
      \xrightarrow{(3)}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1\\
      \end{bmatrix}.
    \end{align*}
    To have a route from $I$ to an upper-triangular matrix with a
    $0$ on its main diagonal, we would need an elementary row
    operation that allowed for multiplying a row by $0$. Since we
    have no such operation available, $I$ cannot be
    row-equivalent to such a matrix, and therefore such a matrix
    cannot be invertible.

    However, if every entry on the main diagonal of $A$ is
    different from $0$, $A$ will always be row-equivalent to $I$.
    $I$ can then be mapped to any such $A$ through a combination
    of elementary row operations (1) and (2), because in each
    column of $I$, there are zeroes above the main diagonal and
    ones on the main diagonal. Each $1$ on the main diagonal can
    be mapped to any scalar using (1):
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1\\
      \end{bmatrix}
      \xrightarrow{(1)}
      \begin{bmatrix}
        a & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1\\
      \end{bmatrix}
      \xrightarrow{(1)}
      \begin{bmatrix}
        a & 0 & 0 & 0\\
        0 & b & 0 & 0\\
        0 & 0 & 1 & 0\\
        0 & 0 & 0 & 1\\
      \end{bmatrix}
      \xrightarrow{(1)}\cdots
    \end{align*}
    The entries above the main diagonal can then be mapped to any
    scalar using (2):
    \begin{align*}
      \begin{bmatrix}
        a & 0 & 0 & 0\\
        0 & b & 0 & 0\\
        0 & 0 & c & 0\\
        0 & 0 & 0 & d\\
      \end{bmatrix}
      \xrightarrow{(2)}
      \begin{bmatrix}
        a & \frac{e}{b}b & 0 & 0\\
        0 & b & 0 & 0\\
        0 & 0 & c & 0\\
        0 & 0 & 0 & d\\
      \end{bmatrix}
      \xrightarrow{}
      \begin{bmatrix}
        a & e & 0 & 0\\
        0 & b & 0 & 0\\
        0 & 0 & c & 0\\
        0 & 0 & 0 & d\\
      \end{bmatrix}
      \xrightarrow{(2)}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        a & e & 0 & 0\\
        0 & b & \frac{f}{c}c & 0\\
        0 & 0 & c & 0\\
        0 & 0 & 0 & d\\
      \end{bmatrix}
      \xrightarrow{}
      \begin{bmatrix}
        a & e & 0 & 0\\
        0 & b & f & 0\\
        0 & 0 & c & 0\\
        0 & 0 & 0 & d\\
      \end{bmatrix}
      \xrightarrow{(2)}\cdots
    \end{align*}
    So, if every entry on the main diagonal of $A$ is different
    from $0$, $I$ will be row-equivalent to $A$, and thus $A$
    will be invertible. Likewise, if $A$ is invertible, $A$ is
    row-equivalent to $I$, and thus $A$ must have all non-$0$
    entries on its main diagonal for there to be a set of
    elementary row operations that would map it to $I$.

  \item
    We would like to prove that if $A$ is an $m \times n$ matrix,
    $B$ is an $n \times m$ matrix, and $n < m$, $AB$ is not
    invertible.

    Because $A$ is an $m \times n$ matrix where $n < m$, it is
    row-equivalent to a row-reduced echelon matrix with at least
    one all-$0$ row. This is because it has more rows than
    columns, so even if every column in the row-reduced echelon
    matrix has a $1$, at least one row will not. As a result,
    $AB$ will be row-equivalent to a row-reduced echelon matrix
    with at least one all-$0$ row as well.

    If we denote the row-reduced echelon matrix to which $A$ is
    row-equivalent as $R_a$, and the series of elementary row
    operations that maps $R_a$ to $A$ as $e_a()$, then $AB =
    e_a(R_a)B$. Since $e_a$ has an inverse $e_a^{-1}$ that is
    also a set of elementary row operations, $e_a(R_a)$ is
    row-equivalent to $e_a^{-1}(e_a(R_a)) = R_a$, and thus $AB$
    is row-equivalent to $R_aB$. Because $R_a$ must have at least
    one all-0 row as we have described, $R_aB$ must as well, and
    thus $R_aB$ cannot be row-equivalent to $I$, which means $AB$
    cannot be either. As such, $AB$ must not be invertible.

    For example:

    \begin{align*}
      \begin{bmatrix}
        1 & 0\\
        0 & 1\\
        0 & 0\\
      \end{bmatrix}
      \begin{bmatrix}
        b_{11} & b_{21} & b_{31}\\
        b_{12} & b_{22} & b_{32}\\
      \end{bmatrix}
      =
      \begin{bmatrix}
        b_{11} & b_{21} & b_{31}\\
        b_{12} & b_{22} & b_{32}\\
        0 & 0 & 0\\
      \end{bmatrix}.
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        b_{11} & b_{21} & b_{31}\\
        b_{12} & b_{22} & b_{32}\\
        0 & 0 & 0\\
      \end{bmatrix}
    \end{align*}
    might be row-equivalent to
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 0\\
      \end{bmatrix},
      \begin{bmatrix}
        0 & 1 & 0\\
        0 & 0 & 1\\
        0 & 0 & 0\\
      \end{bmatrix},
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 0 & 0\\
        0 & 0 & 0\\
      \end{bmatrix},
      \begin{bmatrix}
        0 & 1 & 0\\
        0 & 0 & 0\\
        0 & 0 & 0\\
      \end{bmatrix},
      \begin{bmatrix}
        0 & 0 & 1\\
        0 & 0 & 0\\
        0 & 0 & 0\\
      \end{bmatrix},\ \text{or}\
      \begin{bmatrix}
        0 & 0 & 0\\
        0 & 0 & 0\\
        0 & 0 & 0\\
      \end{bmatrix},
    \end{align*}
    but not
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1\\
      \end{bmatrix}.
    \end{align*}

  \item
    We would like to prove that, if $A$ is an $m \times n$
    matrix, we can, through a finite number of elementary row
    and/or column operations, map $A$ to a matrix $R$ which is
    both \say{row-reduced echelon} and \say{column-reduced
    echelon}, i.e. such that $R_{ij} = 0$ if $i \neq j$, $R_{ii}
    = 1$, $1 \leq i \leq r$, $R_{ii} = 0$ if $i > r$. We would
    also like to show that $R = PAQ$, where $P$ is an invertible
    $m \times m$ matrix and $Q$ is an invertible $n \times n$
    matrix.

    We know already that $A$ can be mapped via a finite number of
    elementary row operations to a row-reduced echelon matrix by
    theorems 4 and 5. We denote this matrix by $A_r$. The
    following is true of $A_r$:

    \begin{enumerate}
      \item
        in each of $A_r$'s rows, the first non-zero element is
        $1$,
      \item
        for a column $c$ of $A_r$, if $c$ contains the first
        non-zero element of a row, all the other elements in $c$
        are $0$,
      \item
        every all-0 row of $R$ comes after every non-all-0 row,
        and
      \item
        if rows $1,\ \ldots,\ r$ are the non-all-0 rows of $R$,
        and $k_i,\ i = 1, \ldots,\ r$ are the indices of the
        first non-0 entry of the corresponding row $i$, $k_1$ has
        the lowest value, $k_2$ the next-lowest, etc.
    \end{enumerate}

    In addition to these, the following must also be true for
    $R$:

    \begin{enumerate}
      \item
        in each of $R$'s columns, the first non-zero element is
        $1$,
      \item
        for a row $r$ of $R$, if $r$ contains the first non-zero
        element of a column, all the other elements in $r$ are
        $0$,
      \item
        every all-0 column of $R$ comes after every non-all-0
        column, and
      \item
        if columns $1,\ \ldots,\ r$ are the non-all-0 columns of
        $R$, and $k_i,\ i = 1, \ldots,\ r$ are the indices of the
        first non-0 entry of the corresponding column $i$, $k_1$
        has the lowest value, $k_2$ the next-lowest, etc.
    \end{enumerate}

    Here are the possible elementary column operations:

    \begin{enumerate}
        \item
          multiplication of one column of $A_r$ by a non-zero
          scalar $n$,
        \item
          replacement of the $c$th column of $A_r$ by column $c$
          plus $n$ times column $d$, $n$ any scalar and $c \neq
          d$, and
        \item
          interchange of two columns of $A_r$.
    \end{enumerate}

    We denote the row-reduced echelon criteria by $r_a$ through
    $r_d$, the column-reduced echelon criteria by $c_a$ through
    $c_d$, and the elementary column operations by $e_a$ through
    $e_c$.

    We know that $c_a$ must be true for those columns of $A_r$
    that contain the first non-0 element of a row by $r_a$ and
    $r_b$. If the last column of $A_r$ contains the first non-0
    element of a row, $c_a$ is satisfied.

    Otherwise, let $j$ be the index of the first column of $A_r$
    for which $c_a$ is unsatisfied. This column must also not
    satisfy $c_b$ by $r_a$. Let $i$ be the smallest index such
    that $A_{ij} \neq 0$. We know that the first non-0 entry in
    row $i$ is $1$ by $r_a$ and that the other entries in the
    column of that first non-0 entry are $0$ by $r_b$. We denote
    this first non-0 entry in $i$ by $i_1$.  We can add $-A_{ij}$
    times the column of $i_1$ to column $j$ by $e_b$, which maps
    $A_{ij}$ to $0$ and leaves the remaining entries in column
    $j$ unaffected. This process can be repeated until all the
    remaning non-0 entries in column $j$ are mapped to $0$. Then
    column $j$ satisfies $c_a$ and $c_b$, since any entries in
    $j$ that do not have a $1$ in their row are already $0$ by
    $r_c$. This process can then be repeated for the remaining
    columns of $A_r$ that do not satisfy $c_a$ and $c_b$.

    This maps every column in $A_r$ that does not satisfy $c_a$
    and $c_b$ to an all-0 column. By $r_d$, these columns must
    all come after those columns that contain non-0 elements, so
    this process also maps $A_r$ to a matrix that satisfies
    $c_c$. Furthermore, the columns that contain non-0 elements
    satisfy $c_d$ by $r_d$, so this process maps $A_r$ to $R$,
    and therefore $A$ can be mapped to $R$ through a finite
    number of elementary row and/or column operations.

    Here is an example using a $2 \times 4$ matrix:
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & a & b\\
        0 & 1 & c & d\\
      \end{bmatrix}
      \xrightarrow{(e_b)}
      \begin{bmatrix}
        1 & 0 & (a - a) & b\\
        0 & 1 & (c - 0) & d\\
      \end{bmatrix}
      \xrightarrow{}
      \begin{bmatrix}
        1 & 0 & 0 & b\\
        0 & 1 & c & d\\
      \end{bmatrix}
      \xrightarrow{(e_b)}
    \end{align*}
    \begin{align*}
      \begin{bmatrix}
        1 & 0 & 0 & b\\
        0 & 1 & 0 & d\\
      \end{bmatrix}
      \xrightarrow{(e_b)}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & d\\
      \end{bmatrix}
      \xrightarrow{(e_b)}
      \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & 1 & 0 & 0\\
      \end{bmatrix}.
    \end{align*}

    Now, we already know that $A_r = PA$ where $P$ is an
    invertible $m \times m$ matrix by corollary 12.2. If $R =
    PAQ$, this implies that $R = A_rQ$.

    We have shown that there is a finite number of elementary
    column operations that maps $A_r$ to $R$; in particlar, $A_r$
    can be mapped to $R$ through a finite number of applications
    of $e_b$. If $c$ and $d$ are column indices and $n$ is a
    scalar, $e_b$ can be more rigorously defined as
    \begin{equation*}
      e_b(A)_{ij} = A_{ij} \text{ if } j \neq c,\text{ }e(A)_{ic} =
      A_{ic} + nA_{id}.
    \end{equation*}
    Now consider an $n \times n$ matrix $E = e_b(I)$ for some
    application of $e_b$. We denote the Kronecker delta by
    $\delta$. Then
    \begin{align*}
      E_{ij} =
      \begin{cases}
        \delta_{ij},\ j \neq c\\
        \delta_{ic} + n\delta_{id},\ j = c.
      \end{cases}
    \end{align*}
    Now consider the product $A_rE$:
    \begin{align*}
      (A_rE)_{ij} = \sum_{k = 1}^{n}A_{r_{ik}}E_{kj} =
      \begin{cases}
        A_{r_{ij}},\ j \neq c\\
        A_{r_{ic}} + nA_{r_{id}},\ j = c.
      \end{cases}
    \end{align*}

    For example, with $c = 1$ and $d = 0$:
    \begin{align*}
      \begin{bmatrix}
        a_{11} & a_{12}\\
        a_{21} & a_{22}\\
      \end{bmatrix}
      \begin{bmatrix}
        1 & n\\
        0 & 1
      \end{bmatrix}
      =
      \begin{bmatrix}
        a_{11} & na_{11} + a_{12}\\
        a_{21} & a_{22}\\
      \end{bmatrix}.
    \end{align*}

    Given this, we can see that $e_b(A_r) = A_rE$. We can
    represent the repeated applications of $e_b$ that map $A_r$
    to $R$ by $e_{b_1},\ldots,e_{b_k}$, such that $R =
    e_{b_k}(\ldots e_{b_1}(A_r)\ldots)$. By what we have just
    shown, this is equivalent to the product of $A_r$ and a
    series of matrices $E_1,\ldots,E_k$, such that
    $e_{b_k}(\ldots e_{b_1}(A_r)\ldots) = A_rE_1\cdots E_k$. If
    we denote $E_1,\ldots,E_k$ by $Q$, we know by theorem 8 that
    $A_rE_1\cdots E_k = A_r(E_1\cdots E_k) = A_rQ$. Since $A_r =
    PA$, we then have that $R = PAQ$.

  \item
    We have
    \begin{align*}
      A =
      \begin{bmatrix}
        1 &
        \frac{1}{2} &
        \cdots &
        \frac{1}{n}\\
        \frac{1}{2} &
        \frac{1}{3} &
        \cdots &
        \frac{1}{n + 1}\\
        \vdots &
        \vdots &
        \ddots &
        \vdots &\\
        \frac{1}{n} &
        \frac{1}{n + 1} &
        \cdots &
        \frac{1}{2n - 1}\\
      \end{bmatrix}.
    \end{align*}
    We would like to see if $A$ is invertible, and if so, if
    $A^{-1}$ has integer entries.

    We can see that
    \begin{align*}
      A_{ij} = \frac{1}{i + j - 1}.
    \end{align*}

    Since $A_{11} = 1$, we can map the entries in column 1 other
    than $A_{11}$ to 0 via
    \begin{align*}
      f_1(A_{ij}) =
      \begin{cases}
        A_{ij},\ i = 1\\
        A_{ij} - A_{i1}A_{1j},\ i \neq 1
      \end{cases}
      =
      \begin{cases}
        \frac{1}{j},\ i = 1\\
        \frac{1}{i + j - 1} - \frac{1}{ij},\ i \neq 1.
      \end{cases}
    \end{align*}
    This is equivalent to repeated application of elementary row
    operation (2). When $i \neq 1,\ j = 1$,
    \begin{align*}
      f_1(A_{ij}) = A_{i1} - A_{i1}A_{11} = A_{i1} - 1A_{i1} =
      0,
    \end{align*}
    so $f_1$ fulfills its purpose.

    We would then like to map $A_{22}$ to 1;
    \begin{align*}
      f_1(A_{22}) = \frac{1}{3} - \frac{1}{4} = \frac{1}{12}.
    \end{align*}
    As such, we can accomplish this via
    \begin{align*}
      f_2(A_{ij}) =
      \begin{cases}
        A_{ij}, i \neq 2\\
        12A_{ij}, i = 2,
      \end{cases}
    \end{align*}
    such that
    \begin{align*}
      f_2(f_1(A_{ij})) =
      \begin{cases}
        \frac{1}{j},\ i = 1\\
        \frac{12}{1 + j} - \frac{12}{2j},\ i = 2\\
        \frac{1}{i + j - 1} - \frac{1}{ij},\ i > 2.
      \end{cases}
    \end{align*}
    Then
    \begin{align*}
      f_2(f_1(A_{22}))
        = \frac{12}{2 + 2 - 1} - \frac{12}{2(2)}
        = \frac{12}{3} - \frac{12}{4} = 4 - 3 = 1.
    \end{align*}
    This is equivalent to an application of elementary row
    operation (1).

    We now need to map the entries in column 2 other than
    $A_{22}$ to 0. This can be done via a function $f_3$ such
    that
    \begin{align*}
      f_3(A_{ij}) =
      \begin{cases}
        A_{2j}, i = 2\\
        A_{ij} - A_{i2}A_{2j}, i \neq 2,
      \end{cases}
    \end{align*}
    such that
    \begin{align*}
      f_3(f_2(f_1(A_{ij}))) =
      \begin{cases}
        \frac{12}{1 + j} - \frac{12}{2j},\ i = 2\\
        (\frac{1}{i + j - 1} - \frac{1}{ij})
          - (\frac{1}{i + 1} - \frac{1}{2i})(\frac{12}{1 + j} -
          \frac{12}{2j}), i \neq 2.
      \end{cases}
    \end{align*}
    When $i \neq 2$, $j = 2$,
    \begin{align*}
      f_3(f_2(f_1(A_{ij})))
      =&\ (\frac{1}{i + 1} - \frac{1}{2i})
          - (\frac{1}{i + 1} - \frac{1}{2i})(\frac{12}{3} -
          \frac{12}{4})\\
      =&\ (\frac{1}{i + 1}
          - \frac{1}{i + 1})
          + (\frac{1}{2i}
          - \frac{1}{2i})\\
      =&\ 0,
    \end{align*}
    so $f_3$ fulfills its purpose. As with $f_1$, $f_3$ is
    equivalent to repeated applications of elementary row
    operation (2).

    We can see a pattern emerging here. If we imagine that our
    sequence of functions starts with a function $f_0$ equivalent
    to identity such that
    \begin{align*}
      f_0(A_{ij}) =
      \begin{cases}
        A_{ij}, i \neq 2\\
        1A_{ij}, i = 2,
      \end{cases}
    \end{align*}
    it seems quite likely that we will be able to map $A$ to $I$
    using a finite sequence of elementary row operations via the
    composition of functions
    \begin{align*}
      f_{2n-1} \circ f_{2n-2} \circ \ldots \circ f_{0},
    \end{align*}
    such that a given function $f_{k}$ in this sequence is
    defined as
    \begin{align*}
      f_{k}(A)_{ij} =
      \begin{cases}
        \begin{cases}
          A_{ij},
        \end{cases} k = 0,\\
        \begin{cases}
          f_{k-1}(A)_{ij}, i \neq k'\\
          f_{k-1}(A)_{ii}^{-1}f_{k-1}(A)_{ij}, i = k',\\
          \text{where}\ k' = \frac{k}{2} + 1,
        \end{cases} k\ \text{even}, k > 0,\\
        \begin{cases}
          f_{k-1}(A)_{ij},\ i = k'\\
          f_{k-1}(A)_{ij} - f_{k-1}(A)_{ik'}f_{k-1}(A)_{k'j},\ i \neq k',\\
          \text{where}\ k' = \frac{k - 1}{2} + 1,
        \end{cases} k\ \text{odd}, k > 0.
      \end{cases}
    \end{align*}
    (If $k$ takes on the sequence $0,\ 1,\ 2,\ 3,\ldots,\ 2n-2,\
    2n-1$, $k'$ takes on the sequence $1,\ 1,\ 2,\ 2,\ldots,\ n,\
    n$.)

    Of course, there's nothing extremely specific to $A$ about
    this composition of functions; one imagines it would map any
    matrix to I provided that matrix is invertible and $A_{ii}$
    is never $0$ for even $k$.

    It would be nice to prove that this composition of functions
    maps $A$ to $I$. Not only would it resolve half of what we
    set out to discover, but since our composition consists
    entirely of elementary row operations (1) and (2), we could
    then apply it to $I$ and find $A^{-1}$, which would tell us
    whether $A^{-1}$ has all-integer entries.

    For notational convenience, let's say
    \begin{align*}
      f_{2n-1} \circ f_{2n-2} \circ \ldots \circ f_{0} = f.
    \end{align*}

    Since we defined $f$ recursively, it seems conceivable that
    we could prove that $f(A) = I$ by induction.  The difficulty
    with this is that every application of $f_k$ for odd $k$ maps
    every row whose index is not $k'$ to a different row. By the
    nature of elementary row operation (2), this has no impact on
    entries in columns whose index is less than $k'$ because the
    preceeding entries in row $k'$ will be $0$, but in the
    ultimate case of $k' = n$ the entries in column $n$ other
    than $A_{nn}$ will be determined by the composition of every
    prior operation. As such, characterizing them based on $A$ is
    not especially straightforward. In other words, although we
    can easily describe $f_{k}$ abstractly, describing it
    concretely from the definition of $A$ is not so easy.

    To be rigorous, it's worth nothing that in these "concrete
    defintions" we are being a bit glib with our notation.
    Technically, our above definition of $f_k$ is correct, but
    our definitions of parts of $f$ in terms of the definition of
    $A_{ij}$ are not, exactly. This is because $A$ is a
    \textit{function}; at the outset of our discussion of
    matrices we defined a matrix as a function from pairs of
    integers into a field. Therefore each $f_k$ is actually a
    higher-order function which associates one matrix to another;
    our "concrete" definitions show what scalars are mapped to by
    this new function.

    Anyway, let's see if we can characterize what happens
    step-by-step to the point where a proof by induction is
    possible.

    First, there's a few important things about $A$ we should
    note.

    If $k + l > m + n$, $A_{kl} < A_{mn}$. This is because $i,j
    \geq 1$, and if $1 < o < p$, $\frac{1}{p} < \frac{1}{o} < 1$.
    So, if
    \begin{align*}
      k + l > m + n
    \end{align*}
    then
    \begin{align*}
      k + l - 1 > m + n - 1,
    \end{align*}
    and thus
    \begin{align*}
      \frac{1}{k + l - 1} < \frac{1}{m + n - 1},
    \end{align*}
    which is equivalent to $A_{kl} < A_{mn}$.

    Also, if $k + l = m + n$, $A_{kl} = A_{mn}$. This is because
    \begin{align*}
      k + l = m + n,
    \end{align*}
    so
    \begin{align*}
      k + l - 1 = m + n - 1,
    \end{align*}
    and thus
    \begin{align*}
      \frac{1}{k + l - 1} = \frac{1}{m + n - 1},
    \end{align*}
    which is equivalent to $A_{kl} = A_{mn}$.

    Also, if $i + j > 2$, then $A_{ij} < 1$. This is
    because, if
    \begin{align*}
      i + j > 2,
    \end{align*}
    then
    \begin{align*}
      i + j - 1 > 2 - 1 = 1,
    \end{align*}
    so
    \begin{align*}
      \frac{1}{i + j - 1} < 1.
    \end{align*}

    Finally, $0 < A_{ij} \leq 1$ in all cases, because $i,j \geq
    1$ and thus $i + j \geq 2$. $A_{11} = 1$ and otherwise
    $A_{ij} < 1$ as we showed above, so $A_{ij} \leq 1$. Also,
    since
    \begin{align*}
      i + j \geq 2 > 0,
    \end{align*}
    then
    \begin{align*}
      i + j - 1 \geq 1 > 0,
    \end{align*}
    so
    \begin{align*}
      0 < \frac{1}{i + j - 1} \leq 1,
    \end{align*}
    and thus $0 < A_{ij} \leq 1$.

    Now. For notational convenience, let's say that
    \begin{align*}
      A_{0_{ij}} =&\ f_0(A)_{ij},\\
      A_{1_{ij}} =&\ f_1(f_0(A)))_{ij},
    \end{align*}
    and so on. Our earlier definition of $f$ then gives us
    \begin{align*}
      A_{k_{ij}} =
      \begin{cases}
        \begin{cases}
          A_{ij},
        \end{cases} k = 0,\\
        \begin{cases}
          A_{k-1_{ij}}, i \neq k'\\
          A_{k-1_{ii}}^{-1}A_{k-1_{ij}}, i = k',\\
          \text{where}\ k' = \frac{k}{2} + 1,
        \end{cases} k\ \text{even}, k > 0,\\
        \begin{cases}
          A_{k-1_{ij}},\ i = k'\\
          A_{k-1_{ij}} - A_{k-1_{ik'}}A_{k-1_{k'j}},\ i \neq k',\\
          \text{where}\ k' = \frac{k - 1}{2} + 1,
        \end{cases} k\ \text{odd}, k > 0.
      \end{cases}
    \end{align*}

    We will also use this notation for $I$, but with a slightly
    different definition:
    \begin{align*}
      I_{k_{ij}} =
      \begin{cases}
        \begin{cases}
          I_{ij},
        \end{cases} k = 0,\\
        \begin{cases}
          I_{k-1_{ij}}, i \neq k'\\
          A_{k-1_{ii}}^{-1}I_{k-1_{ij}}, i = k',\\
          \text{where}\ k' = \frac{k}{2} + 1,
        \end{cases} k\ \text{even}, k > 0,\\
        \begin{cases}
          I_{k-1_{ij}},\ i = k'\\
          I_{k-1_{ij}} - A_{k-1_{ik'}}I_{k-1_{k'j}},\ i \neq k',\\
          \text{where}\ k' = \frac{k - 1}{2} + 1,
        \end{cases} k\ \text{odd}, k > 0.
      \end{cases}
    \end{align*}
    This way, $I_{2n-1}$ should yield $A^{-1}$.

    We know that for $A_1$, $A_{1_{11}} = 1$, and
    otherwise $A_{1_{i1}} = 0$ for $1 < i \leq n$, as we showed
    above. Furthermore, although $A_{1_{22}}$ is less than
    $A_{0_{22}}$, $A_{1_{22}}$ is still greater than $0$.
    $A_{1_{22}} = A_{0_{22}} - A_{0_{21}}A_{0_{12}}$, and since
    $A_{0} = A$,
    \begin{align*}
       0 < A_{0_{21}} = A_{0_{12}} < A_{0_{22}} < 1.
    \end{align*}
    Since $0 \leq q < r < 1$ implies that $0 \leq q^{2} < r < 1$,
    \begin{align*}
      0 < A_{0_{21}}A_{0_{12}} < A_{0_{22}} < 1,
    \end{align*}
    and therefore
    \begin{align*}
      0 < A_{0_{22}} - A_{0_{21}}A_{0_{12}} = A_{1_{22}}.
    \end{align*}
    That means $A_{1_{22}}^{-1}$ exists, so $A_{2}$ is defined.

    Also, $A_{0_{22}} = \frac{1}{2 + 2 - 1} = \frac{1}{3}$, and
    $A_{0_{21}}A_{0_{12}} = \frac{1}{(2 + 1 - 1)(2 + 1 - 1)} =
    \frac{1}{2(2)} = \frac{1}{4}$. $\frac{1}{3} - \frac{1}{4} =
    \frac{4}{12} - \frac{3}{12} = \frac{1}{12} = A_{1_{22}}$, so
    $A_{1_{22}}^{-1} = 12$, an integer.

    $I_0 = I$, so $I_0$ has integer entries by definition. The
    only difference between $I_0$ and $I_1$ is that $I_{1_{i1}} =
    I_{0_{ij}} - A_{0_{i1}}I_{0_{1j}} = 0 - 1A_{0_{i1}} =
    -A_{0_{i1}}$, because the entries $I_{0_{1j}}$, $j > 1$ are
    all 0.

    Now, in particular, $I_{1_{21}} = -\frac{1}{2 + 1 - 1} =
    -\frac{1}{2}$, $I_{1_{22}} = 1$, and otherwise $I_{1_{2j}} =
    0$, $j > 2$. The only difference between $I_1$ and $I_2$ is
    that $I_{2_{2j}} = A_{1_{22}}^{-1}I_{1_{2j}} = 12I_{1_{2j}}$.
    $I_{2_{21}} = 12I_{1_{21}} = -\frac{12}{2} = -6$, $I_{2_{22}}
    = 12I_{1_{22}} = 12(1) = 12$, and otherwise $12I_{2_{2j}} =
    12(0) = 0$ for $j > 2$. So, rows 1 and 2 of $I_2$ have
    all-integer entries, as does the rest of $I_2$ aside from the
    rest of column 1.

    What about $A_2$? Since we showed that $A_{1_{22}}^{-1}$ is
    defined, $A_{2_{22}} = 1$, which is all we're worried about
    here.

    Now, $A_{3_{ij}} = A_{2_{ij}} - A_{2_{i2}}A_{2_{2j}}$, unless
    $i = 2$, in which case $A_{3_{ij}} = A_{2_{ij}}$. When $j =
    2$, $A_{2_{22}} = 1$ as we just showed, so $A_{3_{i2}} =
    A_{2_{i2}} - A_{2_{i2}} = 0$ (unless $i = 2$). At this point,
    $A_{3_{11}} = A_{3_{22}} = 1$, and otherwise for $i,j \leq
    2,\ i \neq j$, $A_{3_{ij}} = 0$, so in general for $i,j \leq
    2,\ i \neq j$, $A_{3_{ij}} = I_{ij}$. So far, so good.

    What about $A_{3_{33}}$? If $A_{3_{33}} = 0$, there's no
    $A_4$, so we need to make sure that's not the case. We know
    that $0 < A_{0_{33}} < 1$. $A_{1_{33}} = A_{0_{33}} -
    A_{0_{31}}A_{0_{13}}$. $A_{0_{33}} = \frac{1}{5}$ and
    $A_{0_{31}}A_{0_{13}} = \frac{1}{3^{2}} = \frac{1}{9}$;
    $\frac{1}{5} - \frac{1}{9} = \frac{9}{45} - \frac{5}{45} =
    \frac{4}{45} = A_{1_{33}} \neq 0$, so we're okay there.
    $A_{2_{33}} = A_{1_{33}}$. $A_{3_{33}} = A_{2_{33}} -
    A_{2_{32}}A_{2_{23}}$.

    $A_{2_{32}} = A_{1_{32}}$. $A_{1_{32}} = A_{0_{32}} -
    A_{0_{31}}A_{0_{12}} = \frac{1}{4} - \frac{1}{6} =
    \frac{1}{12}$.  $A_{2_{23}} = A_{1_{22}}^{-1}A_{1_{23}}$.
    $A_{1_{23}} = A_{0_{23}} - A_{0_{21}}A_{0_{13}} = \frac{1}{4}
    - \frac{1}{6} = \frac{6}{24} - \frac{4}{24} = \frac{1}{12}$,
    so $A_{2_{23}} = 12\frac{1}{12} = 1$. Therefore $A_{3_{33}} =
    \frac{4}{45} - 1\frac{1}{12} = \frac{48}{540} -
    \frac{45}{540} = \frac{3}{540} = \frac{1}{180}$. So, not only
    is $A_{3_{33}}^{-1}$ defined, it's $180$, an integer.

    $I_{2_{31}}$ is the only entry in row 3 of $I_2$ that is not
    an integer; in particuar, it's $-\frac{1}{3}$. $I_{3_{31}} =
    I_{2_{31}} - A_{2_{32}}I_{2_{21}}$. $I_{2_{21}} = 12I_{1_21}
    = 12(-\frac{1}{2}) = -6$. So, $I_{2_{31}} -
    A_{2_{32}}I_{2_{21}} = -\frac{1}{3} - \frac{1}{12}(-6) =
    -\frac{1}{3} + \frac{1}{2} = \frac{1}{6} = I_{3_{31}}$. That
    means that $I_{4_{31}} = 180\frac{1}{6} = 30$, an integer.

    $I_{3_{32}} = I_{2_{32}} - A_{2_{32}}I_{2_{22}} = 0 -
    \frac{1}{12}12 = -1$, so $I_{4_{32}} = -180$, also an
    integer. $I_{4_{33}} = 180$ of course, and otherwise
    $I_{4_{3j}} = 0$ for $j > 3$. One other thing to note is that
    $I_{3_{12}} \neq I_{2_{12}}$ as all of column 2 was affected
    aside from $I_{2_{22}}$, but $I_{3_{12}} = I_{2_{12}} -
    A_{2_{12}}I_{2_{22}} = 0 - \frac{1}{2}(12) = -6$, also an
    integer. Therefore, $I_{4_{ij}}$ has all-integer entries for
    $i \leq 3$.

    At this point, we have more than set up the first half of a
    proof by induction for $A$ being invertible and $A^{-1}$
    having integer entries. What remains is to show this: for odd
    $k$, given that $A_{k_{ij}} = 1$ for $1 \leq i = j \leq k'$,
    $A_{k_{ij}} = 0$ for $1 \leq i \neq j \leq k'$, and
    $I_{k_{ij}}$ are integers for $1 \leq i \leq k'$, then
    $A_{k+2_{ij}} = 1$ for $1 \leq i = j \leq k' + 1$,
    $A_{k+2_{ij}} = 0$ for $1 \leq i \neq j \leq k' + 1$, and
    $I_{k+2_{ij}}$ are integers for $1 \leq i \leq k' + 1$.

    Since $k$ is odd, we know that $A_{k+1_{(k'+1)(k'+1)}}$ =
    $A_{k_{(k'+1)(k'+1)}}^{-1}A_{k_{(k'+1)(k'+1)}}$, which will
    be 1 as long as $A_{k_{(k'+1)(k'+1)}} \neq 0$.

    We know that $0 < A_{0_{(k'+1)(k'+1)}} \leq 1$ by the
    definition of $A$. We also know that as of $A_k$,
    $A_{k_{(k'+1)(k'+1)}}$ will have been modified $k'-1$ times
    via addition/subtraction by the entries above it in the same
    column multiplied by an entry with the same row index as
    that entry's column.

    Well, okay, I have to put this problem aside for now if I'm
    going to keep up with my own schedule for getting through
    this book. I'd like to return to it later, especially since
    it relates to the harmonic series which I have a natural
    interest in.

\end{enumerate}
\end{document}
