\documentclass[12pt]{article}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fontspec}
\usepackage{xfrac}
\usepackage{array}
\usepackage{siunitx}
\usepackage{gensymb}
\usepackage{enumitem}
\title{Matrices}
\author{Zoë Sparks}

\begin{document}

\theoremstyle{definition}

\sisetup{quotient-mode=fraction}
\newtheorem{thm}{Theorem}
\newtheorem*{nthm}{Theorem}
\newtheorem{sthm}{}[thm]
\newtheorem{lemma}{Lemma}[thm]
\newtheorem{cor}{Corollary}[thm]
\newtheorem*{prop}{Property}
\newtheorem*{defn}{Definition}
\newtheorem*{comm}{Comment}
\newtheorem*{exm}{Example}

\maketitle

\begin{defn}
  We can abbreviate this system:
  \begin{equation} \label{eq:syslin}
  \begin{array}{ccccccccc}
    A_{11}x_1 & + & A_{12}x_2 & + & \ldots & + & A_{1n}x_n & = & y_1\\
    A_{21}x_1 & + & A_{22}x_2 & + & \ldots & + & A_{2n}x_n & = & y_2\\
    \vdots    & + & \vdots    & + & \ldots & + & \vdots    & = & \vdots\\
    A_{m1}x_1 & + & A_{m2}x_2 & + & \ldots & + & A_{mn}x_n & = & y_m
  \end{array}
  \end{equation}
  by
  \begin{align*}
    AX = Y
  \end{align*}
  where\\
  \[
    \begin{array}{cccll}
      &A =
      &\begin{bmatrix}
        A_{11} & \cdots & A_{1n}\\
        \vdots & \ddots & \vdots\\
        A_{m1} & \cdots & A_{mn}
      \end{bmatrix}&&\\\\
      X =&
      \begin{bmatrix}
        x_{1} \\
        \vdots\\
        x_{n}
      \end{bmatrix}&
      \text{ and }
      &Y =&
      \begin{bmatrix}
        y_{1} \\
        \vdots\\
        y_{m}
      \end{bmatrix}.
    \end{array}
  \]\\

  We call $A$ the \textbf{matrix of coefficients} of
  \eqref{eq:syslin}. To be strict, though, $A$ is not truly a
  matrix, but rather a representation of a matrix. We define an
  $m \times n$ \textbf{matrix over the field} $F$ to be a
  function $A$ from the set of pairs of integers $(i,j)$, $1 \leq
  i \leq m$, $1 \leq j \leq n$ into the field $F$. We call the
  scalars $A(i,j) = A_{ij}$ the \textbf{entries} of $A$. As
  above, it's often convenient to represent a matrix as a
  rectangular $m \times n$ array with the entries shown inside.
  $X$ above is, or defines, an $n \times 1$ matrix, and $Y$ above
  is an $m \times 1$ matrix.
\end{defn}

\begin{defn}
  An \textbf{elementary row operation} is one of the following
  three functions which associate with each $m \times n$ matrix
  $A$ an $m \times n$ matrix $e(A)$:
  \begin{enumerate}
      \item \label{ero1}
        multiplication of one row of $A$ by a non-zero scalar
        $c$:
        \begin{equation*}
          e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj} = cA_{rj};
        \end{equation*}
      \item \label{ero2}
        replacement of the $r$th row of $A$ by row $r$ plus $c$
        times row $s$, $c$ any scalar and $r \neq s$:
        \begin{equation*}
          e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj} =
          A_{rj} + cA_{sj};
        \end{equation*}
      \item \label{ero3}
        interchange of two rows of $A$:
        \begin{equation*}
          e(A)_{ij} = A_{ij} \text{ if } i \neq r \text{ and } i
          \neq s,\text{ }e(A)_{rj} = A_{sj},\text{ }e(A)_{sj} = A_{rj}.
        \end{equation*}
  \end{enumerate}

  In all of these operations, it doesn't matter much how many
  columns $A$ has, but it's clearly very important how many rows
  it has; for instance, a $1 \times n$ matrix doesn't have two
  row indices not equal to each other. For this reason, we define
  elementary row operations for $m \times n$ matrices over $F$
  for any $n$ but a fixed $m$.
\end{defn}

\begin{thm} \label{thm:elrowinv}
  To each elementary row operation $e$ there corresponds an
  elementary row operation $e_1$ of the same type as $e$ such
  that $e_1(e(A)) = e(e_1(A)) = A$ for each $A$. In other words,
  for each elementary row operation, the inverse function exists
  and is of the same type.
  \begin{proof}
    For \eqref{ero1}, if
      \begin{equation*}
        e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj}
        = cA_{rj},
      \end{equation*}
    then
      \begin{equation*}
        e_{1}(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e_{1}(A)_{rj}
        = c^{-1}A_{rj}.
      \end{equation*}
    Then $e_{1}(e(A))_{rj} = e(e_{1}(A))_{rj} = cc^{-1}A_{rj} =
    A_{rj}$, and otherwise $e_{1}(e(A))_{ij} = e(e_{1}(A))_{rj} =
    A_{ij}$.

    For \eqref{ero2}, if
      \begin{equation*}
        e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj} =
        A_{rj} + cA_{sj},
      \end{equation*}
    then
      \begin{equation*}
        e_{1}(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e_{1}(A)_{rj} =
        A_{rj} - cA_{sj}.
      \end{equation*}
    Then $e_{1}(e(A))_{rj} = e(e_{1}(A))_{rj} = A_{rj} + A_{sj} -
    A_{sj} = A_{rj}$, and otherwise $e_{1}(e(A))_{ij} =
    e(e_{1}(A))_{rj} = A_{ij}$.

    For \eqref{ero3}, if
      \begin{equation*}
        e(A)_{ij} = A_{ij} \text{ if } i \neq r \text{ and } i
        \neq s,\text{ }e(A)_{rj} = A_{sj},\text{ }e(A)_{sj} = A_{rj},
      \end{equation*}
    then $e = e_{1}$. $e(e(A)_{rj}) = e(A_{sj}) = A_{rj}$,
    $e(e(A)_{sj}) = e(A_{rj}) = A_{sj}$, and otherwise
    $e_{1}(e(A))_{ij} = e(e_{1}(A))_{rj} = A_{ij}$.
  \end{proof}

\end{thm}

\begin{defn}
  If we have matrices $A$ and $B$ that are both $m \times n$ and
  over the same field, we say that $B$ is \textbf{row-equivalent}
  to $A$ if $B$ can be produced by a finite series of elementary
  row operations on $A$.
\end{defn}

\begin{defn}
  An \textbf{equivalence relation} is one between two members of
  a set that is \textbf{reflexive}, \textbf{transitive}, and
  \textbf{symmetric}. In other words, if we have $a$, $b$, and
  $c$ of the same set and we represent an equivalence relation by
  $\equiv$, then
  \begin{center}
    $a \equiv a$,\\
    $a \equiv b \iff b \equiv a$,\\
    $a \equiv b \land b \equiv c \implies a \equiv c$.
  \end{center}
\end{defn}

\begin{cor}
  Row equivalence is an equivalence relation.
  \begin{proof}
    By theorem \ref{thm:elrowinv}, we know that any elementary
    row operation has an inverse, which also implies that any
    finite composition of elementary row operations has an
    inverse; therefore we let $e$ stand for a finite composition
    of one or more elementary row operations and $e_1$ for its
    inverse.

    Say we have $m \times n$ matrices $A$, $B$, and $C$ over the
    same field, and we represent row equivalence by $\sim$. It's
    trivially true that $A \sim A$ because $e(e_{1}(A)) = A$. If
    $A \sim B$, and in particular $e(A) = B$, then $e_{1}(B) =
    A$, so $B \sim A$. If also $B \sim C$, and in particular
    $f(B) = C$, then $f(e(A)) = C$, so $A \sim C$.
  \end{proof}
\end{cor}

\begin{thm}
  If $A$ and $B$ are $m \times n$ row-equivalent matrices, then
  for an $n \times 1$ matrix $X$, the homogeneous systems of
  linear equations $AX = 0$ and $BX = 0$ have the same set of
  solutions.
  \begin{proof}
    Each row of $A$ represents a linear equation utilizing the
    coefficients provided by $X$ and equal to $0$.  Multiplying a
    row of $A$ by a non-zero scalar $c$ doesn't change this—if
    $A_{i1}x_1 + \cdots + A_{ij}x_j = 0$, then $c(A_{i1}x_1 +
    \cdots + A_{ij}x_j) = 0$ as well. By the same token, if $r$
    and $s$ are rows of $A$ and $c$ is any scalar, replacing $r$
    with $r + cs$ won't change this either, because both $r$ and
    $s$ equal $0$ with the coefficients in $X$. The same is true
    for swapping two rows of $A$, for the same reason. As such,
    for any composition of elementary row operations $e$, if $AX
    = 0$, $e(A)X = 0$ as well and vice-versa, so any two
    row-equivalent matrices with the same dimensions will have
    the same solutions. Another way of putting this is that, if
    $B$ is row-equivalent to $A$, all the equations in $AX = 0$
    will be linear combinations of $BX = 0$ and vice versa,
    because of the nature of elementary row operations.
  \end{proof}
\end{thm}

\begin{defn}
  We call an $m \times n$ matrix $R$ \textbf{row-reduced} if:
  \begin{enumerate}
      \item in each of $R$'s rows, the first non-zero element is
        $1$, and
      \item for a column $c$ of $R$, if $c$ contains the first
        non-zero element of a row, all the other elements in $c$
        are $0$.
  \end{enumerate}
\end{defn}

\begin{comm}
  Here are a few examples of row-reduced matrices:
  \begin{align*}
    \begin{bmatrix}
      0 & 0 & 1 & 3\\
      1 & 0 & 0 & i\\
      0 & 1 & 0 & -2
    \end{bmatrix}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1
    \end{bmatrix}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1\\
    \end{bmatrix}
  \end{align*}
  If a matrix like this is used to express a system of
  homogeneous linear equations, it makes the possible solutions
  of the system very obvious. For example, in the case of the
  first matrix above, if we have
  \begin{align*}
    x_3 + 3x_4 & = 0,\\
    x_1 + ix_4 & = 0,\\
    x_2 - 2x_4 & = 0,\\
  \end{align*}
  then for a scalar $c$ the solutions are $(-ic,\ 2c,\ -3c,\ c)$,
  for this system or any linear combination of it.
\end{comm}

\begin{defn}
  The last matrix represented above is an example of an
  \textbf{identity matrix}. This is a square matrix such that, if
  an element $A_{ij}$ is at a position such that $i = j$, then
  $A_{ij} = 1$, and otherwise $A_{ij} = 0$.
\end{defn}

\begin{defn}
  The \textbf{Kronecker delta} is a function of two variables
  $\delta_{ij}$ such that:
  \begin{equation*}
    \delta_{ij} =
      \begin{cases}
        1, & \text{if}\ i = j \\
        0, & \text{if}\ i \neq j.
      \end{cases}
  \end{equation*}
  This function maps the row and column indices of a square
  matrix to the elements of an identity matrix of the
  corresponding dimensions at the positions indicated by those
  indices. It's named for Leopold Kronecker, a 19th-century
  algebraic number theorist from Prussia.
\end{defn}

\begin{thm}
  Every $m \times n$ matrix over a field $F$ is row-equivalent to
  a row-reduced matrix.
  \begin{proof}
    By repeated induction.

    First, consider a scalar $c$ in $F$, which we could call a $1
    \times 1$ matrix over $F$. If $c = 0$, then as a matrix $c$
    is already row-reduced. Otherwise, in any field, there is a
    multiplicative inverse for every element other than $0$.
    Therefore, if $c \neq 0$, we can map $c$ to $cc^{-1} = 1$
    according to elementary row operation \eqref{ero1}, and this
    matrix is row-reduced. Therefore, any $1 \times 1$ matrix is
    row-equivalent to a row-reduced matrix.

    \begin{align*}
      \begin{bmatrix}
        c
      \end{bmatrix}
      \xrightarrow{\eqref{ero1}}
      \begin{bmatrix}
        cc^{-1}
      \end{bmatrix}
      \xrightarrow{}
      \begin{bmatrix}
        1
      \end{bmatrix}
    \end{align*}\\

    Now consider an $r \times 1$ row-reduced matrix $A$. We call
    $A_{i} = 1$ the only element of $A$ that is $\neq 0$. Then
    consider an $(r + 1) \times 1$ matrix $A'$ with all rows
    equal to those of $A$ except $A'_{r + 1}$ and such that
    $A'_{r + 1} \neq 0$. In any field, there is an additive
    inverse for every element. Therefore, we can map $A'_{r + 1}$
    to $A'_{r + 1} - A'_{r + 1}A_{i} = 0$ according to elementary
    row operation \eqref{ero2}; this maps $A'$ to a row-reduced
    matrix. Therefore, any $m \times 1$ matrix is row-equivalent
    to a row-reduced matrix.

    \begin{align*}
      \begin{bmatrix}
        0\\
        \vdots\\
        1\\
        0\\
        \vdots\\
        0\\
        A_{r + 1}
      \end{bmatrix}
      \xrightarrow{\eqref{ero2}}
      \begin{bmatrix}
        0\\
        \vdots\\
        1\\
        0\\
        \vdots\\
        0\\
        A_{r + 1} - A_{r + 1}
      \end{bmatrix}
      \xrightarrow{}
      \begin{bmatrix}
        0\\
        \vdots\\
        1\\
        0\\
        \vdots\\
        0\\
        0\\
      \end{bmatrix}
    \end{align*}\\

    Now consider an $r \times s$ row-reduced matrix $B$. Unless
    $B$ has a row with all elements $0$, any $r \times (s + q)$
    matrix $B'$ such that if $(i,j)$ indexes an element in $B$
    then $B'_{ij} = B_{ij}$ will also be row-reduced. This is
    because all the rows in both matrices will still satisfy the
    conditions for row-reducedness, as their first non-zero
    elements will all be present in $B$.

    Now consider an $r \times s$ row-reduced matrix $B$ with $0 <
    d < r$ rows with all elements $0$, and an $r \times (s + 1)$
    matrix $B'$ with all elements $B'_{11}$ to $B'_{rs}$ equal to
    those at the same indices in $B$ and that is not row-reduced.
    For the column $(s + 1)$ in $B'$, we can pick at random an
    element $p$ in $(s + 1)$ such that all the prior elements in
    $p$'s row are $0$, multiply $p$'s row by $p^{-1}$ in
    accordance with elementary row operation \eqref{ero1}, and
    then repeatedly apply elementary row operation \eqref{ero2}
    using $p$'s row to map all the other elements in $p$'s column
    to $0$, as this will have no effect on the other columns.
    This maps $B'$ to a row-reduced matrix.

    \begin{align*}
      &\begin{bmatrix}
        1      & 0 & \cdots &   & 0 & B'_{1(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots & 0 & 1 & B'_{k(s+1)}\\
        0      &   & \cdots &   & 0 & B'_{(k+1)(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & p\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & B'_{r(s+1)}
      \end{bmatrix}
      \xrightarrow{\eqref{ero1}}
      \begin{bmatrix}
        1      & 0 & \cdots &   & 0 & B'_{1(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots & 0 & 1 & B'_{k(s+1)}\\
        0      &   & \cdots &   & 0 & B'_{(k+1)(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & pp^{-1}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & B'_{r(s+1)}
      \end{bmatrix}
      \xrightarrow{}\\\\
      &\begin{bmatrix}
        1      & 0 & \cdots &   & 0 & B'_{1(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots & 0 & 1 & B'_{k(s+1)}\\
        0      &   & \cdots &   & 0 & B'_{(k+1)(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & 1\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & B'_{r(s+1)}
      \end{bmatrix}
      \xrightarrow{\eqref{ero2}}
      \begin{bmatrix}
        1 - 0  & 0 - 0 & \cdots &   & 0 - 0 & B'_{1(s+1)} - B'_{1(s+1)}\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots & 0 & 1     & B'_{k(s+1)}\\
        0      &       & \cdots &   & 0     & B'_{(k+1)(s+1)}\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots &   & 0     & 1\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots &   & 0     & B'_{r(s+1)}\\
      \end{bmatrix}
      \xrightarrow{}\\\\
      &\begin{bmatrix}
        1      & 0     & \cdots &   & 0     & 0\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots & 0 & 1     & B'_{k(s+1)}\\
        0      &       & \cdots &   & 0     & B'_{(k+1)(s+1)}\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots &   & 0     & 1\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots &   & 0     & B'_{r(s+1)}\\
      \end{bmatrix}
      \xrightarrow{\eqref{ero2}\ldots}
      \begin{bmatrix}
        1      & 0     & \cdots &   & 0     & 0\\
        \vdots &       & \ddots &   &       & \vdots\\
        0      &       & \cdots & 0 & 1     & 0\\
        0      &       & \cdots &   & 0     & 0\\
        \vdots &       & \ddots &   &       & \vdots\\
        0      &       & \cdots &   & 0     & 1\\
        \vdots &       & \ddots &   &       & \vdots\\
        0      &       & \cdots &   & 0     & 0\\
      \end{bmatrix}
    \end{align*}\\

    Now consider an $r \times s$ row-reduced matrix $C$ and an
    $(r + 1) \times s$ matrix $C'$ such that all elements
    $C'_{11}$ to $C'_{rs}$ are equal to those at the same indices
    in $C$ and that is not row-reduced. Every element in the row
    $(r + 1)$ can be mapped to $0$ without changing the other
    elements in $C'$ by applying elementary row operation
    \eqref{ero2}, as every other row will have a single $1$ with
    $0$s in its column and row otherwise aside from the element
    in row $(r + 1)$. Carried out in full, this maps $C'$ to a
    row-reduced matrix.

    \begin{align*}
      \begin{bmatrix}
        1          & 0 & \cdots &   & 0\\
        \vdots     &   & \ddots &   &  \\
        0          &   & \cdots & 0 & 1\\
        C_{(r+1)1} &   & \cdots &   & C_{(r+1)s}
      \end{bmatrix}
      &\xrightarrow{\eqref{ero2}}
      \begin{bmatrix}
        1                       & 0 & \cdots &   & 0\\
        \vdots                  &   & \ddots &   &  \\
        0                       &   & \cdots & 0 & 1\\
        C_{(r+1)1} - C_{(r+1)1} &   & \cdots &   & C_{(r+1)s} - 0
      \end{bmatrix}
      \xrightarrow{}\\\\
      \begin{bmatrix}
        1      & 0 & \cdots &   & 0\\
        \vdots &   & \ddots &   &  \\
        0      &   & \cdots & 0 & 1\\
        0      & C_{(r+1)2} & \cdots &   & C_{(r+1)s}
      \end{bmatrix}
      &\xrightarrow{\eqref{ero2}\ldots}
      \begin{bmatrix}
        1      & 0 & \cdots &   & 0\\
        \vdots &   & \ddots &   &  \\
        0      &   & \cdots & 0 & 1\\
        0      &   & \cdots &   & 0
      \end{bmatrix}
    \end{align*}\\

    All together, this shows that any $m \times n$ matrix over
    $F$ is row-equivalent to a row-reduced matrix.
  \end{proof}
\end{thm}

\end{document}
