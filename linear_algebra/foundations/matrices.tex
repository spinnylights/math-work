\documentclass[12pt]{article}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fontspec}
\usepackage{xfrac}
\usepackage{array}
\usepackage{siunitx}
\usepackage{gensymb}
\usepackage{enumitem}
\usepackage{dirtytalk}
\title{Matrices}
\author{Zoë Sparks}

\begin{document}

\theoremstyle{definition}

\sisetup{quotient-mode=fraction}
\newtheorem{thm}{Theorem}
\newtheorem*{nthm}{Theorem}
\newtheorem{sthm}{}[thm]
\newtheorem{lemma}{Lemma}[thm]
\newtheorem{cor}{Corollary}[thm]
\newtheorem*{prop}{Property}
\newtheorem*{defn}{Definition}
\newtheorem*{comm}{Comment}
\newtheorem*{exm}{Example}

\maketitle

\begin{defn}
  We can abbreviate this system:
  \begin{equation} \label{eq:syslin}
  \begin{array}{ccccccccc}
    A_{11}x_1 & + & A_{12}x_2 & + & \ldots & + & A_{1n}x_n & = & y_1\\
    A_{21}x_1 & + & A_{22}x_2 & + & \ldots & + & A_{2n}x_n & = & y_2\\
    \vdots    & + & \vdots    & + & \ldots & + & \vdots    & = & \vdots\\
    A_{m1}x_1 & + & A_{m2}x_2 & + & \ldots & + & A_{mn}x_n & = & y_m
  \end{array}
  \end{equation}
  by
  \begin{align*}
    AX = Y
  \end{align*}
  where\\
  \[
    \begin{array}{cccll}
      &A =
      &\begin{bmatrix}
        A_{11} & \cdots & A_{1n}\\
        \vdots & \ddots & \vdots\\
        A_{m1} & \cdots & A_{mn}
      \end{bmatrix}&&\\\\
      X =&
      \begin{bmatrix}
        x_{1} \\
        \vdots\\
        x_{n}
      \end{bmatrix}&
      \text{ and }
      &Y =&
      \begin{bmatrix}
        y_{1} \\
        \vdots\\
        y_{m}
      \end{bmatrix}.
    \end{array}
  \]\\

  We call $A$ the \textbf{matrix of coefficients} of
  \eqref{eq:syslin}. To be strict, though, $A$ is not truly a
  matrix, but rather a representation of a matrix. We define an
  $m \times n$ \textbf{matrix over the field} $F$ to be a
  function $A$ from the set of pairs of integers $(i,j)$, $1 \leq
  i \leq m$, $1 \leq j \leq n$ into the field $F$. We call the
  scalars $A(i,j) = A_{ij}$ the \textbf{entries} of $A$. As
  above, it's often convenient to represent a matrix as a
  rectangular $m \times n$ array with the entries shown inside.
  $X$ above is, or defines, an $n \times 1$ matrix, and $Y$ above
  is an $m \times 1$ matrix.
\end{defn}

\begin{defn}
  An \textbf{elementary row operation} is one of the following
  three functions which associate with each $m \times n$ matrix
  $A$ an $m \times n$ matrix $e(A)$:
  \begin{enumerate}
      \item \label{ero1}
        multiplication of one row of $A$ by a non-zero scalar
        $c$:
        \begin{equation*}
          e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj} = cA_{rj};
        \end{equation*}
      \item \label{ero2}
        replacement of the $r$th row of $A$ by row $r$ plus $c$
        times row $s$, $c$ any scalar and $r \neq s$:
        \begin{equation*}
          e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj} =
          A_{rj} + cA_{sj};
        \end{equation*}
      \item \label{ero3}
        interchange of two rows of $A$:
        \begin{equation*}
          e(A)_{ij} = A_{ij} \text{ if } i \neq r \text{ and } i
          \neq s,\text{ }e(A)_{rj} = A_{sj},\text{ }e(A)_{sj} = A_{rj}.
        \end{equation*}
  \end{enumerate}

  In all of these operations, it doesn't matter much how many
  columns $A$ has, but it's clearly very important how many rows
  it has; for instance, a $1 \times n$ matrix doesn't have two
  row indices not equal to each other. For this reason, we define
  elementary row operations for $m \times n$ matrices over $F$
  for any $n$ but a fixed $m$.
\end{defn}

\begin{thm} \label{thm:elrowinv}
  To each elementary row operation $e$ there corresponds an
  elementary row operation $e_1$ of the same type as $e$ such
  that $e_1(e(A)) = e(e_1(A)) = A$ for each $A$. In other words,
  for each elementary row operation, the inverse function exists
  and is of the same type.
  \begin{proof}
    For \eqref{ero1}, if
      \begin{equation*}
        e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj}
        = cA_{rj},
      \end{equation*}
    then
      \begin{equation*}
        e_{1}(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e_{1}(A)_{rj}
        = c^{-1}A_{rj}.
      \end{equation*}
    Then $e_{1}(e(A))_{rj} = e(e_{1}(A))_{rj} = cc^{-1}A_{rj} =
    A_{rj}$, and otherwise $e_{1}(e(A))_{ij} = e(e_{1}(A))_{rj} =
    A_{ij}$.

    For \eqref{ero2}, if
      \begin{equation*}
        e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj} =
        A_{rj} + cA_{sj},
      \end{equation*}
    then
      \begin{equation*}
        e_{1}(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e_{1}(A)_{rj} =
        A_{rj} - cA_{sj}.
      \end{equation*}
    Then $e_{1}(e(A))_{rj} = e(e_{1}(A))_{rj} = A_{rj} + A_{sj} -
    A_{sj} = A_{rj}$, and otherwise $e_{1}(e(A))_{ij} =
    e(e_{1}(A))_{rj} = A_{ij}$.

    For \eqref{ero3}, if
      \begin{equation*}
        e(A)_{ij} = A_{ij} \text{ if } i \neq r \text{ and } i
        \neq s,\text{ }e(A)_{rj} = A_{sj},\text{ }e(A)_{sj} = A_{rj},
      \end{equation*}
    then $e = e_{1}$. $e(e(A)_{rj}) = e(A_{sj}) = A_{rj}$,
    $e(e(A)_{sj}) = e(A_{rj}) = A_{sj}$, and otherwise
    $e_{1}(e(A))_{ij} = e(e_{1}(A))_{rj} = A_{ij}$.
  \end{proof}

\end{thm}

\begin{defn}
  If we have matrices $A$ and $B$ that are both $m \times n$ and
  over the same field, we say that $B$ is \textbf{row-equivalent}
  to $A$ if $B$ can be produced by a finite series of elementary
  row operations on $A$.
\end{defn}

\begin{defn}
  An \textbf{equivalence relation} is one between two members of
  a set that is \textbf{reflexive}, \textbf{transitive}, and
  \textbf{symmetric}. In other words, if we have $a$, $b$, and
  $c$ of the same set and we represent an equivalence relation by
  $\equiv$, then
  \begin{center}
    $a \equiv a$,\\
    $a \equiv b \iff b \equiv a$,\\
    $a \equiv b \land b \equiv c \implies a \equiv c$.
  \end{center}
\end{defn}

\begin{cor}
  Row equivalence is an equivalence relation.
  \begin{proof}
    By theorem \ref{thm:elrowinv}, we know that any elementary
    row operation has an inverse, which also implies that any
    finite composition of elementary row operations has an
    inverse; therefore we let $e$ stand for a finite composition
    of one or more elementary row operations and $e_1$ for its
    inverse.

    Say we have $m \times n$ matrices $A$, $B$, and $C$ over the
    same field, and we represent row equivalence by $\sim$. It's
    trivially true that $A \sim A$ because $e(e_{1}(A)) = A$. If
    $A \sim B$, and in particular $e(A) = B$, then $e_{1}(B) =
    A$, so $B \sim A$. If also $B \sim C$, and in particular
    $f(B) = C$, then $f(e(A)) = C$, so $A \sim C$.
  \end{proof}
\end{cor}

\begin{thm} \label{thm:roweqsamesol}
  If $A$ and $B$ are $m \times n$ row-equivalent matrices, then
  for an $n \times 1$ matrix $X$, the homogeneous systems of
  linear equations $AX = 0$ and $BX = 0$ have the same set of
  solutions.
  \begin{proof}
    Each row of $A$ represents a linear equation utilizing the
    coefficients provided by $X$ and equal to $0$.  Multiplying a
    row of $A$ by a non-zero scalar $c$ doesn't change this—if
    $A_{i1}x_1 + \cdots + A_{ij}x_j = 0$, then $c(A_{i1}x_1 +
    \cdots + A_{ij}x_j) = 0$ as well. By the same token, if $r$
    and $s$ are rows of $A$ and $c$ is any scalar, replacing $r$
    with $r + cs$ won't change this either, because both $r$ and
    $s$ equal $0$ with the coefficients in $X$. The same is true
    for swapping two rows of $A$, for the same reason. As such,
    for any composition of elementary row operations $e$, if $AX
    = 0$, $e(A)X = 0$ as well and vice-versa, so any two
    row-equivalent matrices with the same dimensions will have
    the same solutions. Another way of putting this is that, if
    $B$ is row-equivalent to $A$, all the equations in $AX = 0$
    will be linear combinations of $BX = 0$ and vice versa,
    because of the nature of elementary row operations.
  \end{proof}
\end{thm}

\begin{defn}
  We call an $m \times n$ matrix $R$ \textbf{row-reduced} if:
  \begin{enumerate}
      \item in each of $R$'s rows, the first non-zero element is
        $1$, and
      \item for a column $c$ of $R$, if $c$ contains the first
        non-zero element of a row, all the other elements in $c$
        are $0$.
  \end{enumerate}
\end{defn}

\begin{comm}
  Here are a few examples of row-reduced matrices:
  \begin{align*}
    \begin{bmatrix}
      0 & 0 & 1 & 3\\
      1 & 0 & 0 & i\\
      0 & 1 & 0 & -2
    \end{bmatrix}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1
    \end{bmatrix}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1\\
    \end{bmatrix}
  \end{align*}
  If a matrix like this is used to express a system of
  homogeneous linear equations, it makes the possible solutions
  of the system very obvious. For example, in the case of the
  first matrix above, if we have
  \begin{align*}
    x_3 + 3x_4 & = 0,\\
    x_1 + ix_4 & = 0,\\
    x_2 - 2x_4 & = 0,\\
  \end{align*}
  then for a scalar $c$ the solutions are $(-ic,\ 2c,\ -3c,\ c)$,
  for this system or any linear combination of it.
\end{comm}

\begin{defn}
  The last matrix represented above is an example of an
  \textbf{identity matrix}. This is a square matrix such that, if
  an element $A_{ij}$ is at a position such that $i = j$, then
  $A_{ij} = 1$, and otherwise $A_{ij} = 0$.
\end{defn}

\begin{defn}
  The \textbf{Kronecker delta} is a function of two variables
  $\delta_{ij}$ such that:
  \begin{equation*}
    \delta_{ij} =
      \begin{cases}
        1, & \text{if}\ i = j \\
        0, & \text{if}\ i \neq j.
      \end{cases}
  \end{equation*}
  This function maps the row and column indices of a square
  matrix to the elements of an identity matrix of the
  corresponding dimensions at the positions indicated by those
  indices. It's named for Leopold Kronecker, a 19th-century
  algebraic number theorist from Prussia.
\end{defn}

\begin{thm} \label{thm:roweqrowred}
  Every $m \times n$ matrix over a field $F$ is row-equivalent to
  a row-reduced matrix.
  \begin{proof}
    By repeated induction.

    First, consider a scalar $c$ in $F$, which we could call a $1
    \times 1$ matrix over $F$. If $c = 0$, then as a matrix $c$
    is already row-reduced. Otherwise, in any field, there is a
    multiplicative inverse for every element other than $0$.
    Therefore, if $c \neq 0$, we can map $c$ to $cc^{-1} = 1$
    according to elementary row operation \eqref{ero1}, and this
    matrix is row-reduced. Therefore, any $1 \times 1$ matrix is
    row-equivalent to a row-reduced matrix.

    \begin{align*}
      \begin{bmatrix}
        c
      \end{bmatrix}
      \xrightarrow{\eqref{ero1}}
      \begin{bmatrix}
        cc^{-1}
      \end{bmatrix}
      \xrightarrow{}
      \begin{bmatrix}
        1
      \end{bmatrix}
    \end{align*}\\

    Now consider an $r \times 1$ row-reduced matrix $A$. We call
    $A_{i} = 1$ the only element of $A$ that is $\neq 0$. Then
    consider an $(r + 1) \times 1$ matrix $A'$ with all rows
    equal to those of $A$ except $A'_{r + 1}$ and such that
    $A'_{r + 1} \neq 0$. In any field, there is an additive
    inverse for every element. Therefore, we can map $A'_{r + 1}$
    to $A'_{r + 1} - A'_{r + 1}A_{i} = 0$ according to elementary
    row operation \eqref{ero2}; this maps $A'$ to a row-reduced
    matrix. Therefore, any $m \times 1$ matrix is row-equivalent
    to a row-reduced matrix.

    \begin{align*}
      \begin{bmatrix}
        0\\
        \vdots\\
        1\\
        0\\
        \vdots\\
        0\\
        A_{r + 1}
      \end{bmatrix}
      \xrightarrow{\eqref{ero2}}
      \begin{bmatrix}
        0\\
        \vdots\\
        1\\
        0\\
        \vdots\\
        0\\
        A_{r + 1} - A_{r + 1}
      \end{bmatrix}
      \xrightarrow{}
      \begin{bmatrix}
        0\\
        \vdots\\
        1\\
        0\\
        \vdots\\
        0\\
        0\\
      \end{bmatrix}
    \end{align*}\\

    Now consider an $r \times s$ row-reduced matrix $B$. Unless
    $B$ has a row with all elements $0$, any $r \times (s + q)$
    matrix $B'$ such that if $(i,j)$ indexes an element in $B$
    then $B'_{ij} = B_{ij}$ will also be row-reduced. This is
    because all the rows in both matrices will still satisfy the
    conditions for row-reducedness, as their first non-zero
    elements will all be present in $B$.

    Now consider an $r \times s$ row-reduced matrix $B$ with $0 <
    d < r$ rows with all elements $0$, and an $r \times (s + 1)$
    matrix $B'$ with all elements $B'_{11}$ to $B'_{rs}$ equal to
    those at the same indices in $B$ and that is not row-reduced.
    For the column $(s + 1)$ in $B'$, we can pick at random an
    element $p$ in $(s + 1)$ such that all the prior elements in
    $p$'s row are $0$, multiply $p$'s row by $p^{-1}$ in
    accordance with elementary row operation \eqref{ero1}, and
    then repeatedly apply elementary row operation \eqref{ero2}
    using $p$'s row to map all the other elements in $p$'s column
    to $0$, as this will have no effect on the other columns.
    This maps $B'$ to a row-reduced matrix.

    \begin{align*}
      &\begin{bmatrix}
        1      & 0 & \cdots &   & 0 & B'_{1(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots & 0 & 1 & B'_{k(s+1)}\\
        0      &   & \cdots &   & 0 & B'_{(k+1)(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & p\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & B'_{r(s+1)}
      \end{bmatrix}
      \xrightarrow{\eqref{ero1}}
      \begin{bmatrix}
        1      & 0 & \cdots &   & 0 & B'_{1(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots & 0 & 1 & B'_{k(s+1)}\\
        0      &   & \cdots &   & 0 & B'_{(k+1)(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & pp^{-1}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & B'_{r(s+1)}
      \end{bmatrix}
      \xrightarrow{}\\\\
      &\begin{bmatrix}
        1      & 0 & \cdots &   & 0 & B'_{1(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots & 0 & 1 & B'_{k(s+1)}\\
        0      &   & \cdots &   & 0 & B'_{(k+1)(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & 1\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & B'_{r(s+1)}
      \end{bmatrix}
      \xrightarrow{\eqref{ero2}}
      \begin{bmatrix}
        1 - 0  & 0 - 0 & \cdots &   & 0 - 0 & B'_{1(s+1)} - B'_{1(s+1)}\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots & 0 & 1     & B'_{k(s+1)}\\
        0      &       & \cdots &   & 0     & B'_{(k+1)(s+1)}\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots &   & 0     & 1\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots &   & 0     & B'_{r(s+1)}\\
      \end{bmatrix}
      \xrightarrow{}\\\\
      &\begin{bmatrix}
        1      & 0     & \cdots &   & 0     & 0\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots & 0 & 1     & B'_{k(s+1)}\\
        0      &       & \cdots &   & 0     & B'_{(k+1)(s+1)}\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots &   & 0     & 1\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots &   & 0     & B'_{r(s+1)}\\
      \end{bmatrix}
      \xrightarrow{\eqref{ero2}\ldots}
      \begin{bmatrix}
        1      & 0     & \cdots &   & 0     & 0\\
        \vdots &       & \ddots &   &       & \vdots\\
        0      &       & \cdots & 0 & 1     & 0\\
        0      &       & \cdots &   & 0     & 0\\
        \vdots &       & \ddots &   &       & \vdots\\
        0      &       & \cdots &   & 0     & 1\\
        \vdots &       & \ddots &   &       & \vdots\\
        0      &       & \cdots &   & 0     & 0\\
      \end{bmatrix}
    \end{align*}\\

    Now consider an $r \times s$ row-reduced matrix $C$ and an
    $(r + 1) \times s$ matrix $C'$ such that all elements
    $C'_{11}$ to $C'_{rs}$ are equal to those at the same indices
    in $C$ and that is not row-reduced. Every element in the row
    $(r + 1)$ can be mapped to $0$ without changing the other
    elements in $C'$ by applying elementary row operation
    \eqref{ero2}, as every other row will have a single $1$ with
    $0$s in its column and row otherwise aside from the element
    in row $(r + 1)$. Carried out in full, this maps $C'$ to a
    row-reduced matrix.

    \begin{align*}
      \begin{bmatrix}
        1          & 0 & \cdots &   & 0\\
        \vdots     &   & \ddots &   &  \\
        0          &   & \cdots & 0 & 1\\
        C_{(r+1)1} &   & \cdots &   & C_{(r+1)s}
      \end{bmatrix}
      &\xrightarrow{\eqref{ero2}}
      \begin{bmatrix}
        1                       & 0 & \cdots &   & 0\\
        \vdots                  &   & \ddots &   &  \\
        0                       &   & \cdots & 0 & 1\\
        C_{(r+1)1} - C_{(r+1)1} &   & \cdots &   & C_{(r+1)s} - 0
      \end{bmatrix}
      \xrightarrow{}\\\\
      \begin{bmatrix}
        1      & 0 & \cdots &   & 0\\
        \vdots &   & \ddots &   &  \\
        0      &   & \cdots & 0 & 1\\
        0      & C_{(r+1)2} & \cdots &   & C_{(r+1)s}
      \end{bmatrix}
      &\xrightarrow{\eqref{ero2}\ldots}
      \begin{bmatrix}
        1      & 0 & \cdots &   & 0\\
        \vdots &   & \ddots &   &  \\
        0      &   & \cdots & 0 & 1\\
        0      &   & \cdots &   & 0
      \end{bmatrix}
    \end{align*}\\

    All together, this shows that any $m \times n$ matrix over
    $F$ is row-equivalent to a row-reduced matrix.
  \end{proof}
\end{thm}

\begin{defn}
  (mine.) We say that a row or column of an $m \times n$ matrix
  is \textbf{all-0} to indicate that every element in the row or
  column is $0$.
\end{defn}

\begin{thm} \label{thm:mzerononeq}
  (mine.) If $A$ and $B$ are $m \times n$ row-reduced matrices
  over a field and the number of all-0 rows in $B$ is greater
  than in $A$, $A$ and $B$ are \textit{not} row-equivalent.
  \begin{proof}
    Consider the simplest case in which $A$ and $B$ are $1
    \times 1$. We would then have $A = [1]$ and $B = [0]$. There
    is no elementary row operation that will map $A$ to $B$ or
    vice-versa; elementary row operation \eqref{ero1} requires a
    non-zero coefficient, and \eqref{ero2} and \eqref{ero3} are
    unusable because $A$ and $B$ both have only one row.

    This situation holds even when $A$ and $B$ are $m \times 1$
    for any $m \in \mathbb{N}_1$, as $A$ will have only one row
    with an entry of $1$ and otherwise every row in $A$ and $B$
    will have entries of $0$. Elementary row operations
    \eqref{ero2} and \eqref{ero3} are usable in this case, but
    are of no help; \eqref{ero2} can't map $A$'s $1$ entry to $0$
    in this case as it can only add $0$ to it, nor will
    \eqref{ero3} be able to map the $1$ entry to $0$, only move
    it around.

    If $A$ and $B$ are $m \times n$ for any $m,n \in
    \mathbb{N}_1$, this situation still holds, as every row in
    $A$ that is not all-0 will have a $1$ as its first entry, and
    this entry will be in a column that would be all-0 if not for
    this entry. $B$ will have at least one more all-zero row than
    $A$, meaning that we would need a composition of elementary
    row operations that could map one of $A$'s non-all-0 rows to
    an all-0 row. As we have shown, no such composition exists,
    as the case for the first non-0 entry in that row will be the
    same as in the $m \times 1$ case. If $B$ has more than one
    all-zero row than $A$ does, this situation does not change,
    so $A$ and $B$ are not row-equivalent.
  \end{proof}
\end{thm}

\begin{comm}
  (mine.) Theorem \ref{thm:mzerononeq} shows that you should take
  some care in what you assume about a given $m \times n$ matrix.
  Although theorem \ref{thm:roweqrowred} shows that every $m
  \times n$ matrix over a field is row-equivalent to a
  row-reduced matrix, this does \textit{not} indicate that it is
  row-equivalent to the $m \times n$ identity matrix for that
  field. It may be row-equivalent to a row-reduced matrix with at
  least one all-0 row, and this is \textit{not} row-equivalent to
  an identity matrix.
\end{comm}

\begin{defn}
  A \textbf{row-reduced echelon matrix} is an $m \times n$ matrix
  $R$ such that
  \begin{enumerate}
      \item
        $R$ is row-reduced,
      \item
        every all-0 row of $R$ comes after every non-all-0 row,
        and
      \item
        if rows $1,\ \ldots,\ r$ are the non-all-0 rows of $R$,
        and $k_i,\ i = 1, \ldots,\ r$ are the indices of the
        first non-0 entry of the corresponding row $i$, $k_1$ has
        the lowest value, $k_2$ the next-lowest, etc.
  \end{enumerate}

  \begin{exm}
    This is a simple example of a row-reduced echelon matrix:
    \begin{align*}
      \begin{bmatrix}
        1 & 0\\
        0 & 1\\
      \end{bmatrix}.
    \end{align*}
    This, however, is not a row-reduced echelon matrix:
    \begin{align*}
      \begin{bmatrix}
        0 & 1\\
        1 & 0\\
      \end{bmatrix}.
    \end{align*}
    This is:
    \begin{align*}
      \begin{bmatrix}
        1 & 0\\
        0 & 1\\
        0 & 0\\
      \end{bmatrix},
    \end{align*}
    but this is not:
    \begin{align*}
      \begin{bmatrix}
        1 & 0\\
        0 & 0\\
        0 & 1\\
      \end{bmatrix}.
    \end{align*}
    This is, though:
    \begin{align*}
      \begin{bmatrix}
        0 & 0\\
        0 & 0\\
        0 & 0\\
      \end{bmatrix}.
    \end{align*}
    Here is a more complicated example of one:
    \begin{align*}
      \begin{bmatrix}
        1 & 5i & 0 & 0 & 3\\
        0 & 0  & 1 & 0 & -\frac{2}{3}\\
        0 & 0  & 0 & 1 & -7\\
        0 & 0  & 0 & 0 & 0\\
        0 & 0  & 0 & 0 & 0\\
      \end{bmatrix}.
    \end{align*}
  \end{exm}

  \begin{comm}
    In military jargon, an \say{echelon} is \say{a formation of
    troops, ships, aircraft, or vehicles in parallel rows with
    the end of each row projecting further than the one in
    front,} according to Oxford.
  \end{comm}

\end{defn}

\begin{thm} \label{thm:rowredeqrowredech}
  Every row-reduced $m \times n$ matrix $A$ is row-equivalent to
  a row-reduced echelon matrix.
  \begin{proof}
    Since $A$ is row-reduced, we know that every first-non-0
    entry in a row of $A$ has a unique column index compared to
    the other first-non-0 entries. This is because the first
    non-0 entry in a row of a row-reduced matrix must be in a
    column that would be all-0 if not for that entry.
    Furthermore, the specific row indices of the all-0 rows of a
    row-reduced echelon matrix are immaterial as long as they
    come after the non-all-0 rows. Since column indices are in
    $\mathbb{N}_1$, $\mathbb{N}_1$ is well-ordered, and there is
    a finite number $n$ of columns, a finite number of
    applications of elementary row operation \eqref{ero3} will
    suffice to map $A$ to a row-reduced echelon matrix.
  \end{proof}
\end{thm}

\begin{comm}
  If $R$ is an $m \times n$ row-reduced echelon matrix, the
  system of linear equations $RX = 0$ is particularly easy to
  solve.

  Consider the last of the example row-reduced echelon
  matrices above:
  \begin{align*}
    \begin{bmatrix}
      1 & 5i & 0 & 0 & 3\\
      0 & 0  & 1 & 0 & -\frac{2}{3}\\
      0 & 0  & 0 & 1 & -7\\
      0 & 0  & 0 & 0 & 0\\
      0 & 0  & 0 & 0 & 0\\
    \end{bmatrix}.
  \end{align*}
  If we call this $R$, the system $RX = 0$ could be represented
  as
  \begin{alignat*}{6}
     x_1\ &+&\ 5ix_2\ &+&\ 0x_3\ &+&\ 0x_4\ &+&\           3x_5\ &=&\ &0\\
    0x_1\ &+&\  0x_2\ &+&\ 1x_3\ &+&\ 0x_4\ &-&\ \frac{2}{3}x_5\ &=&\ &0\\
    0x_1\ &+&\  0x_2\ &+&\ 0x_3\ &+&\  x_4\ &-&\           7x_5\ &=&\ &0\\
    0x_1\ &+&\  0x_2\ &+&\ 0x_3\ &+&\ 0x_4\ &+&\           0x_5\ &=&\ &0\\
    0x_1\ &+&\  0x_2\ &+&\ 0x_3\ &+&\ 0x_4\ &+&\           0x_5\ &=&\ &0,
  \end{alignat*}
  or more simply as
  \begin{alignat*}{6}
     x_1\ &+&\ 5ix_2\ & &\      & & & &   &+&            3x_5\ &=&\ &0\\
     & &  & &         & &  x_3\ & &       &-&  \frac{2}{3}x_5\ &=&\ &0\\
     & &  & &         & &       & &\ x_4\ &-&\           7x_5\ &=&\ &0.
  \end{alignat*}
  In turn, this could be rewritten as
  \begin{align*}
    x_1 =&\ -5ix_2 - 3x_5\\
    x_3 =&\ \frac{2}{3}x_5\\
    x_4 =&\ 7x_5.
  \end{align*}

  Each of the unknowns in a system of linear equations
  corresponding to a first-non-0 row entry in a row-reduced
  matrix only occurs once, as with $x_1,\ x_3,\ x_4$ here. This
  allows us to state the solutions in terms of the other
  unknowns, which we can allow to vary freely, e.g. by assigning
  variables such that $x_2 = a$ and $x_5 = b$; the solutions to
  this system are then $(-5ia -3b,\ a,\ \frac{2}{3}b,\ 7b,\ b)$.

  A significant thing to note about this is that if the number of
  non-all-0 rows in $R$ is less than the number of columns, $RX =
  0$ has at least one non-trivial solution, i.e. a solution in
  which not all the unknowns are $0$. As we have shown, some of
  the unknowns will vary freely in the set of solutions to $RX =
  0$ under these conditions.
\end{comm}

\begin{thm} \label{thm:mltnnontriv}
  If $A$ is an $m \times n$ matrix over the field $F$ such that
  $m < n$, the homogeneous system of linear equations $AX = 0$
  has a non-trivial solution.
  \begin{proof}
    By theorem \ref{thm:roweqrowred}, we know that $A$ is
    row-equivalent to a row-reduced matrix $A'$, and we know by
    theorem \ref{thm:roweqsamesol} that $AX = 0$ and $A'X = 0$
    have the same solutions. Because $A'$ has less rows than
    columns, there will be more unknowns in $A'X = 0$ than there
    are first-non-0 entries in $A'$. In the set of solutions of
    $A'X = 0$, there will be a solution for every element in $F$
    for each unknown that does not correspond to a first-non-0
    entry in $A'$. Since $F$ must have at least additive and
    multiplicative identity elements, $A'X = 0$ will have a
    non-trivial solution, and so $AX = 0$ will as well.
  \end{proof}
\end{thm}

\begin{thm}
  If $A$ is an $n \times n$ (i.e. square) matrix, $A$ is
  row-equivalent to the $n \times n$ identity matrix if and only
  if $AX = 0$ has only the trivial solution.
  \begin{proof}
    If $A$ is row-equivalent to the $n \times n$ identity matrix,
    $AX = 0$ must have only the trivial solution, because every
    unknown in $AX = 0$ will occur only once and be said to equal
    $0$. If $A$ is not row-equivalent to the $n \times n$
    identity matrix, then if $A'$ is $A$ in a row-reduced form,
    $A'$ will have at least one all-0 row. An equation in $A'X =
    0$ corresponding to an all-0 row in $A'$ will express a
    tautology, i.e. that $0 = 0$, and thus the solution set of
    $A'X = 0$ will be equivalent to that for an $m \times n,\ m <
    n$ matrix $A''$ with rows equal to the non-all-0 rows of
    $A'$. By theorem \ref{thm:mltnnontriv}, $A''X = 0$ will have
    a non-trivial solution, and thus so will $AX = 0$.
  \end{proof}
\end{thm}

\begin{defn}
  If $A$ is an $m \times n$ matrix, the \textbf{augmented matrix}
  $A'$ of the system $AX = Y$ is the $m \times (n + 1)$ matrix
  such that columns $1,\ \ldots,\ n$ of $A'$ are the same as
  those of $A$ and column $(n + 1)$ of $A'$ is the same as that
  of $Y$. To be precise, $A'$ is such that
  \begin{align*}
    A'_{ij} =&\ A_{ij}\ \text{if}\ j \leq n;\\
    A'_{ij} =&\ Y_{1j}\ \text{if}\ j = n + 1.
  \end{align*}

  \begin{exm}
    If
    \begin{align*}
      A =
      \begin{bmatrix}
        1 & 0 & -3\\
        0 & 3 &  2\\
        5 & 0 &  9\\
      \end{bmatrix}\ \text{and}\ 
      Y =
      \begin{bmatrix}
        7\\
        8\\
        0\\
      \end{bmatrix},
    \end{align*}
    then
    \begin{align*}
      A' =
      \begin{bmatrix}
        1 & 0 & -3 & 7\\
        0 & 3 &  2 & 8\\
        5 & 0 &  9 & 0\\
      \end{bmatrix}.
    \end{align*}
  \end{exm}
\end{defn}

\begin{comm}
  By putting the augmented matrix of a system of linear equations
  into row-reduced echelon form, it becomes straightforward both
  to say whether or not solutions exist for the system and what
  those solutions could be if they do exist. If we put $A'$ above
  into row-reduced echelon form:
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & -3 & 7\\
      0 & 3 &  2 & 8\\
      5 & 0 &  9 & 0
    \end{bmatrix}
    \xrightarrow{(1)}
    \begin{bmatrix}
      1 & 0 & -3 & 7\\
      0 & 1 &  \frac{2}{3} & \frac{8}{3}\\
      5 & 0 &  9 & 0
    \end{bmatrix}
    \xrightarrow{(2)}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & -3 & 7\\
      0 & 1 &  \frac{2}{3} & \frac{8}{3}\\
      0 & 0 & 24 & -35
    \end{bmatrix}
    \xrightarrow{(1)}
    \begin{bmatrix}
      1 & 0 & -3 & 7\\
      0 & 1 &  \frac{2}{3} & \frac{8}{3}\\
      0 & 0 & 1 & -\frac{35}{24}
    \end{bmatrix}
    \xrightarrow{(2)}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 0 & \frac{21}{8}\\
      0 & 1 & 0 & \frac{131}{36}\\
      0 & 0 & 1 & -\frac{35}{24}
    \end{bmatrix},
  \end{align*}
  we can see that $AX = Y$ is equivalent to
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      x_1\\
      x_2\\
      x_3
    \end{bmatrix}
    =
    \begin{bmatrix}
      \frac{21}{8}\\
      \frac{131}{36}\\
      -\frac{35}{24}
    \end{bmatrix},
  \end{align*}
  so $AX = Y$ has only the solution $(x_1,\ x_2,\ x_3) =
  (\frac{21}{8},\ \frac{131}{36},\ -\frac{35}{24})$.

  If $A'$ was instead
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 0 & \frac{21}{8}\\
      0 & 1 & 0 & \frac{131}{36}\\
      0 & 0 & 0 & 0
    \end{bmatrix},
  \end{align*}
  we would instead have a set of solutions $(x_1,\ x_2,\ x_3) =
  (\frac{21}{8},\ \frac{131}{36},\ a)$ in which $a$ could take on
  any value in the field.

  This procedure works to characterize the possible solutions of
  the system even if the elements of $Y$ are unknown. For
  example, if we want to describe the solutions of
  \begin{align*}
    \begin{bmatrix}
      5  & 3  & -2\\
      4  & 0  & 8 \\
      -6 & -6 & 12
    \end{bmatrix}
    \begin{bmatrix}
      x_1\\
      x_2\\
      x_3
    \end{bmatrix}
    =
    \begin{bmatrix}
      y_1\\
      y_2\\
      y_3
    \end{bmatrix},
  \end{align*}
  we can proceed in the same manner:
  \begin{align*}
    \begin{bmatrix}
      5  & 3  & -2 & y_1\\
      4  & 0  & 8  & y_2\\
      -6 & -6 & 12 & y_3
    \end{bmatrix}
    \xrightarrow{(2)}
    \begin{bmatrix}
      5 & 3 & -2 & y_1\\
      4 & 0 & 8  & y_2\\
      4 & 0 & 8  & (y_3 + 2y_1)
    \end{bmatrix}
    \xrightarrow{(2)}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      5 & 3 & -2 & y_1\\
      4 & 0 & 8  & y_2\\
      0 & 0 & 0  & (y_3 + 2y_1 - y_2)
    \end{bmatrix}
    \xrightarrow{(1)}
    \begin{bmatrix}
      5 & 3 & -2 & y_1\\
      1 & 0 & 2  & \frac{1}{4}y_2\\
      0 & 0 & 0  & (y_3 + 2y_1 - y_2)
    \end{bmatrix}
    \xrightarrow{(1)}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      \frac{5}{3} & 1 & -\frac{2}{3} & \frac{1}{3}y_1\\
      1 & 0 & 2  & \frac{1}{4}y_2\\
      0 & 0 & 0  & (y_3 + 2y_1 - y_2)
    \end{bmatrix}
    \xrightarrow{(2)}
    \begin{bmatrix}
      0 & 1 & -4 & (\frac{1}{3}y_1 - \frac{5}{12}y_2)\\
      1 & 0 & 2  & \frac{1}{4}y_2\\
      0 & 0 & 0  & (y_3 + 2y_1 - y_2)
    \end{bmatrix}
    \xrightarrow{(3)}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 2  & \frac{1}{4}y_2\\
      0 & 1 & -4 & (\frac{1}{3}y_1 - \frac{5}{12}y_2)\\
      0 & 0 & 0  & (y_3 + 2y_1 - y_2)
    \end{bmatrix}.
  \end{align*}
  This implies that the system has a solution if the given
  scalars $(y_1,\ y_2,\ y_3)$ are such that $2y_1 - y_2 + y_3 =
  0$. In that case, if we say $x_3 = c$, we can describe the
  solutions by
  \begin{align*}
    x_1 =&\ -2c + \frac{1}{4}y_2\\
    x_2 =&\ 4c + \frac{1}{3}y_1 - \frac{5}{12}y_2.
  \end{align*}
\end{comm}

\begin{comm}
  We can see that whether or not a system $AX = Y$ has a solution
  depends on whether or not the entries of $Y$ satisfy certain
  relations between each other, relations that are determined by
  the entries of $A$ and a composition of elementary row
  operations. For this reason, if $AX = Y$ has a solution in a
  field $F$, and the entries of both $A$ and $Y$ lie in a
  subfield $F_1$ of $F$, $AX = Y$ has a solution in $F_1$.

  For example, in the case of the system
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 2 \\
      0 & 1 & -4\\
      0 & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
      x_1\\
      x_2\\
      x_3
    \end{bmatrix}
    =
    \begin{bmatrix}
      \frac{1}{4}y_2\\
      (\frac{1}{3}y_1 - \frac{5}{12}y_2)\\
      (y_3 + 2y_1 - y_2)
    \end{bmatrix}
  \end{align*}
  above, if we describe this as $AX = Y$, the entries of $A$ and
  $Y$ lie within $\mathbb{Q}$. We could find solutions in
  $\mathbb{R}$ or $\mathbb{C}$ by assigning irrational or
  imaginary values to $y_1$, $y_2$, or $y_3$ that fit the
  constraints we found. However, since the elementary row
  operations involve only addition, subtraction, multiplication,
  and division, these constraints did not require a departure
  from $\mathbb{Q}$, so solutions can be found in $\mathbb{Q}$ by
  assigning rational values to the parameters.
\end{comm}

\begin{defn}
  If $A$ is an $m \times n$ matrix over a field $F$ and $B$ is an
  $n \times p$ matrix over $F$, we say that the \textbf{product}
  $AB$ is the $m \times p$ matrix $C$ whose $i,\ j$ entry is
  \begin{align*}
    C_{ij} =&\ \sum_{r = 1}^{n} A_{ir}B_{rj}.
  \end{align*}
\end{defn}

\begin{exm}
  \begin{align*}
    \begin{bmatrix}
      1 & -1\\
      3 & 2\\
      0 & -7
    \end{bmatrix}
    \begin{bmatrix}
      4  & 1 & 8\\
      20 & 5 & 7
    \end{bmatrix}:
  \end{align*}
  \begin{alignat*}{7}
    \gamma_1 &=&\ 1(4\ \ 1\ \ 8) &-&\ 1(20\ \ 5\ \ 7)
      &=&\ (-16&\ &-4&\ \ &1&)\\
    \gamma_2 &=&\ 3(4\ \ 1\ \ 8) &+&\ 2(20\ \ 5\ \ 7)
      &=&\ (52&\ &13&\ \ &38&)\\
    \gamma_3 &=&\ 0(4\ \ 1\ \ 8) &-&\ 7(20\ \ 5\ \ 7)
      &=&\ (-140&\ &-35&\ \ &-49&)
  \end{alignat*}
  \begin{align*}
    \begin{bmatrix}
      -16 & -4 & 1\\
      52 & 13 & 38\\
      -140 & -35  & -49
    \end{bmatrix}.
  \end{align*}
\end{exm}

\begin{exm}
  \begin{align*}
    \begin{bmatrix}
      1 & 1 & 1\\
      1 & 1 & 1
    \end{bmatrix}
    \begin{bmatrix}
      2\\
      3\\
      4
    \end{bmatrix}:
  \end{align*}
  \begin{alignat*}{7}
    \gamma_1 &=&\ 1(2) &+&\ 1(3) &+& 1(4) &=&\ (9)\\
    \gamma_2 &=&\ 1(2) &+&\ 1(3) &+& 1(4) &=&\ (9)\\
  \end{alignat*}
  \begin{align*}
    \begin{bmatrix}
      9\\
      9
    \end{bmatrix}.
  \end{align*}
\end{exm}

\begin{exm}
  \begin{align*}
    \begin{bmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1
    \end{bmatrix}:
  \end{align*}
  \begin{alignat*}{7}
    \gamma_1 &=&\ 1(1\ \ 0\ \ 0) &+&\ 2(0\ \ 1\ \ 0) &+&\ 3(0\ \ 0\ \ 1)
      &=&\ (1&\ &2&\ \ &3&)\\
    \gamma_2 &=&\ 4(1\ \ 0\ \ 0) &+&\ 5(0\ \ 1\ \ 0) &+&\ 6(0\ \ 0\ \ 1)
      &=&\ (4&\ &5&\ \ &6&)\\
  \end{alignat*}
  \begin{align*}
    \begin{bmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{bmatrix}.
  \end{align*}
\end{exm}

\begin{exm}
  \begin{align*}
    \begin{bmatrix}
      1 & 0\\
      0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{bmatrix}:
  \end{align*}
  \begin{alignat*}{7}
    \gamma_1 &=&\ 1(1\ \ 2\ \ 3) &+&\ 0(4\ \ 5\ \ 6)
      &=&\ (1&\ &2&\ \ &3&)\\
    \gamma_2 &=&\ 0(1\ \ 2\ \ 3) &+&\ 1(4\ \ 5\ \ 6)
      &=&\ (4&\ &5&\ \ &6&)\\
  \end{alignat*}
  \begin{align*}
    \begin{bmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{bmatrix}.
  \end{align*}
\end{exm}

\begin{exm}
  \begin{align*}
    \begin{bmatrix}
      2 & 0\\
      0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{bmatrix}:
  \end{align*}
  \begin{alignat*}{7}
    \gamma_1 &=&\ 2(1\ \ 2\ \ 3) &+&\ 0(4\ \ 5\ \ 6)
      &=&\ (2&\ &4&\ \ &6&)\\
    \gamma_2 &=&\ 0(1\ \ 2\ \ 3) &+&\ 1(4\ \ 5\ \ 6)
      &=&\ (4&\ &5&\ \ &6&)\\
  \end{alignat*}
  \begin{align*}
    \begin{bmatrix}
      2 & 4 & 6\\
      4 & 5 & 6
    \end{bmatrix}.
  \end{align*}
\end{exm}

\begin{exm}
  \begin{align*}
    \begin{bmatrix}
      1 & 0\\
      -1 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{bmatrix}:
  \end{align*}
  \begin{alignat*}{7}
    \gamma_1 &=&\ 1(1\ \ 2\ \ 3) &+&\ 0(4\ \ 5\ \ 6)
      &=&\ (1&\ &2&\ \ &3&)\\
    \gamma_2 &=&\ -1(1\ \ 2\ \ 3) &+&\ 1(4\ \ 5\ \ 6)
      &=&\ (3&\ &3&\ \ &3&)\\
  \end{alignat*}
  \begin{align*}
    \begin{bmatrix}
      1 & 2 & 3\\
      3 & 3 & 3
    \end{bmatrix}.
  \end{align*}
\end{exm}

\begin{exm}
  \begin{align*}
    \begin{bmatrix}
      0 & 1\\
      1 & 0
    \end{bmatrix}
    \begin{bmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{bmatrix}:
  \end{align*}
  \begin{alignat*}{7}
    \gamma_1 &=&\ 0(1\ \ 2\ \ 3) &+&\ 1(4\ \ 5\ \ 6)
      &=&\ (4&\ &5&\ \ &6&)\\
    \gamma_2 &=&\ 1(1\ \ 2\ \ 3) &+&\ 0(4\ \ 5\ \ 6)
      &=&\ (1&\ &2&\ \ &3&)\\
  \end{alignat*}
  \begin{align*}
    \begin{bmatrix}
      4 & 5 & 6\\
      1 & 2 & 3
    \end{bmatrix}.
  \end{align*}
\end{exm}

\end{document}
