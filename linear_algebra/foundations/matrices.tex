\documentclass[12pt]{article}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fontspec}
\usepackage{xfrac}
\usepackage{array}
\usepackage{siunitx}
\usepackage{gensymb}
\usepackage{enumitem}
\title{Matrices}
\author{Zoë Sparks}

\begin{document}

\theoremstyle{definition}

\sisetup{quotient-mode=fraction}
\newtheorem{thm}{Theorem}
\newtheorem*{nthm}{Theorem}
\newtheorem{sthm}{}[thm]
\newtheorem{lemma}{Lemma}[thm]
\newtheorem{cor}{Corollary}[thm]
\newtheorem*{prop}{Property}
\newtheorem*{defn}{Definition}
\newtheorem*{comm}{Comment}
\newtheorem*{exm}{Example}

\maketitle

\begin{defn}
  We can abbreviate this system:
  \begin{equation} \label{eq:syslin}
  \begin{array}{ccccccccc}
    A_{11}x_1 & + & A_{12}x_2 & + & \ldots & + & A_{1n}x_n & = & y_1\\
    A_{21}x_1 & + & A_{22}x_2 & + & \ldots & + & A_{2n}x_n & = & y_2\\
    \vdots    & + & \vdots    & + & \ldots & + & \vdots    & = & \vdots\\
    A_{m1}x_1 & + & A_{m2}x_2 & + & \ldots & + & A_{mn}x_n & = & y_m
  \end{array}
  \end{equation}
  by
  \begin{align*}
    AX = Y
  \end{align*}
  where\\
  \[
    \begin{array}{cccll}
      &A =
      &\begin{bmatrix}
        A_{11} & \cdots & A_{1n}\\
        \vdots & \ddots & \vdots\\
        A_{m1} & \cdots & A_{mn}
      \end{bmatrix}&&\\\\
      X =&
      \begin{bmatrix}
        x_{1} \\
        \vdots\\
        x_{n}
      \end{bmatrix}&
      \text{ and }
      &Y =&
      \begin{bmatrix}
        y_{1} \\
        \vdots\\
        y_{m}
      \end{bmatrix}.
    \end{array}
  \]\\

  We call $A$ the \textbf{matrix of coefficients} of
  \eqref{eq:syslin}. To be strict, though, $A$ is not truly a
  matrix, but rather a representation of a matrix. We define an
  $m \times n$ \textbf{matrix over the field} $F$ to be a
  function $A$ from the set of pairs of integers $(i,j)$, $1 \leq
  i \leq m$, $1 \leq j \leq n$ into the field $F$. We call the
  scalars $A(i,j) = A_{ij}$ the \textbf{entries} of $A$. As
  above, it's often convenient to represent a matrix as a
  rectangular $m \times n$ array with the entries shown inside.
  $X$ above is, or defines, an $n \times 1$ matrix, and $Y$ above
  is an $m \times 1$ matrix.
\end{defn}

\begin{defn}
  An \textbf{elementary row operation} is one of the following
  three functions which associate with each $m \times n$ matrix
  $A$ an $m \times n$ matrix $e(A)$:
  \begin{enumerate}
      \item \label{ero1}
        multiplication of one row of $A$ by a non-zero scalar
        $c$:
        \begin{equation*}
          e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj} = cA_{rj};
        \end{equation*}
      \item \label{ero2}
        replacement of the $r$th row of $A$ by row $r$ plus $c$
        times row $s$, $c$ any scalar and $r \neq s$:
        \begin{equation*}
          e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj} =
          A_{rj} + cA_{sj};
        \end{equation*}
      \item \label{ero3}
        interchange of two rows of $A$:
        \begin{equation*}
          e(A)_{ij} = A_{ij} \text{ if } i \neq r \text{ and } i
          \neq s,\text{ }e(A)_{rj} = A_{sj},\text{ }e(A)_{sj} = A_{rj}.
        \end{equation*}
  \end{enumerate}

  In all of these operations, it doesn't matter much how many
  columns $A$ has, but it's clearly very important how many rows
  it has; for instance, a $1 \times n$ matrix doesn't have two
  row indices not equal to each other. For this reason, we define
  elementary row operations for $m \times n$ matrices over $F$
  for any $n$ but a fixed $m$.
\end{defn}

\begin{thm} \label{thm:elrowinv}
  To each elementary row operation $e$ there corresponds an
  elementary row operation $e_1$ of the same type as $e$ such
  that $e_1(e(A)) = e(e_1(A)) = A$ for each $A$. In other words,
  for each elementary row operation, the inverse function exists
  and is of the same type.
  \begin{proof}
    For \eqref{ero1}, if
      \begin{equation*}
        e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj}
        = cA_{rj},
      \end{equation*}
    then
      \begin{equation*}
        e_{1}(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e_{1}(A)_{rj}
        = c^{-1}A_{rj}.
      \end{equation*}
    Then $e_{1}(e(A))_{rj} = e(e_{1}(A))_{rj} = cc^{-1}A_{rj} =
    A_{rj}$, and otherwise $e_{1}(e(A))_{ij} = e(e_{1}(A))_{rj} =
    A_{ij}$.

    For \eqref{ero2}, if
      \begin{equation*}
        e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj} =
        A_{rj} + cA_{sj},
      \end{equation*}
    then
      \begin{equation*}
        e_{1}(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e_{1}(A)_{rj} =
        A_{rj} - cA_{sj}.
      \end{equation*}
    Then $e_{1}(e(A))_{rj} = e(e_{1}(A))_{rj} = A_{rj} + A_{sj} -
    A_{sj} = A_{rj}$, and otherwise $e_{1}(e(A))_{ij} =
    e(e_{1}(A))_{rj} = A_{ij}$.

    For \eqref{ero3}, if
      \begin{equation*}
        e(A)_{ij} = A_{ij} \text{ if } i \neq r \text{ and } i
        \neq s,\text{ }e(A)_{rj} = A_{sj},\text{ }e(A)_{sj} = A_{rj},
      \end{equation*}
    then $e = e_{1}$. $e(e(A)_{rj}) = e(A_{sj}) = A_{rj}$,
    $e(e(A)_{sj}) = e(A_{rj}) = A_{sj}$, and otherwise
    $e_{1}(e(A))_{ij} = e(e_{1}(A))_{rj} = A_{ij}$.
  \end{proof}

\end{thm}

\begin{defn}
  If we have matrices $A$ and $B$ that are both $m \times n$ and
  over the same field, we say that $B$ is \textbf{row-equivalent}
  to $A$ if $B$ can be produced by a finite series of elementary
  row operations on $A$.
\end{defn}

\begin{defn}
  An \textbf{equivalence relation} is one between two members of
  a set that is \textbf{reflexive}, \textbf{transitive}, and
  \textbf{symmetric}. In other words, if we have $a$, $b$, and
  $c$ of the same set and we represent an equivalence relation by
  $\equiv$, then
  \begin{center}
    $a \equiv a$,\\
    $a \equiv b \iff b \equiv a$,\\
    $a \equiv b \land b \equiv c \implies a \equiv c$.
  \end{center}
\end{defn}

\begin{cor}
  Row equivalence is an equivalence relation.
  \begin{proof}
    By theorem \ref{thm:elrowinv}, we know that any elementary
    row operation has an inverse, which also implies that any
    finite composition of elementary row operations has an
    inverse; therefore we let $e$ stand for a finite composition
    of one or more elementary row operations and $e_1$ for its
    inverse.

    Say we have $m \times n$ matrices $A$, $B$, and $C$ over the
    same field, and we represent row equivalence by $\sim$. It's
    trivially true that $A \sim A$ because $e(e_{1}(A)) = A$. If
    $A \sim B$, and in particular $e(A) = B$, then $e_{1}(B) =
    A$, so $B \sim A$. If also $B \sim C$, and in particular
    $f(B) = C$, then $f(e(A)) = C$, so $A \sim C$.
  \end{proof}
\end{cor}

\begin{thm}
  If $A$ and $B$ are $m \times n$ row-equivalent matrices, then
  for an $n \times 1$ matrix $X$, the homogeneous systems of
  linear equations $AX = 0$ and $BX = 0$ have the same set of
  solutions.
  \begin{proof}
    Each row of $A$ represents a linear equation utilizing the
    coefficients provided by $X$ and equal to $0$.  Multiplying a
    row of $A$ by a non-zero scalar $c$ doesn't change this—if
    $A_{i1}x_1 + \cdots + A_{ij}x_j = 0$, then $c(A_{i1}x_1 +
    \cdots + A_{ij}x_j) = 0$ as well. By the same token, if $r$
    and $s$ are rows of $A$ and $c$ is any scalar, replacing $r$
    with $r + cs$ won't change this either, because both $r$ and
    $s$ equal $0$ with the coefficients in $X$. The same is true
    for swapping two rows of $A$, for the same reason. As such,
    for any composition of elementary row operations $e$, if $AX
    = 0$, $e(A)X = 0$ as well and vice-versa, so any two
    row-equivalent matrices with the same dimensions will have
    the same solutions. Another way of putting this is that, if
    $B$ is row-equivalent to $A$, all the equations in $AX = 0$
    will be linear combinations of $BX = 0$ and vice versa,
    because of the nature of elementary row operations.
  \end{proof}
\end{thm}

\begin{defn}
  We call an $m \times n$ matrix $R$ \textbf{row-reduced} if:
  \begin{enumerate}
      \item in each of $R$'s rows, the first non-zero element is
        $1$, and
      \item for a column $c$ of $R$, if $c$ contains the first
        non-zero element of a row, all the other elements in $c$
        are $0$.
  \end{enumerate}
\end{defn}

\begin{comm}
  Here are a few examples of row-reduced matrices:
  \begin{align*}
    \begin{bmatrix}
      0 & 0 & 1 & 3\\
      1 & 0 & 0 & i\\
      0 & 1 & 0 & -2
    \end{bmatrix}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1
    \end{bmatrix}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1\\
    \end{bmatrix}
  \end{align*}
\end{comm}

\begin{defn}
  The last matrix represented above is an example of an
  \textbf{identity matrix}. This is a square matrix such that, if
  an element $A_{ij}$ is at a position such that $i = j$, then
  $A_{ij} = 1$, and otherwise $A_{ij} = 0$.
\end{defn}

\begin{defn}
  The \textbf{Kronecker delta} is a function of two variables
  $\delta_{ij}$ such that:
  \begin{equation*}
    \delta_{ij} =
      \begin{cases}
        1, & \text{if}\ i = j \\
        0, & \text{if}\ i \neq j.
      \end{cases}
  \end{equation*}
  This function maps the row and column indices of a square
  matrix to the elements of an identity matrix of the
  corresponding dimensions at the positions indicated by those
  indices. It's named for Leopold Kronecker, a 19th-century
  algebraic number theorist from Prussia.
\end{defn}

\end{document}
