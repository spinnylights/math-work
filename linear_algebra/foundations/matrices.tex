\documentclass[12pt]{article}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fontspec}
\usepackage{xfrac}
\usepackage{array}
\usepackage{siunitx}
\usepackage{gensymb}
\usepackage{enumitem}
\usepackage{dirtytalk}
\usepackage{bm}
\title{Matrices}
\author{Zoë Sparks}

\begin{document}

\theoremstyle{definition}

\sisetup{quotient-mode=fraction}
\newtheorem{thm}{Theorem}
\newtheorem*{nthm}{Theorem}
\newtheorem{sthm}{}[thm]
\newtheorem{lemma}{Lemma}[thm]
\newtheorem*{nlemma}{Lemma}
\newtheorem{cor}{Corollary}[thm]
\newtheorem*{prop}{Property}
\newtheorem*{defn}{Definition}
\newtheorem*{comm}{Comment}
\newtheorem*{exm}{Example}

\maketitle

\begin{defn}
  We can abbreviate this system:
  \begin{equation} \label{eq:syslin}
  \begin{array}{ccccccccc}
    A_{11}x_1 & + & A_{12}x_2 & + & \ldots & + & A_{1n}x_n & = & y_1\\
    A_{21}x_1 & + & A_{22}x_2 & + & \ldots & + & A_{2n}x_n & = & y_2\\
    \vdots    & + & \vdots    & + & \ldots & + & \vdots    & = & \vdots\\
    A_{m1}x_1 & + & A_{m2}x_2 & + & \ldots & + & A_{mn}x_n & = & y_m
  \end{array}
  \end{equation}
  by
  \begin{align*}
    AX = Y
  \end{align*}
  where\\
  \[
    \begin{array}{cccll}
      &A =
      &\begin{bmatrix}
        A_{11} & \cdots & A_{1n}\\
        \vdots & \ddots & \vdots\\
        A_{m1} & \cdots & A_{mn}
      \end{bmatrix}&&\\\\
      X =&
      \begin{bmatrix}
        x_{1} \\
        \vdots\\
        x_{n}
      \end{bmatrix}&
      \text{ and }
      &Y =&
      \begin{bmatrix}
        y_{1} \\
        \vdots\\
        y_{m}
      \end{bmatrix}.
    \end{array}
  \]\\

  We call $A$ the \textbf{matrix of coefficients} of
  \eqref{eq:syslin}. To be strict, though, $A$ is not truly a
  matrix, but rather a representation of a matrix. We define an
  $m \times n$ \textbf{matrix over the field} $F$ to be a
  function $A$ from the set of pairs of integers $(i,j)$, $1 \leq
  i \leq m$, $1 \leq j \leq n$ into the field $F$. We call the
  scalars $A(i,j) = A_{ij}$ the \textbf{entries} of $A$. As
  above, it's often convenient to represent a matrix as a
  rectangular $m \times n$ array with the entries shown inside.
  $X$ above is, or defines, an $n \times 1$ matrix, and $Y$ above
  is an $m \times 1$ matrix.
\end{defn}

\begin{defn}
  An \textbf{elementary row operation} is one of the following
  three functions which associate with each $m \times n$ matrix
  $A$ an $m \times n$ matrix $e(A)$:
  \begin{enumerate}
      \item \label{ero1}
        multiplication of one row of $A$ by a non-zero scalar
        $c$:
        \begin{equation*}
          e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj} = cA_{rj};
        \end{equation*}
      \item \label{ero2}
        replacement of the $r$th row of $A$ by row $r$ plus $c$
        times row $s$, $c$ any scalar and $r \neq s$:
        \begin{equation*}
          e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj} =
          A_{rj} + cA_{sj};
        \end{equation*}
      \item \label{ero3}
        interchange of two rows of $A$:
        \begin{equation*}
          e(A)_{ij} = A_{ij} \text{ if } i \neq r \text{ and } i
          \neq s,\text{ }e(A)_{rj} = A_{sj},\text{ }e(A)_{sj} = A_{rj}.
        \end{equation*}
  \end{enumerate}

  In all of these operations, it doesn't matter much how many
  columns $A$ has, but it's clearly very important how many rows
  it has; for instance, a $1 \times n$ matrix doesn't have two
  row indices not equal to each other. For this reason, we define
  elementary row operations for $m \times n$ matrices over $F$
  for any $n$ but a fixed $m$.
\end{defn}

\begin{thm} \label{thm:elrowinv}
  To each elementary row operation $e$ there corresponds an
  elementary row operation $e_1$ of the same type as $e$ such
  that $e_1(e(A)) = e(e_1(A)) = A$ for each $A$. In other words,
  for each elementary row operation, the inverse function exists
  and is of the same type.
  \begin{proof}
    For \eqref{ero1}, if
      \begin{equation*}
        e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj}
        = cA_{rj},
      \end{equation*}
    then
      \begin{equation*}
        e_{1}(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e_{1}(A)_{rj}
        = c^{-1}A_{rj}.
      \end{equation*}
    Then $e_{1}(e(A))_{rj} = e(e_{1}(A))_{rj} = cc^{-1}A_{rj} =
    A_{rj}$, and otherwise $e_{1}(e(A))_{ij} = e(e_{1}(A))_{rj} =
    A_{ij}$.

    For \eqref{ero2}, if
      \begin{equation*}
        e(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e(A)_{rj} =
        A_{rj} + cA_{sj},
      \end{equation*}
    then
      \begin{equation*}
        e_{1}(A)_{ij} = A_{ij} \text{ if } i \neq r,\text{ }e_{1}(A)_{rj} =
        A_{rj} - cA_{sj}.
      \end{equation*}
    Then $e_{1}(e(A))_{rj} = e(e_{1}(A))_{rj} = A_{rj} + A_{sj} -
    A_{sj} = A_{rj}$, and otherwise $e_{1}(e(A))_{ij} =
    e(e_{1}(A))_{rj} = A_{ij}$.

    For \eqref{ero3}, if
      \begin{equation*}
        e(A)_{ij} = A_{ij} \text{ if } i \neq r \text{ and } i
        \neq s,\text{ }e(A)_{rj} = A_{sj},\text{ }e(A)_{sj} = A_{rj},
      \end{equation*}
    then $e = e_{1}$. $e(e(A)_{rj}) = e(A_{sj}) = A_{rj}$,
    $e(e(A)_{sj}) = e(A_{rj}) = A_{sj}$, and otherwise
    $e_{1}(e(A))_{ij} = e(e_{1}(A))_{rj} = A_{ij}$.
  \end{proof}

\end{thm}

\begin{defn}
  If we have matrices $A$ and $B$ that are both $m \times n$ and
  over the same field, we say that $B$ is \textbf{row-equivalent}
  to $A$ if $B$ can be produced by a finite series of elementary
  row operations on $A$.
\end{defn}

\begin{defn}
  An \textbf{equivalence relation} is one between two members of
  a set that is \textbf{reflexive}, \textbf{transitive}, and
  \textbf{symmetric}. In other words, if we have $a$, $b$, and
  $c$ of the same set and we represent an equivalence relation by
  $\equiv$, then
  \begin{center}
    $a \equiv a$,\\
    $a \equiv b \iff b \equiv a$,\\
    $a \equiv b \land b \equiv c \implies a \equiv c$.
  \end{center}
\end{defn}

\begin{cor} \label{cor:rowequivisequivrel}
  Row equivalence is an equivalence relation.
  \begin{proof}
    By theorem \ref{thm:elrowinv}, we know that any elementary
    row operation has an inverse, which also implies that any
    finite composition of elementary row operations has an
    inverse; therefore we let $e$ stand for a finite composition
    of one or more elementary row operations and $e_1$ for its
    inverse.

    Say we have $m \times n$ matrices $A$, $B$, and $C$ over the
    same field, and we represent row equivalence by $\sim$. It's
    trivially true that $A \sim A$ because $e(e_{1}(A)) = A$. If
    $A \sim B$, and in particular $e(A) = B$, then $e_{1}(B) =
    A$, so $B \sim A$. If also $B \sim C$, and in particular
    $f(B) = C$, then $f(e(A)) = C$, so $A \sim C$.
  \end{proof}
\end{cor}

\begin{thm} \label{thm:roweqsamesol}
  If $A$ and $B$ are $m \times n$ row-equivalent matrices, then
  for an $n \times 1$ matrix $X$, the homogeneous systems of
  linear equations $AX = 0$ and $BX = 0$ have the same set of
  solutions.
  \begin{proof}
    Each row of $A$ represents a linear equation utilizing the
    coefficients provided by $X$ and equal to $0$.  Multiplying a
    row of $A$ by a non-zero scalar $c$ doesn't change this—if
    $A_{i1}x_1 + \cdots + A_{ij}x_j = 0$, then $c(A_{i1}x_1 +
    \cdots + A_{ij}x_j) = 0$ as well. By the same token, if $r$
    and $s$ are rows of $A$ and $c$ is any scalar, replacing $r$
    with $r + cs$ won't change this either, because both $r$ and
    $s$ equal $0$ with the coefficients in $X$. The same is true
    for swapping two rows of $A$, for the same reason. As such,
    for any composition of elementary row operations $e$, if $AX
    = 0$, $e(A)X = 0$ as well and vice-versa, so any two
    row-equivalent matrices with the same dimensions will have
    the same solutions. Another way of putting this is that, if
    $B$ is row-equivalent to $A$, all the equations in $AX = 0$
    will be linear combinations of $BX = 0$ and vice versa,
    because of the nature of elementary row operations.
  \end{proof}
\end{thm}

\begin{defn}
  We call an $m \times n$ matrix $R$ \textbf{row-reduced} if:
  \begin{enumerate}
      \item in each of $R$'s rows, the first non-zero element is
        $1$, and
      \item for a column $c$ of $R$, if $c$ contains the first
        non-zero element of a row, all the other elements in $c$
        are $0$.
  \end{enumerate}
\end{defn}

\begin{comm}
  Here are a few examples of row-reduced matrices:
  \begin{align*}
    \begin{bmatrix}
      0 & 0 & 1 & 3\\
      1 & 0 & 0 & i\\
      0 & 1 & 0 & -2
    \end{bmatrix}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1
    \end{bmatrix}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1\\
    \end{bmatrix}
  \end{align*}
  If a matrix like this is used to express a system of
  homogeneous linear equations, it makes the possible solutions
  of the system very obvious. For example, in the case of the
  first matrix above, if we have
  \begin{align*}
    x_3 + 3x_4 & = 0,\\
    x_1 + ix_4 & = 0,\\
    x_2 - 2x_4 & = 0,\\
  \end{align*}
  then for a scalar $c$ the solutions are $(-ic,\ 2c,\ -3c,\ c)$,
  for this system or any linear combination of it.
\end{comm}

\begin{defn}
  The last matrix represented above is an example of an
  \textbf{identity matrix}. This is a square matrix such that, if
  an element $A_{ij}$ is at a position such that $i = j$, then
  $A_{ij} = 1$, and otherwise $A_{ij} = 0$.
\end{defn}

\begin{defn}
  The \textbf{Kronecker delta} is a function of two variables
  $\delta_{ij}$ such that:
  \begin{equation*}
    \delta_{ij} =
      \begin{cases}
        1, & \text{if}\ i = j \\
        0, & \text{if}\ i \neq j.
      \end{cases}
  \end{equation*}
  This function maps the row and column indices of a square
  matrix to the elements of an identity matrix of the
  corresponding dimensions at the positions indicated by those
  indices. It's named for Leopold Kronecker, a 19th-century
  algebraic number theorist from Prussia.
\end{defn}

\begin{thm} \label{thm:roweqrowred}
  Every $m \times n$ matrix over a field $F$ is row-equivalent to
  a row-reduced matrix.
  \begin{proof}
    By repeated induction.

    First, consider a scalar $c$ in $F$, which we could call a $1
    \times 1$ matrix over $F$. If $c = 0$, then as a matrix $c$
    is already row-reduced. Otherwise, in any field, there is a
    multiplicative inverse for every element other than $0$.
    Therefore, if $c \neq 0$, we can map $c$ to $cc^{-1} = 1$
    according to elementary row operation \eqref{ero1}, and this
    matrix is row-reduced. Therefore, any $1 \times 1$ matrix is
    row-equivalent to a row-reduced matrix.

    \begin{align*}
      \begin{bmatrix}
        c
      \end{bmatrix}
      \xrightarrow{\eqref{ero1}}
      \begin{bmatrix}
        cc^{-1}
      \end{bmatrix}
      \xrightarrow{}
      \begin{bmatrix}
        1
      \end{bmatrix}
    \end{align*}\\

    Now consider an $r \times 1$ row-reduced matrix $A$. We call
    $A_{i} = 1$ the only element of $A$ that is $\neq 0$. Then
    consider an $(r + 1) \times 1$ matrix $A'$ with all rows
    equal to those of $A$ except $A'_{r + 1}$ and such that
    $A'_{r + 1} \neq 0$. In any field, there is an additive
    inverse for every element. Therefore, we can map $A'_{r + 1}$
    to $A'_{r + 1} - A'_{r + 1}A_{i} = 0$ according to elementary
    row operation \eqref{ero2}; this maps $A'$ to a row-reduced
    matrix. Therefore, any $m \times 1$ matrix is row-equivalent
    to a row-reduced matrix.

    \begin{align*}
      \begin{bmatrix}
        0\\
        \vdots\\
        1\\
        0\\
        \vdots\\
        0\\
        A_{r + 1}
      \end{bmatrix}
      \xrightarrow{\eqref{ero2}}
      \begin{bmatrix}
        0\\
        \vdots\\
        1\\
        0\\
        \vdots\\
        0\\
        A_{r + 1} - A_{r + 1}
      \end{bmatrix}
      \xrightarrow{}
      \begin{bmatrix}
        0\\
        \vdots\\
        1\\
        0\\
        \vdots\\
        0\\
        0\\
      \end{bmatrix}
    \end{align*}\\

    Now consider an $r \times s$ row-reduced matrix $B$. Unless
    $B$ has a row with all elements $0$, any $r \times (s + q)$
    matrix $B'$ such that if $(i,j)$ indexes an element in $B$
    then $B'_{ij} = B_{ij}$ will also be row-reduced. This is
    because all the rows in both matrices will still satisfy the
    conditions for row-reducedness, as their first non-zero
    elements will all be present in $B$.

    Now consider an $r \times s$ row-reduced matrix $B$ with $0 <
    d < r$ rows with all elements $0$, and an $r \times (s + 1)$
    matrix $B'$ with all elements $B'_{11}$ to $B'_{rs}$ equal to
    those at the same indices in $B$ and that is not row-reduced.
    For the column $(s + 1)$ in $B'$, we can pick at random an
    element $p$ in $(s + 1)$ such that all the prior elements in
    $p$'s row are $0$, multiply $p$'s row by $p^{-1}$ in
    accordance with elementary row operation \eqref{ero1}, and
    then repeatedly apply elementary row operation \eqref{ero2}
    using $p$'s row to map all the other elements in $p$'s column
    to $0$, as this will have no effect on the other columns.
    This maps $B'$ to a row-reduced matrix.

    \begin{align*}
      &\begin{bmatrix}
        1      & 0 & \cdots &   & 0 & B'_{1(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots & 0 & 1 & B'_{k(s+1)}\\
        0      &   & \cdots &   & 0 & B'_{(k+1)(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & p\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & B'_{r(s+1)}
      \end{bmatrix}
      \xrightarrow{\eqref{ero1}}
      \begin{bmatrix}
        1      & 0 & \cdots &   & 0 & B'_{1(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots & 0 & 1 & B'_{k(s+1)}\\
        0      &   & \cdots &   & 0 & B'_{(k+1)(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & pp^{-1}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & B'_{r(s+1)}
      \end{bmatrix}
      \xrightarrow{}\\\\
      &\begin{bmatrix}
        1      & 0 & \cdots &   & 0 & B'_{1(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots & 0 & 1 & B'_{k(s+1)}\\
        0      &   & \cdots &   & 0 & B'_{(k+1)(s+1)}\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & 1\\
        \vdots &   & \ddots &   &   & \vdots     \\
        0      &   & \cdots &   & 0 & B'_{r(s+1)}
      \end{bmatrix}
      \xrightarrow{\eqref{ero2}}
      \begin{bmatrix}
        1 - 0  & 0 - 0 & \cdots &   & 0 - 0 & B'_{1(s+1)} - B'_{1(s+1)}\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots & 0 & 1     & B'_{k(s+1)}\\
        0      &       & \cdots &   & 0     & B'_{(k+1)(s+1)}\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots &   & 0     & 1\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots &   & 0     & B'_{r(s+1)}\\
      \end{bmatrix}
      \xrightarrow{}\\\\
      &\begin{bmatrix}
        1      & 0     & \cdots &   & 0     & 0\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots & 0 & 1     & B'_{k(s+1)}\\
        0      &       & \cdots &   & 0     & B'_{(k+1)(s+1)}\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots &   & 0     & 1\\
        \vdots &       & \ddots &   &       & \vdots     \\
        0      &       & \cdots &   & 0     & B'_{r(s+1)}\\
      \end{bmatrix}
      \xrightarrow{\eqref{ero2}\ldots}
      \begin{bmatrix}
        1      & 0     & \cdots &   & 0     & 0\\
        \vdots &       & \ddots &   &       & \vdots\\
        0      &       & \cdots & 0 & 1     & 0\\
        0      &       & \cdots &   & 0     & 0\\
        \vdots &       & \ddots &   &       & \vdots\\
        0      &       & \cdots &   & 0     & 1\\
        \vdots &       & \ddots &   &       & \vdots\\
        0      &       & \cdots &   & 0     & 0\\
      \end{bmatrix}
    \end{align*}\\

    Now consider an $r \times s$ row-reduced matrix $C$ and an
    $(r + 1) \times s$ matrix $C'$ such that all elements
    $C'_{11}$ to $C'_{rs}$ are equal to those at the same indices
    in $C$ and that is not row-reduced. Every element in the row
    $(r + 1)$ can be mapped to $0$ without changing the other
    elements in $C'$ by applying elementary row operation
    \eqref{ero2}, as every other row will have a single $1$ with
    $0$s in its column and row otherwise aside from the element
    in row $(r + 1)$. Carried out in full, this maps $C'$ to a
    row-reduced matrix.

    \begin{align*}
      \begin{bmatrix}
        1          & 0 & \cdots &   & 0\\
        \vdots     &   & \ddots &   &  \\
        0          &   & \cdots & 0 & 1\\
        C_{(r+1)1} &   & \cdots &   & C_{(r+1)s}
      \end{bmatrix}
      &\xrightarrow{\eqref{ero2}}
      \begin{bmatrix}
        1                       & 0 & \cdots &   & 0\\
        \vdots                  &   & \ddots &   &  \\
        0                       &   & \cdots & 0 & 1\\
        C_{(r+1)1} - C_{(r+1)1} &   & \cdots &   & C_{(r+1)s} - 0
      \end{bmatrix}
      \xrightarrow{}\\\\
      \begin{bmatrix}
        1      & 0 & \cdots &   & 0\\
        \vdots &   & \ddots &   &  \\
        0      &   & \cdots & 0 & 1\\
        0      & C_{(r+1)2} & \cdots &   & C_{(r+1)s}
      \end{bmatrix}
      &\xrightarrow{\eqref{ero2}\ldots}
      \begin{bmatrix}
        1      & 0 & \cdots &   & 0\\
        \vdots &   & \ddots &   &  \\
        0      &   & \cdots & 0 & 1\\
        0      &   & \cdots &   & 0
      \end{bmatrix}
    \end{align*}\\

    All together, this shows that any $m \times n$ matrix over
    $F$ is row-equivalent to a row-reduced matrix.
  \end{proof}
\end{thm}

\begin{defn}
  (mine.) We say that a row or column of an $m \times n$ matrix
  is \textbf{all-0} to indicate that every element in the row or
  column is $0$.
\end{defn}

\begin{thm} \label{thm:mzerononeq}
  (mine.) If $A$ and $B$ are $m \times n$ row-reduced matrices
  over a field and the number of all-0 rows in $B$ is greater
  than in $A$, $A$ and $B$ are \textit{not} row-equivalent.
  \begin{proof}
    Consider the simplest case in which $A$ and $B$ are $1
    \times 1$. We would then have $A = [1]$ and $B = [0]$. There
    is no elementary row operation that will map $A$ to $B$ or
    vice-versa; elementary row operation \eqref{ero1} requires a
    non-zero coefficient, and \eqref{ero2} and \eqref{ero3} are
    unusable because $A$ and $B$ both have only one row.

    This situation holds even when $A$ and $B$ are $m \times 1$
    for any $m \in \mathbb{N}_1$, as $A$ will have only one row
    with an entry of $1$ and otherwise every row in $A$ and $B$
    will have entries of $0$. Elementary row operations
    \eqref{ero2} and \eqref{ero3} are usable in this case, but
    are of no help; \eqref{ero2} can't map $A$'s $1$ entry to $0$
    in this case as it can only add $0$ to it, nor will
    \eqref{ero3} be able to map the $1$ entry to $0$, only move
    it around.

    If $A$ and $B$ are $m \times n$ for any $m,n \in
    \mathbb{N}_1$, this situation still holds, as every row in
    $A$ that is not all-0 will have a $1$ as its first entry, and
    this entry will be in a column that would be all-0 if not for
    this entry. $B$ will have at least one more all-zero row than
    $A$, meaning that we would need a composition of elementary
    row operations that could map one of $A$'s non-all-0 rows to
    an all-0 row. As we have shown, no such composition exists,
    as the case for the first non-0 entry in that row will be the
    same as in the $m \times 1$ case. If $B$ has more than one
    all-zero row than $A$ does, this situation does not change,
    so $A$ and $B$ are not row-equivalent.
  \end{proof}
\end{thm}

\begin{comm}
  (mine.) Theorem \ref{thm:mzerononeq} shows that you should take
  some care in what you assume about a given $m \times n$ matrix.
  Although theorem \ref{thm:roweqrowred} shows that every $m
  \times n$ matrix over a field is row-equivalent to a
  row-reduced matrix, this does \textit{not} indicate that it is
  row-equivalent to the $m \times n$ identity matrix for that
  field. It may be row-equivalent to a row-reduced matrix with at
  least one all-0 row, and this is \textit{not} row-equivalent to
  an identity matrix.
\end{comm}

\begin{defn}
  A \textbf{row-reduced echelon matrix} is an $m \times n$ matrix
  $R$ such that
  \begin{enumerate}
      \item
        $R$ is row-reduced,
      \item
        every all-0 row of $R$ comes after every non-all-0 row,
        and
      \item
        if rows $1,\ \ldots,\ r$ are the non-all-0 rows of $R$,
        and $k_i,\ i = 1, \ldots,\ r$ are the indices of the
        first non-0 entry of the corresponding row $i$, $k_1$ has
        the lowest value, $k_2$ the next-lowest, etc.
  \end{enumerate}

  \begin{exm}
    This is a simple example of a row-reduced echelon matrix:
    \begin{align*}
      \begin{bmatrix}
        1 & 0\\
        0 & 1\\
      \end{bmatrix}.
    \end{align*}
    This, however, is not a row-reduced echelon matrix:
    \begin{align*}
      \begin{bmatrix}
        0 & 1\\
        1 & 0\\
      \end{bmatrix}.
    \end{align*}
    This is:
    \begin{align*}
      \begin{bmatrix}
        1 & 0\\
        0 & 1\\
        0 & 0\\
      \end{bmatrix},
    \end{align*}
    but this is not:
    \begin{align*}
      \begin{bmatrix}
        1 & 0\\
        0 & 0\\
        0 & 1\\
      \end{bmatrix}.
    \end{align*}
    This is, though:
    \begin{align*}
      \begin{bmatrix}
        0 & 0\\
        0 & 0\\
        0 & 0\\
      \end{bmatrix}.
    \end{align*}
    Here is a more complicated example of one:
    \begin{align*}
      \begin{bmatrix}
        1 & 5i & 0 & 0 & 3\\
        0 & 0  & 1 & 0 & -\frac{2}{3}\\
        0 & 0  & 0 & 1 & -7\\
        0 & 0  & 0 & 0 & 0\\
        0 & 0  & 0 & 0 & 0\\
      \end{bmatrix}.
    \end{align*}
  \end{exm}

  \begin{comm}
    In military jargon, an \say{echelon} is \say{a formation of
    troops, ships, aircraft, or vehicles in parallel rows with
    the end of each row projecting further than the one in
    front,} according to Oxford.
  \end{comm}

\end{defn}

\begin{thm} \label{thm:rowredeqrowredech}
  Every row-reduced $m \times n$ matrix $A$ is row-equivalent to
  a row-reduced echelon matrix.
  \begin{proof}
    Since $A$ is row-reduced, we know that every first-non-0
    entry in a row of $A$ has a unique column index compared to
    the other first-non-0 entries. This is because the first
    non-0 entry in a row of a row-reduced matrix must be in a
    column that would be all-0 if not for that entry.
    Furthermore, the specific row indices of the all-0 rows of a
    row-reduced echelon matrix are immaterial as long as they
    come after the non-all-0 rows. Since column indices are in
    $\mathbb{N}_1$, $\mathbb{N}_1$ is well-ordered, and there is
    a finite number $n$ of columns, a finite number of
    applications of elementary row operation \eqref{ero3} will
    suffice to map $A$ to a row-reduced echelon matrix.
  \end{proof}
\end{thm}

\begin{comm}
  If $R$ is an $m \times n$ row-reduced echelon matrix, the
  system of linear equations $RX = 0$ is particularly easy to
  solve.

  Consider the last of the example row-reduced echelon
  matrices above:
  \begin{align*}
    \begin{bmatrix}
      1 & 5i & 0 & 0 & 3\\
      0 & 0  & 1 & 0 & -\frac{2}{3}\\
      0 & 0  & 0 & 1 & -7\\
      0 & 0  & 0 & 0 & 0\\
      0 & 0  & 0 & 0 & 0\\
    \end{bmatrix}.
  \end{align*}
  If we call this $R$, the system $RX = 0$ could be represented
  as
  \begin{alignat*}{6}
     x_1\ &+&\ 5ix_2\ &+&\ 0x_3\ &+&\ 0x_4\ &+&\           3x_5\ &=&\ &0\\
    0x_1\ &+&\  0x_2\ &+&\ 1x_3\ &+&\ 0x_4\ &-&\ \frac{2}{3}x_5\ &=&\ &0\\
    0x_1\ &+&\  0x_2\ &+&\ 0x_3\ &+&\  x_4\ &-&\           7x_5\ &=&\ &0\\
    0x_1\ &+&\  0x_2\ &+&\ 0x_3\ &+&\ 0x_4\ &+&\           0x_5\ &=&\ &0\\
    0x_1\ &+&\  0x_2\ &+&\ 0x_3\ &+&\ 0x_4\ &+&\           0x_5\ &=&\ &0,
  \end{alignat*}
  or more simply as
  \begin{alignat*}{6}
     x_1\ &+&\ 5ix_2\ & &\      & & & &   &+&            3x_5\ &=&\ &0\\
     & &  & &         & &  x_3\ & &       &-&  \frac{2}{3}x_5\ &=&\ &0\\
     & &  & &         & &       & &\ x_4\ &-&\           7x_5\ &=&\ &0.
  \end{alignat*}
  In turn, this could be rewritten as
  \begin{align*}
    x_1 =&\ -5ix_2 - 3x_5\\
    x_3 =&\ \frac{2}{3}x_5\\
    x_4 =&\ 7x_5.
  \end{align*}

  Each of the unknowns in a system of linear equations
  corresponding to a first-non-0 row entry in a row-reduced
  matrix only occurs once, as with $x_1,\ x_3,\ x_4$ here. This
  allows us to state the solutions in terms of the other
  unknowns, which we can allow to vary freely, e.g. by assigning
  variables such that $x_2 = a$ and $x_5 = b$; the solutions to
  this system are then $(-5ia -3b,\ a,\ \frac{2}{3}b,\ 7b,\ b)$.

  A significant thing to note about this is that if the number of
  non-all-0 rows in $R$ is less than the number of columns, $RX =
  0$ has at least one non-trivial solution, i.e. a solution in
  which not all the unknowns are $0$. As we have shown, some of
  the unknowns will vary freely in the set of solutions to $RX =
  0$ under these conditions.
\end{comm}

\begin{thm} \label{thm:mltnnontriv}
  If $A$ is an $m \times n$ matrix over the field $F$ such that
  $m < n$, the homogeneous system of linear equations $AX = 0$
  has a non-trivial solution.
  \begin{proof}
    By theorem \ref{thm:roweqrowred}, we know that $A$ is
    row-equivalent to a row-reduced matrix $A'$, and we know by
    theorem \ref{thm:roweqsamesol} that $AX = 0$ and $A'X = 0$
    have the same solutions. Because $A'$ has less rows than
    columns, there will be more unknowns in $A'X = 0$ than there
    are first-non-0 entries in $A'$. In the set of solutions of
    $A'X = 0$, there will be a solution for every element in $F$
    for each unknown that does not correspond to a first-non-0
    entry in $A'$. Since $F$ must have at least additive and
    multiplicative identity elements, $A'X = 0$ will have a
    non-trivial solution, and so $AX = 0$ will as well.
  \end{proof}
\end{thm}

\begin{thm} \label{thm:roweqtoidiffonlytriv}
  If $A$ is an $n \times n$ (i.e. square) matrix, $A$ is
  row-equivalent to the $n \times n$ identity matrix if and only
  if $AX = 0$ has only the trivial solution.
  \begin{proof}
    If $A$ is row-equivalent to the $n \times n$ identity matrix,
    $AX = 0$ must have only the trivial solution, because every
    unknown in $AX = 0$ will occur only once and be said to equal
    $0$. If $A$ is not row-equivalent to the $n \times n$
    identity matrix, then if $A'$ is $A$ in a row-reduced form,
    $A'$ will have at least one all-0 row. An equation in $A'X =
    0$ corresponding to an all-0 row in $A'$ will express a
    tautology, i.e. that $0 = 0$, and thus the solution set of
    $A'X = 0$ will be equivalent to that for an $m \times n,\ m <
    n$ matrix $A''$ with rows equal to the non-all-0 rows of
    $A'$. By theorem \ref{thm:mltnnontriv}, $A''X = 0$ will have
    a non-trivial solution, and thus so will $AX = 0$.
  \end{proof}
\end{thm}

\begin{defn}
  If $A$ is an $m \times n$ matrix, the \textbf{augmented matrix}
  $A'$ of the system $AX = Y$ is the $m \times (n + 1)$ matrix
  such that columns $1,\ \ldots,\ n$ of $A'$ are the same as
  those of $A$ and column $(n + 1)$ of $A'$ is the same as that
  of $Y$. To be precise, $A'$ is such that
  \begin{align*}
    A'_{ij} =&\ A_{ij}\ \text{if}\ j \leq n;\\
    A'_{ij} =&\ Y_{1j}\ \text{if}\ j = n + 1.
  \end{align*}

  \begin{exm}
    If
    \begin{align*}
      A =
      \begin{bmatrix}
        1 & 0 & -3\\
        0 & 3 &  2\\
        5 & 0 &  9\\
      \end{bmatrix}\ \text{and}\ 
      Y =
      \begin{bmatrix}
        7\\
        8\\
        0\\
      \end{bmatrix},
    \end{align*}
    then
    \begin{align*}
      A' =
      \begin{bmatrix}
        1 & 0 & -3 & 7\\
        0 & 3 &  2 & 8\\
        5 & 0 &  9 & 0\\
      \end{bmatrix}.
    \end{align*}
  \end{exm}
\end{defn}

\begin{comm}
  By putting the augmented matrix of a system of linear equations
  into row-reduced echelon form, it becomes straightforward both
  to say whether or not solutions exist for the system and what
  those solutions could be if they do exist. If we put $A'$ above
  into row-reduced echelon form:
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & -3 & 7\\
      0 & 3 &  2 & 8\\
      5 & 0 &  9 & 0
    \end{bmatrix}
    \xrightarrow{(1)}
    \begin{bmatrix}
      1 & 0 & -3 & 7\\
      0 & 1 &  \frac{2}{3} & \frac{8}{3}\\
      5 & 0 &  9 & 0
    \end{bmatrix}
    \xrightarrow{(2)}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & -3 & 7\\
      0 & 1 &  \frac{2}{3} & \frac{8}{3}\\
      0 & 0 & 24 & -35
    \end{bmatrix}
    \xrightarrow{(1)}
    \begin{bmatrix}
      1 & 0 & -3 & 7\\
      0 & 1 &  \frac{2}{3} & \frac{8}{3}\\
      0 & 0 & 1 & -\frac{35}{24}
    \end{bmatrix}
    \xrightarrow{(2)}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 0 & \frac{21}{8}\\
      0 & 1 & 0 & \frac{131}{36}\\
      0 & 0 & 1 & -\frac{35}{24}
    \end{bmatrix},
  \end{align*}
  we can see that $AX = Y$ is equivalent to
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      x_1\\
      x_2\\
      x_3
    \end{bmatrix}
    =
    \begin{bmatrix}
      \frac{21}{8}\\
      \frac{131}{36}\\
      -\frac{35}{24}
    \end{bmatrix},
  \end{align*}
  so $AX = Y$ has only the solution $(x_1,\ x_2,\ x_3) =
  (\frac{21}{8},\ \frac{131}{36},\ -\frac{35}{24})$.

  If $A'$ was instead
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 0 & \frac{21}{8}\\
      0 & 1 & 0 & \frac{131}{36}\\
      0 & 0 & 0 & 0
    \end{bmatrix},
  \end{align*}
  we would instead have a set of solutions $(x_1,\ x_2,\ x_3) =
  (\frac{21}{8},\ \frac{131}{36},\ a)$ in which $a$ could take on
  any value in the field.

  This procedure works to characterize the possible solutions of
  the system even if the elements of $Y$ are unknown. For
  example, if we want to describe the solutions of
  \begin{align*}
    \begin{bmatrix}
      5  & 3  & -2\\
      4  & 0  & 8 \\
      -6 & -6 & 12
    \end{bmatrix}
    \begin{bmatrix}
      x_1\\
      x_2\\
      x_3
    \end{bmatrix}
    =
    \begin{bmatrix}
      y_1\\
      y_2\\
      y_3
    \end{bmatrix},
  \end{align*}
  we can proceed in the same manner:
  \begin{align*}
    \begin{bmatrix}
      5  & 3  & -2 & y_1\\
      4  & 0  & 8  & y_2\\
      -6 & -6 & 12 & y_3
    \end{bmatrix}
    \xrightarrow{(2)}
    \begin{bmatrix}
      5 & 3 & -2 & y_1\\
      4 & 0 & 8  & y_2\\
      4 & 0 & 8  & (y_3 + 2y_1)
    \end{bmatrix}
    \xrightarrow{(2)}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      5 & 3 & -2 & y_1\\
      4 & 0 & 8  & y_2\\
      0 & 0 & 0  & (y_3 + 2y_1 - y_2)
    \end{bmatrix}
    \xrightarrow{(1)}
    \begin{bmatrix}
      5 & 3 & -2 & y_1\\
      1 & 0 & 2  & \frac{1}{4}y_2\\
      0 & 0 & 0  & (y_3 + 2y_1 - y_2)
    \end{bmatrix}
    \xrightarrow{(1)}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      \frac{5}{3} & 1 & -\frac{2}{3} & \frac{1}{3}y_1\\
      1 & 0 & 2  & \frac{1}{4}y_2\\
      0 & 0 & 0  & (y_3 + 2y_1 - y_2)
    \end{bmatrix}
    \xrightarrow{(2)}
    \begin{bmatrix}
      0 & 1 & -4 & (\frac{1}{3}y_1 - \frac{5}{12}y_2)\\
      1 & 0 & 2  & \frac{1}{4}y_2\\
      0 & 0 & 0  & (y_3 + 2y_1 - y_2)
    \end{bmatrix}
    \xrightarrow{(3)}
  \end{align*}
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 2  & \frac{1}{4}y_2\\
      0 & 1 & -4 & (\frac{1}{3}y_1 - \frac{5}{12}y_2)\\
      0 & 0 & 0  & (y_3 + 2y_1 - y_2)
    \end{bmatrix}.
  \end{align*}
  This implies that the system has a solution if the given
  scalars $(y_1,\ y_2,\ y_3)$ are such that $2y_1 - y_2 + y_3 =
  0$. In that case, if we say $x_3 = c$, we can describe the
  solutions by
  \begin{align*}
    x_1 =&\ -2c + \frac{1}{4}y_2\\
    x_2 =&\ 4c + \frac{1}{3}y_1 - \frac{5}{12}y_2.
  \end{align*}
\end{comm}

\begin{comm}
  We can see that whether or not a system $AX = Y$ has a solution
  depends on whether or not the entries of $Y$ satisfy certain
  relations between each other, relations that are determined by
  the entries of $A$ and a composition of elementary row
  operations. For this reason, if $AX = Y$ has a solution in a
  field $F$, and the entries of both $A$ and $Y$ lie in a
  subfield $F_1$ of $F$, $AX = Y$ has a solution in $F_1$.

  For example, in the case of the system
  \begin{align*}
    \begin{bmatrix}
      1 & 0 & 2 \\
      0 & 1 & -4\\
      0 & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
      x_1\\
      x_2\\
      x_3
    \end{bmatrix}
    =
    \begin{bmatrix}
      \frac{1}{4}y_2\\
      (\frac{1}{3}y_1 - \frac{5}{12}y_2)\\
      (y_3 + 2y_1 - y_2)
    \end{bmatrix}
  \end{align*}
  above, if we describe this as $AX = Y$, the entries of $A$ and
  $Y$ lie within $\mathbb{Q}$. We could find solutions in
  $\mathbb{R}$ or $\mathbb{C}$ by assigning irrational or
  imaginary values to $y_1$, $y_2$, or $y_3$ that fit the
  constraints we found. However, since the elementary row
  operations involve only addition, subtraction, multiplication,
  and division, these constraints did not require a departure
  from $\mathbb{Q}$, so solutions can be found in $\mathbb{Q}$ by
  assigning rational values to the parameters.
\end{comm}

\begin{defn}
  If $A$ is an $m \times n$ matrix over a field $F$ and $B$ is an
  $n \times p$ matrix over $F$, we say that the \textbf{product}
  $AB$ is the $m \times p$ matrix $C$ whose $i,\ j$ entry is
  \begin{align*}
    C_{ij} =&\ \sum_{r = 1}^{n} A_{ir}B_{rj}.
  \end{align*}
\end{defn}

\begin{exm}
  \begin{align*}
    \begin{bmatrix}
      1 & -1\\
      3 & 2\\
      0 & -7
    \end{bmatrix}
    \begin{bmatrix}
      4  & 1 & 8\\
      20 & 5 & 7
    \end{bmatrix}:
  \end{align*}
  \begin{alignat*}{7}
    \gamma_1 &=&\ 1(4\ \ 1\ \ 8) &-&\ 1(20\ \ 5\ \ 7)
      &=&\ (-16&\ &-4&\ \ &1&)\\
    \gamma_2 &=&\ 3(4\ \ 1\ \ 8) &+&\ 2(20\ \ 5\ \ 7)
      &=&\ (52&\ &13&\ \ &38&)\\
    \gamma_3 &=&\ 0(4\ \ 1\ \ 8) &-&\ 7(20\ \ 5\ \ 7)
      &=&\ (-140&\ &-35&\ \ &-49&)
  \end{alignat*}
  \begin{align*}
    \begin{bmatrix}
      -16 & -4 & 1\\
      52 & 13 & 38\\
      -140 & -35  & -49
    \end{bmatrix}.
  \end{align*}
\end{exm}

\begin{exm}
  \begin{align*}
    \begin{bmatrix}
      1 & 1 & 1\\
      1 & 1 & 1
    \end{bmatrix}
    \begin{bmatrix}
      2\\
      3\\
      4
    \end{bmatrix}:
  \end{align*}
  \begin{alignat*}{7}
    \gamma_1 &=&\ 1(2) &+&\ 1(3) &+& 1(4) &=&\ (9)\\
    \gamma_2 &=&\ 1(2) &+&\ 1(3) &+& 1(4) &=&\ (9)\\
  \end{alignat*}
  \begin{align*}
    \begin{bmatrix}
      9\\
      9
    \end{bmatrix}.
  \end{align*}
\end{exm}

\begin{exm}
  \begin{align*}
    \begin{bmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{bmatrix}
    \begin{bmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1
    \end{bmatrix}:
  \end{align*}
  \begin{alignat*}{7}
    \gamma_1 &=&\ 1(1\ \ 0\ \ 0) &+&\ 2(0\ \ 1\ \ 0) &+&\ 3(0\ \ 0\ \ 1)
      &=&\ (1&\ &2&\ \ &3&)\\
    \gamma_2 &=&\ 4(1\ \ 0\ \ 0) &+&\ 5(0\ \ 1\ \ 0) &+&\ 6(0\ \ 0\ \ 1)
      &=&\ (4&\ &5&\ \ &6&)\\
  \end{alignat*}
  \begin{align*}
    \begin{bmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{bmatrix}.
  \end{align*}
\end{exm}

\begin{exm}
  \begin{align*}
    \begin{bmatrix}
      1 & 0\\
      0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{bmatrix}:
  \end{align*}
  \begin{alignat*}{7}
    \gamma_1 &=&\ 1(1\ \ 2\ \ 3) &+&\ 0(4\ \ 5\ \ 6)
      &=&\ (1&\ &2&\ \ &3&)\\
    \gamma_2 &=&\ 0(1\ \ 2\ \ 3) &+&\ 1(4\ \ 5\ \ 6)
      &=&\ (4&\ &5&\ \ &6&)\\
  \end{alignat*}
  \begin{align*}
    \begin{bmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{bmatrix}.
  \end{align*}
\end{exm}

\begin{exm}
  \begin{align*}
    \begin{bmatrix}
      2 & 0\\
      0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{bmatrix}:
  \end{align*}
  \begin{alignat*}{7}
    \gamma_1 &=&\ 2(1\ \ 2\ \ 3) &+&\ 0(4\ \ 5\ \ 6)
      &=&\ (2&\ &4&\ \ &6&)\\
    \gamma_2 &=&\ 0(1\ \ 2\ \ 3) &+&\ 1(4\ \ 5\ \ 6)
      &=&\ (4&\ &5&\ \ &6&)\\
  \end{alignat*}
  \begin{align*}
    \begin{bmatrix}
      2 & 4 & 6\\
      4 & 5 & 6
    \end{bmatrix}.
  \end{align*}
\end{exm}

\begin{exm}
  \begin{align*}
    \begin{bmatrix}
      1 & 0\\
      -1 & 1
    \end{bmatrix}
    \begin{bmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{bmatrix}:
  \end{align*}
  \begin{alignat*}{7}
    \gamma_1 &=&\ 1(1\ \ 2\ \ 3) &+&\ 0(4\ \ 5\ \ 6)
      &=&\ (1&\ &2&\ \ &3&)\\
    \gamma_2 &=&\ -1(1\ \ 2\ \ 3) &+&\ 1(4\ \ 5\ \ 6)
      &=&\ (3&\ &3&\ \ &3&)\\
  \end{alignat*}
  \begin{align*}
    \begin{bmatrix}
      1 & 2 & 3\\
      3 & 3 & 3
    \end{bmatrix}.
  \end{align*}
\end{exm}

\begin{exm}
  \begin{align*}
    \begin{bmatrix}
      0 & 1\\
      1 & 0
    \end{bmatrix}
    \begin{bmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{bmatrix}:
  \end{align*}
  \begin{alignat*}{7}
    \gamma_1 &=&\ 0(1\ \ 2\ \ 3) &+&\ 1(4\ \ 5\ \ 6)
      &=&\ (4&\ &5&\ \ &6&)\\
    \gamma_2 &=&\ 1(1\ \ 2\ \ 3) &+&\ 0(4\ \ 5\ \ 6)
      &=&\ (1&\ &2&\ \ &3&)\\
  \end{alignat*}
  \begin{align*}
    \begin{bmatrix}
      4 & 5 & 6\\
      1 & 2 & 3
    \end{bmatrix}.
  \end{align*}
\end{exm}

\begin{comm}
  Note that matrix multiplication is not commutative;
  \begin{align*}
    \begin{bmatrix}
      0 & 0 & 1 & 0\\
      0 & 1 & 0 & 0
    \end{bmatrix}
    \begin{bmatrix}
      1 & 2\\
      3 & 4\\
      5 & 6\\
      7 & 8
    \end{bmatrix}
    =
    \begin{bmatrix}
      5 & 6\\
      3 & 4
    \end{bmatrix},\\
  \end{align*}
  \begin{center}
    while
  \end{center}
  \begin{align*}
    \begin{bmatrix}
      1 & 2\\
      3 & 4\\
      5 & 6\\
      7 & 8
    \end{bmatrix}
    \begin{bmatrix}
      0 & 0 & 1 & 0\\
      0 & 1 & 0 & 0
    \end{bmatrix}
    =
    \begin{bmatrix}
      0 & 2 & 1 & 0\\
      0 & 4 & 3 & 0\\
      0 & 6 & 5 & 0\\
      0 & 8 & 7 & 0
    \end{bmatrix}.
  \end{align*}

  This implies that, if $I$ is the $m \times m$ identity matrix
  and $A$ is an $m \times n$ matrix, $IA = A$, while if $I$ is
  the $n \times n$ identity matrix, $AI = A$. It also implies
  that if $0^{k,m}$ is the $k \times m$ zero matrix, $0^{k,n} =
  0^{k,m}A$, while $A0^{n,p} = 0^{m,p}$.
\end{comm}

\begin{comm}
  If $B$ is an $n \times p$ matrix, it can be convenient to
  notate the columns of $B$ as $1 \times n$ matrices
  $B_1,\ldots,B_p$, such that
  \begin{align*}
    B_j =
    \begin{bmatrix}
      B_{1j}\\
      \vdots\\
      B_{nj}
    \end{bmatrix},\ 
    1 \leq j \leq p.
  \end{align*}

  $B$ is then the succession of these columns:
  \begin{align*}
    B = [B_1,\ldots,B_p].
  \end{align*}

  The product matrix $AB$ is then formed from the $i$th row of
  $A$ and the $j$th column of $B$, such that the $j$ column of
  $AB$ is $AB_j$:
  \begin{align*}
    A =
    \begin{bmatrix}
      A_{11} & \ldots & A_{1n}\\
      \vdots & \ddots & \vdots\\
      A_{m1} & \ldots & A_{mn}\\
    \end{bmatrix};
  \end{align*}
  \begin{align*}
    B =
    \begin{bmatrix}
      B_{11} & \ldots & B_{1p}\\
      \vdots & \ddots & \vdots\\
      B_{n1} & \ldots & B_{np}\\
    \end{bmatrix};
  \end{align*}
  \begin{align*}
    AB =
    \begin{bmatrix}
      (A_{11}B_{11}\ +\ \ldots\ +\ A_{1n}B_{n1})\
        & \cdots
        & (A_{11}B_{1p}\ +\ \ldots\ +\ A_{1n}B_{np})\\
      \vdots & \ddots & \vdots\\
      (A_{m1}B_{11}\ +\ \ldots\ +\ A_{mn}B_{n1})\
        & \cdots
        & (A_{m1}B_{1p}\ +\ \ldots\ +\ A_{mn}B_{np})\\
    \end{bmatrix};
  \end{align*}
  \begin{align*}
    AB_j =
    \begin{bmatrix}
      (A_{11}B_{1j}\ +\ \ldots\ +\ A_{1n}B_{nj})\\
      \vdots\\
      (A_{m1}B_{1j}\ +\ \ldots\ +\ A_{mn}B_{nj})\\
    \end{bmatrix}.
  \end{align*}
\end{comm}

\begin{thm} \label{thm:matmulassoc}
  If $A,\ B,\ C$ are matrices over the same field and the
  products $BC$ and $A(BC)$ are defined, then so are the products
  $AB$ and $(AB)C$, and
  \begin{align*}
    A(BC) = (AB)C.
  \end{align*}
  In other words, although matrix multiplication is not
  commutative, it is associative.
  \begin{proof}
    Say $B$ is an $m \times n$ matrix, i.e. a matrix with $n$
    columns. For $BC$ to be defined, $C$ must be an $n \times p$
    matrix, i.e. a matrix with $n$ rows. $BC$ will then be an $m
    \times p$ matrix, so for $A(BC)$ to be defined, $A$ must be a
    $q \times m$ matrix. $A(BC)$ is then a $q \times p$ matrix.

    Therefore, since $A$ is a $q \times m$ matrix, $AB$ is
    defined, since $B$ is an $m \times n$ matrix; $AB$ is then a
    $q \times n$ matrix. In that case, $(AB)C$ is defined as
    well, since $C$ is an $n \times p$ matrix; $(AB)C$ is then a
    $q \times p$ matrix.

    We have
    \begin{align*}
      A =
      \begin{bmatrix}
        A_{11}
          & \cdots
          & A_{1m}\\
        \vdots
          & \ddots
          & \vdots\\
        A_{q1}
          & \cdots
          & A_{qm}
      \end{bmatrix},\
      B =
      \begin{bmatrix}
        B_{11}
          & \cdots
          & B_{1n}\\
        \vdots
          & \ddots
          & \vdots\\
        B_{m1}
          & \cdots
          & B_{mn}
      \end{bmatrix},\
      C =
      \begin{bmatrix}
        C_{11}
          & \cdots
          & C_{1p}\\
        \vdots
          & \ddots
          & \vdots\\
        C_{n1}
          & \cdots
          & C_{np}
      \end{bmatrix}.
    \end{align*}

    \begin{align*}
      BC =
      \begin{bmatrix}
        \sum_{s = 1}^{n} B_{1s}C_{s1}
          & \cdots
          & \sum_{s = 1}^{n} B_{1s}C_{sp}\\
        \vdots
          & \ddots
          & \vdots\\
        \sum_{s = 1}^{n} B_{ms}C_{s1}
          & \cdots
          & \sum_{s = 1}^{n} B_{ms}C_{sp}
      \end{bmatrix},
    \end{align*}
    so if $BC_j,\ 1 \leq j \leq p$ is a column of $BC$,
    \begin{align*}
      A(BC)_j =&
      \begin{bmatrix}
        A_{11}(\sum_{s = 1}^{n} B_{1s}C_{sj})
          + \ldots
          + A_{1m}(\sum_{s = 1}^{n} B_{ms}C_{sj})\\
        \vdots\\
        A_{q1}(\sum_{s = 1}^{n} B_{1s}C_{sj})
          + \ldots
          + A_{qm}(\sum_{s = 1}^{n} B_{ms}C_{sj})
      \end{bmatrix}\\
      =&
      \begin{bmatrix}
        \sum_{r = 1}^{m}\sum_{s = 1}^{n} A_{1r}B_{rs}C_{sj}\\
        \vdots\\
        \sum_{r = 1}^{m}\sum_{s = 1}^{n} A_{qr}B_{rs}C_{sj}\\
      \end{bmatrix}.
    \end{align*}
    Therefore
    \begin{align*}
      A(BC) =
      \begin{bmatrix}
        \sum_{r = 1}^{m}\sum_{s = 1}^{n} A_{1r}B_{rs}C_{s1}
          & \cdots
          & \sum_{r = 1}^{m}\sum_{s = 1}^{n} A_{1r}B_{rs}C_{sp}\\
        \vdots & \ddots & \vdots\\
        \sum_{r = 1}^{m}\sum_{s = 1}^{n} A_{qr}B_{rs}C_{s1}
          & \cdots
          & \sum_{r = 1}^{m}\sum_{s = 1}^{n} A_{qr}B_{rs}C_{sp}\\
      \end{bmatrix}.
    \end{align*}
    Likewise,
    \begin{align*}
      AB =
      \begin{bmatrix}
        \sum_{r = 1}^{m} A_{1r}B_{r1}
          & \cdots
          & \sum_{r = 1}^{m} A_{1r}B_{rn}\\
        \vdots
          & \ddots
          & \vdots\\
        \sum_{r = 1}^{m} A_{qr}B_{r1}
          & \cdots
          & \sum_{r = 1}^{m} A_{qr}B_{rn}\\
      \end{bmatrix},
    \end{align*}
    so if $C_j,\ 1 \leq j \leq p$ is a column of $C$,
    \begin{align*}
      (AB)C_j =&
      \begin{bmatrix}
        (\sum_{r = 1}^{m} A_{1r}B_{r1})C_{1j}
          + \ldots
          + (\sum_{r = 1}^{m} A_{1r}B_{rn})C_{nj}\\
        \vdots\\
        (\sum_{r = 1}^{m} A_{qr}B_{r1})C_{1j}
          + \ldots
          + (\sum_{r = 1}^{m} A_{qr}B_{rn})C_{nj}\\
      \end{bmatrix}\\
      =&
      \begin{bmatrix}
        \sum_{r = 1}^{m}\sum_{s = 1}^{n} A_{1r}B_{rs}C_{sj}\\
        \vdots\\
        \sum_{r = 1}^{m}\sum_{s = 1}^{n} A_{qr}B_{rs}C_{sj}\\
      \end{bmatrix},
    \end{align*}
    and
    \begin{align*}
      (AB)C =&
      \begin{bmatrix}
        \sum_{r = 1}^{m}\sum_{s = 1}^{n} A_{1r}B_{rs}C_{s1}
          & \cdots
          & \sum_{r = 1}^{m}\sum_{s = 1}^{n} A_{1r}B_{rs}C_{sp}\\
        \vdots & \ddots & \vdots\\
        \sum_{r = 1}^{m}\sum_{s = 1}^{n} A_{qr}B_{rs}C_{s1}
          & \cdots
          & \sum_{r = 1}^{m}\sum_{s = 1}^{n} A_{qr}B_{rs}C_{sp}\\
      \end{bmatrix}\\
      =&\ A(BC).
    \end{align*}
  \end{proof}
\end{thm}

\begin{defn}
  If $A$ is an $n \times n$ (square) matrix, the product $AA$ is
  defined; we denote this matrix by $\bm{A^2}$. Furthermore,
  $A(AA) = (AA)A$ by theorem \ref{thm:matmulassoc}, so $AAA$ is
  unambiguously defined; we denote this matrix by $\bm{A^3}$.
  Following this logic, $AA\ldots A$ for $k$
  $A$s is unambiguously defined, and we denote this matrix by
  $\bm{A^k}$.
\end{defn}

\begin{defn}
  If an $m \times n$ matrix $A$ is such that it can be generated
  from the $m \times m$ identity matrix using a single elementary
  row operation, we call $A$ an \textbf{elementary matrix}.

  \begin{exm}
    Here are the five possible forms of $2 \times 2$ elementary
    matrices:
    \begin{align*}
      \begin{bmatrix}
        0 & 1\\
        1 & 0
      \end{bmatrix},\
      \begin{bmatrix}
        1 & c\\
        0 & 1
      \end{bmatrix},\
      \begin{bmatrix}
        1 & 0\\
        c & 1
      \end{bmatrix},\
      \begin{bmatrix}
        c & 0\\
        0 & 1
      \end{bmatrix}\ \text{if}\ c \neq 0,
      \begin{bmatrix}
        1 & 0\\
        0 & c
      \end{bmatrix}\ \text{if}\ c \neq 0.
    \end{align*}
  \end{exm}
\end{defn}

\begin{thm} \label{thm:elrowelmat}
  If $e$ is an elementary row operation and we have an $m \times
  m$ elementary matrix $E = e(I)$, then for any $m \times n$
  matrix $A$,
  \begin{align*}
    e(A) = EA.
  \end{align*}
  \begin{proof}
    We know by theorem \ref{thm:elrowinv} that there is an
    elementary row operation $e^{-1}$ such that $e^{-1}(e(A)) =
    A$. If $E = e(I)$, then $e^{-1}(E) = e^{-1}(e(I)) = I$, so
    $e^{-1}(E)A = A$. Therefore $e^{-1}(e(A)) = e^{-1}(E)A = A$,
    so $e(e^{-1}(e(A))) = e(e^{-1}(E))A$ and thus $e(A) = EA$.
  \end{proof}
\end{thm}

\begin{cor} \label{cor:rowequivelmatprod}
  If $A$ and $B$ are $m \times n$ matrices over the same field,
  $B$ is row-equivalent to $A$ if and only if there is an $m
  \times m$ matrix $P$ such that $B = PA$ and $P$ is the product
  of elementary matrices.

  \begin{proof}
    If $B$ is row-equivalent to $A$, there is a finite series of
    elementary row operations that maps $A$ to $B$. We denote
    this hypothetical series of elementary row operations by $f$,
    such that $B = f(A)$. If $P$ is the product of elementary
    matrices, we know by theorem \ref{thm:elrowelmat} that there
    is a finite series of elementary row operations $f_1$ such
    that $f_1(I) = P$. Then $PA = f_1(A)$, and if $B = PA$, $B =
    f_1(A)$, so $f_1 = f$ and $B$ is row-equivalent to $A$.

    Conversely, if there is no matrix like $P$, there is no
    finite series of elementary row operations that will map $A$
    to $B$, so $A$ and $B$ are not row-equivalent. As described,
    $P$ is the matrix that would be obtained by applying this
    series of elementary row operations to $I$, where $I$ is the
    $m \times m$ identity matrix.
  \end{proof}
\end{cor}

\begin{defn}
  If $A$ is an $n \times n$ matrix over a field, a matrix $B$
  such that $BA = I$ is called a \textbf{left inverse} of $A$, a
  likewise matrix $B$ such that $AB = I$ is called a
  \textbf{right inverse} of $A$, and if $AB = BA = I$, $B$ is
  called a \textbf{two-sided inverse} of $A$, and $A$ is said to
  be \textbf{invertible}.

  \begin{nlemma}
    If $A$ has a left inverse $B$ and a right inverse $C$, then
    $B = C$.
    \begin{proof}
      If $BA = I$, then $BAC = IC$. If $AC = I$, then $BAC =
      B(AC) = BI = B$ and $IC = C$, so $B = C$.
    \end{proof}
  \end{nlemma}

  As such, if $A$ has a left and right inverse, it is invertible
  and has a unique two-sided inverse. We denote this two-sided
  inverse by $A^{-1}$, and call it the \textbf{inverse} of $A$.
\end{defn}

\begin{thm} \label{thm:ifinvertsoinv}
  Let $A$ and $B$ be $n \times n$ matrices over the same field
  $F$.
  \begin{enumerate}
    \item If $A$ is invertible, so is $A^{-1}$, and $(A^{-1})^{-1}
      = A$.
    \item If both $A$ and $B$ are invertible, so is $AB$, and
      $(AB)^{-1} = B^{-1}A^{-1}$.
  \end{enumerate}

  \begin{proof}
    If $A$ is invertible, $AA^{-1} = A^{-1}A = I$. Then
    $A^{-1}AA^{-1}A = IA^{-1}A = A^{-1}A = I$ and
    $AA^{-1}AA^{-1} = IAA^{-1} = AA^{-1} = I$, so $A^{-1}A =
    AA^{-1} = I$, and thus $A^{-1}$ is invertible and
    $(A^{-1})^{-1} = A$. This proves (1).

    If both $A$ and $B$ are invertible, $ABB^{-1}A^{-1} = AA^{-1}
    = B^{-1}A^{-1}AB = B^{-1}B = I$, so $AB$ is invertible and
    $(AB)^{-1} = B^{-1}A^{-1}$. This proves (2).
  \end{proof}
\end{thm}

\begin{thm} \label{thm:elmatsareinv}
  Elementary matrices are invertible.

  \begin{proof}
    If $E = e(I)$ is an elementary matrix for some elementary row
    operation $e$, we know by theorem \ref{thm:elrowinv} that
    there is an elementary row operation $e^{-1}$ such that
    $e^{-1}(e(I)) = I$. By theorem \ref{thm:elrowelmat}, there is
    an elementary matrix $E' = e^{-1}(I)$ such that $E'E =
    e^{-1}(E) = e^{-1}(e(I)) = I$. Likewise, $EE' = e(E') =
    e(e^{-1}(I)) = I$, so $E' = E^{-1}$ and thus $E$ is
    invertible.
  \end{proof}
\end{thm}

\begin{thm} \label{thm:invroweqelmatprod}
  If $A$ is an $n \times n$ matrix, the following statements are
  equivalent.
  \begin{enumerate}
    \item $A$ is invertible.
    \item $A$ is row-equivalent to the $n \times n$ identity
      matrix.
    \item $A$ is a product of elementary matrices.
  \end{enumerate}

  \begin{proof}
    If $A$ is invertible, then $I = A^{-1}A$, where $I$ is the $n
    \times n$ identity matrix. By corollary
    \ref{cor:rowequivelmatprod}, $I$ is then row-equivalent to
    $A$, and thus $A$ is row-equivalent to $I$ by corollary
    \ref{cor:rowequivisequivrel}. Thus (1) implies (2), and (2)
    implies (1) by the same argument in reverse. Likewise, $I =
    AA^{-1}$, so $A$ is a product of elementary matrices by
    corollary \ref{cor:rowequivelmatprod}. Thus (1) implies (3),
    so (2) implies (3) as well by proxy.

    If $A$ is a product of elementary matrices $E_1 \ldots E_n$,
    theorem \ref{thm:elmatsareinv} implies that there is a
    product of their inverses $E_n^{-1} \ldots E_1^{-1}$ such
    that $I =\\ E_n^{-1} \ldots E_1^{-1}(E_1 \ldots E_n) = (E_1
    \ldots E_n)E_n^{-1} \ldots E_1^{-1} = AA^{-1} = A^{-1}A$.
    Thus $A$ is invertible, so (3) implies (1), and thus (3)
    implies (2) as well by proxy. Therefore (1), (2), and (3) are
    equivalent.
  \end{proof}
\end{thm}

\begin{cor}
  If $A$ is an invertible $n \times n$ matrix, there is a
  sequence of elementary row operations that reduces $A$ to the
  $n \times n$ identity matrix $I$, and that same set of
  operations yields $A^{-1}$ when applied to $I$.

  \begin{proof}
    Since $A$ is a product of elementary matrices, theorem
    \ref{thm:elrowelmat} implies that there is a sequence of
    elementary row operations $e_1,\ldots,e_k$ such that\\
    $e_k(\ldots e_1(I)\ldots) = A$. By theorem
    \ref{thm:elrowinv}, there is then a sequence of elementary
    row operations $e_k^{-1},\ldots,e_1^{-1}$ such that
    $e_1^{-1}(\ldots e_k^{-1}(A)\ldots) = I$. Going the other way
    by theorem \ref{thm:elrowelmat}, $e_1^{-1}(\ldots
    e_k^{-1}(I)\ldots)$ is equivalent to taking the product of
    $I$ and a sequence of elementary matrices
    $E_1^{-1},\ldots,E_k^{-1}$, i.e. $E_1^{-1}\ldots E_k^{-1}I$.
    Since $E_1^{-1}\ldots E_k^{-1}IA = e_1^{-1}(\ldots
    e_k^{-1}(A)\ldots) = I$, and $AE_1^{-1}\ldots E_k^{-1}I =
    e_k^{-1}(\ldots e_1(e_1^{-1}(\ldots
    e_k^{-1}(I)\ldots)\ldots)\ldots) = I$, $e_1^{-1}(\ldots
    e_k^{-1}(I)\ldots) = A^{-1}$.
  \end{proof}
\end{cor}

\begin{cor}
  If $A$ and $B$ are $m \times n$ matrices, $B$ is row-equivalent
  to $A$ if and only if $B = PA$ where $P$ is an invertible $m
  \times m$ matrix.

  \begin{proof}
    By corollary \ref{cor:rowequivelmatprod}, $P$ is the product
    of elementary matrices, so $P$ is invertible.
  \end{proof}
\end{cor}

\begin{thm} \label{thm:invonlytrivsoleveryequiv}
  For an $n \times n$ matrix $A$, the following statements are
  equivalent:

  \begin{enumerate}
    \item $A$ is invertible.
    \item The homogeneous system $AX = 0$ has only the trivial
      solution $X = 0$.
    \item The system of equations $AX = Y$ has a solution $X$ for
      each $n \times 1$ matrix $Y$.
  \end{enumerate}

  \begin{proof}
    If $A$ is invertible, $A$ is row-equivalent to the $n \times
    n$ identity matrix by theorem \ref{thm:invroweqelmatprod},
    and thus $AX = 0$ has only the trivial solution $X = 0$ by
    theorem \ref{thm:roweqtoidiffonlytriv}. Thus (1) implies (2).

    If $A$ is invertible and $AX = Y$, $A^{-1}AX = X = A^{-1}Y$.
    $A^{-1}Y$ is defined, as $A^{-1}$ is an $n \times n$ matrix
    and $Y$ is an $n \times 1$ matrix. Therefore $AX = Y$ has the
    solution $X = A^{-1}Y$ for each $n \times 1$ matrix $Y$ and
    thus (1) implies (3).

    By theorem \ref{thm:roweqtoidiffonlytriv}, if $AX = 0$ has
    only the trivial solution $X = 0$, $A$ is row-equivalent to
    the $n \times n$ identity matrix. This implies that $A$ is
    invertible by theorem \ref{thm:invroweqelmatprod}, and thus
    (2) implies (1), as well as (3) by proxy.

    If $AX = Y$ has a solution $X$ for each $n \times 1$ matrix
    $Y$, corollary \ref{cor:rowequivelmatprod} implies that $A$
    is the product of elementary matrices, which by theorem
    \ref{thm:invroweqelmatprod} implies that $A$ is invertible,
    and thus that (3) implies (1). Then (3) implies (2) by proxy,
    and thus (1), (2), and (3) are equivalent.
  \end{proof}
\end{thm}

\end{document}
